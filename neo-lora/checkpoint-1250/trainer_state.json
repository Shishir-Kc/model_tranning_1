{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 20.061046600341797,
      "learning_rate": 0.0002,
      "loss": 12.2464,
      "step": 1
    },
    {
      "epoch": 0.008,
      "grad_norm": 18.934911727905273,
      "learning_rate": 0.00019984,
      "loss": 12.6729,
      "step": 2
    },
    {
      "epoch": 0.012,
      "grad_norm": 17.685413360595703,
      "learning_rate": 0.00019968,
      "loss": 11.4358,
      "step": 3
    },
    {
      "epoch": 0.016,
      "grad_norm": 16.4833984375,
      "learning_rate": 0.00019952000000000001,
      "loss": 9.6997,
      "step": 4
    },
    {
      "epoch": 0.02,
      "grad_norm": 12.430697441101074,
      "learning_rate": 0.00019936000000000002,
      "loss": 9.2096,
      "step": 5
    },
    {
      "epoch": 0.024,
      "grad_norm": 16.8298282623291,
      "learning_rate": 0.00019920000000000002,
      "loss": 8.6252,
      "step": 6
    },
    {
      "epoch": 0.028,
      "grad_norm": 13.19626522064209,
      "learning_rate": 0.00019904,
      "loss": 8.6728,
      "step": 7
    },
    {
      "epoch": 0.032,
      "grad_norm": 10.312897682189941,
      "learning_rate": 0.00019888,
      "loss": 8.0406,
      "step": 8
    },
    {
      "epoch": 0.036,
      "grad_norm": 10.453070640563965,
      "learning_rate": 0.00019872000000000002,
      "loss": 7.9622,
      "step": 9
    },
    {
      "epoch": 0.04,
      "grad_norm": 10.959054946899414,
      "learning_rate": 0.00019856000000000002,
      "loss": 7.7196,
      "step": 10
    },
    {
      "epoch": 0.044,
      "grad_norm": 9.860020637512207,
      "learning_rate": 0.0001984,
      "loss": 8.0855,
      "step": 11
    },
    {
      "epoch": 0.048,
      "grad_norm": 9.09835433959961,
      "learning_rate": 0.00019824,
      "loss": 6.9418,
      "step": 12
    },
    {
      "epoch": 0.052,
      "grad_norm": 10.862785339355469,
      "learning_rate": 0.00019808,
      "loss": 7.1844,
      "step": 13
    },
    {
      "epoch": 0.056,
      "grad_norm": 7.014791011810303,
      "learning_rate": 0.00019792000000000003,
      "loss": 6.6578,
      "step": 14
    },
    {
      "epoch": 0.06,
      "grad_norm": 6.489748477935791,
      "learning_rate": 0.00019776,
      "loss": 6.1203,
      "step": 15
    },
    {
      "epoch": 0.064,
      "grad_norm": 7.161588191986084,
      "learning_rate": 0.0001976,
      "loss": 6.6885,
      "step": 16
    },
    {
      "epoch": 0.068,
      "grad_norm": 6.779227256774902,
      "learning_rate": 0.00019744,
      "loss": 6.7651,
      "step": 17
    },
    {
      "epoch": 0.072,
      "grad_norm": 6.246061325073242,
      "learning_rate": 0.00019728,
      "loss": 5.9513,
      "step": 18
    },
    {
      "epoch": 0.076,
      "grad_norm": 6.103057384490967,
      "learning_rate": 0.00019712,
      "loss": 5.8315,
      "step": 19
    },
    {
      "epoch": 0.08,
      "grad_norm": 4.573336601257324,
      "learning_rate": 0.00019696,
      "loss": 5.8488,
      "step": 20
    },
    {
      "epoch": 0.084,
      "grad_norm": 5.5548553466796875,
      "learning_rate": 0.0001968,
      "loss": 6.7801,
      "step": 21
    },
    {
      "epoch": 0.088,
      "grad_norm": 5.584327220916748,
      "learning_rate": 0.00019664000000000001,
      "loss": 6.6814,
      "step": 22
    },
    {
      "epoch": 0.092,
      "grad_norm": 4.703382968902588,
      "learning_rate": 0.00019648000000000002,
      "loss": 5.7224,
      "step": 23
    },
    {
      "epoch": 0.096,
      "grad_norm": 3.934079170227051,
      "learning_rate": 0.00019632000000000002,
      "loss": 5.542,
      "step": 24
    },
    {
      "epoch": 0.1,
      "grad_norm": 5.1800856590271,
      "learning_rate": 0.00019616000000000002,
      "loss": 5.9337,
      "step": 25
    },
    {
      "epoch": 0.104,
      "grad_norm": 4.005115509033203,
      "learning_rate": 0.000196,
      "loss": 5.1341,
      "step": 26
    },
    {
      "epoch": 0.108,
      "grad_norm": 4.510472297668457,
      "learning_rate": 0.00019584,
      "loss": 5.1955,
      "step": 27
    },
    {
      "epoch": 0.112,
      "grad_norm": 3.779062509536743,
      "learning_rate": 0.00019568000000000002,
      "loss": 5.319,
      "step": 28
    },
    {
      "epoch": 0.116,
      "grad_norm": 6.125605583190918,
      "learning_rate": 0.00019552000000000003,
      "loss": 6.4279,
      "step": 29
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.5000410079956055,
      "learning_rate": 0.00019536,
      "loss": 5.1367,
      "step": 30
    },
    {
      "epoch": 0.124,
      "grad_norm": 5.420403957366943,
      "learning_rate": 0.0001952,
      "loss": 5.5785,
      "step": 31
    },
    {
      "epoch": 0.128,
      "grad_norm": 4.950883865356445,
      "learning_rate": 0.00019504,
      "loss": 5.2778,
      "step": 32
    },
    {
      "epoch": 0.132,
      "grad_norm": 5.254561901092529,
      "learning_rate": 0.00019488000000000003,
      "loss": 5.4604,
      "step": 33
    },
    {
      "epoch": 0.136,
      "grad_norm": 4.723464012145996,
      "learning_rate": 0.00019472,
      "loss": 5.2663,
      "step": 34
    },
    {
      "epoch": 0.14,
      "grad_norm": 5.712512969970703,
      "learning_rate": 0.00019456,
      "loss": 5.6358,
      "step": 35
    },
    {
      "epoch": 0.144,
      "grad_norm": 4.970522880554199,
      "learning_rate": 0.0001944,
      "loss": 5.0189,
      "step": 36
    },
    {
      "epoch": 0.148,
      "grad_norm": 5.200202465057373,
      "learning_rate": 0.00019424,
      "loss": 4.4885,
      "step": 37
    },
    {
      "epoch": 0.152,
      "grad_norm": 4.406681537628174,
      "learning_rate": 0.00019408,
      "loss": 4.8783,
      "step": 38
    },
    {
      "epoch": 0.156,
      "grad_norm": 4.711677074432373,
      "learning_rate": 0.00019392000000000001,
      "loss": 4.8633,
      "step": 39
    },
    {
      "epoch": 0.16,
      "grad_norm": 4.874056339263916,
      "learning_rate": 0.00019376000000000002,
      "loss": 4.4928,
      "step": 40
    },
    {
      "epoch": 0.164,
      "grad_norm": 4.290464401245117,
      "learning_rate": 0.00019360000000000002,
      "loss": 4.6091,
      "step": 41
    },
    {
      "epoch": 0.168,
      "grad_norm": 5.282986164093018,
      "learning_rate": 0.00019344,
      "loss": 5.1494,
      "step": 42
    },
    {
      "epoch": 0.172,
      "grad_norm": 4.097511291503906,
      "learning_rate": 0.00019328000000000002,
      "loss": 4.2619,
      "step": 43
    },
    {
      "epoch": 0.176,
      "grad_norm": 4.250088691711426,
      "learning_rate": 0.00019312000000000002,
      "loss": 4.6425,
      "step": 44
    },
    {
      "epoch": 0.18,
      "grad_norm": 3.986868143081665,
      "learning_rate": 0.00019296,
      "loss": 4.2055,
      "step": 45
    },
    {
      "epoch": 0.184,
      "grad_norm": 4.682357311248779,
      "learning_rate": 0.0001928,
      "loss": 4.3103,
      "step": 46
    },
    {
      "epoch": 0.188,
      "grad_norm": 3.9126458168029785,
      "learning_rate": 0.00019264,
      "loss": 4.4681,
      "step": 47
    },
    {
      "epoch": 0.192,
      "grad_norm": 3.834871292114258,
      "learning_rate": 0.00019248000000000003,
      "loss": 4.1019,
      "step": 48
    },
    {
      "epoch": 0.196,
      "grad_norm": 3.975426435470581,
      "learning_rate": 0.00019232,
      "loss": 4.2062,
      "step": 49
    },
    {
      "epoch": 0.2,
      "grad_norm": 4.886996269226074,
      "learning_rate": 0.00019216,
      "loss": 4.5523,
      "step": 50
    },
    {
      "epoch": 0.204,
      "grad_norm": 4.6538472175598145,
      "learning_rate": 0.000192,
      "loss": 4.2765,
      "step": 51
    },
    {
      "epoch": 0.208,
      "grad_norm": 4.469482898712158,
      "learning_rate": 0.00019184,
      "loss": 3.9133,
      "step": 52
    },
    {
      "epoch": 0.212,
      "grad_norm": 4.707077503204346,
      "learning_rate": 0.00019168,
      "loss": 4.5122,
      "step": 53
    },
    {
      "epoch": 0.216,
      "grad_norm": 3.8613271713256836,
      "learning_rate": 0.00019152,
      "loss": 4.0031,
      "step": 54
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.49188232421875,
      "learning_rate": 0.00019136,
      "loss": 4.3242,
      "step": 55
    },
    {
      "epoch": 0.224,
      "grad_norm": 4.083105087280273,
      "learning_rate": 0.0001912,
      "loss": 3.8386,
      "step": 56
    },
    {
      "epoch": 0.228,
      "grad_norm": 4.317741394042969,
      "learning_rate": 0.00019104000000000001,
      "loss": 4.1552,
      "step": 57
    },
    {
      "epoch": 0.232,
      "grad_norm": 5.2887115478515625,
      "learning_rate": 0.00019088000000000002,
      "loss": 4.1994,
      "step": 58
    },
    {
      "epoch": 0.236,
      "grad_norm": 6.235316753387451,
      "learning_rate": 0.00019072000000000002,
      "loss": 4.2927,
      "step": 59
    },
    {
      "epoch": 0.24,
      "grad_norm": 4.926854133605957,
      "learning_rate": 0.00019056000000000002,
      "loss": 4.0875,
      "step": 60
    },
    {
      "epoch": 0.244,
      "grad_norm": 4.026132583618164,
      "learning_rate": 0.0001904,
      "loss": 4.0491,
      "step": 61
    },
    {
      "epoch": 0.248,
      "grad_norm": 3.7835822105407715,
      "learning_rate": 0.00019024000000000002,
      "loss": 4.266,
      "step": 62
    },
    {
      "epoch": 0.252,
      "grad_norm": 4.562314987182617,
      "learning_rate": 0.00019008000000000002,
      "loss": 3.7474,
      "step": 63
    },
    {
      "epoch": 0.256,
      "grad_norm": 4.889725208282471,
      "learning_rate": 0.00018992,
      "loss": 3.9533,
      "step": 64
    },
    {
      "epoch": 0.26,
      "grad_norm": 3.8174190521240234,
      "learning_rate": 0.00018976,
      "loss": 3.8764,
      "step": 65
    },
    {
      "epoch": 0.264,
      "grad_norm": 5.701510906219482,
      "learning_rate": 0.0001896,
      "loss": 3.9716,
      "step": 66
    },
    {
      "epoch": 0.268,
      "grad_norm": 4.210887908935547,
      "learning_rate": 0.00018944000000000003,
      "loss": 3.4722,
      "step": 67
    },
    {
      "epoch": 0.272,
      "grad_norm": 4.354431629180908,
      "learning_rate": 0.00018928,
      "loss": 3.9102,
      "step": 68
    },
    {
      "epoch": 0.276,
      "grad_norm": 4.312203884124756,
      "learning_rate": 0.00018912,
      "loss": 4.1193,
      "step": 69
    },
    {
      "epoch": 0.28,
      "grad_norm": 3.62094783782959,
      "learning_rate": 0.00018896,
      "loss": 3.6005,
      "step": 70
    },
    {
      "epoch": 0.284,
      "grad_norm": 3.5226221084594727,
      "learning_rate": 0.0001888,
      "loss": 3.5414,
      "step": 71
    },
    {
      "epoch": 0.288,
      "grad_norm": 3.969780921936035,
      "learning_rate": 0.00018864,
      "loss": 3.3691,
      "step": 72
    },
    {
      "epoch": 0.292,
      "grad_norm": 3.7505085468292236,
      "learning_rate": 0.00018848,
      "loss": 3.6671,
      "step": 73
    },
    {
      "epoch": 0.296,
      "grad_norm": 4.094271659851074,
      "learning_rate": 0.00018832,
      "loss": 3.3047,
      "step": 74
    },
    {
      "epoch": 0.3,
      "grad_norm": 4.512025356292725,
      "learning_rate": 0.00018816000000000001,
      "loss": 4.1495,
      "step": 75
    },
    {
      "epoch": 0.304,
      "grad_norm": 5.1209869384765625,
      "learning_rate": 0.000188,
      "loss": 4.1159,
      "step": 76
    },
    {
      "epoch": 0.308,
      "grad_norm": 3.7767419815063477,
      "learning_rate": 0.00018784000000000002,
      "loss": 3.4664,
      "step": 77
    },
    {
      "epoch": 0.312,
      "grad_norm": 3.853895425796509,
      "learning_rate": 0.00018768000000000002,
      "loss": 3.6251,
      "step": 78
    },
    {
      "epoch": 0.316,
      "grad_norm": 4.818998336791992,
      "learning_rate": 0.00018752,
      "loss": 3.4777,
      "step": 79
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.936589479446411,
      "learning_rate": 0.00018736,
      "loss": 3.7431,
      "step": 80
    },
    {
      "epoch": 0.324,
      "grad_norm": 3.8378844261169434,
      "learning_rate": 0.00018720000000000002,
      "loss": 3.8454,
      "step": 81
    },
    {
      "epoch": 0.328,
      "grad_norm": 4.402586460113525,
      "learning_rate": 0.00018704000000000003,
      "loss": 3.4823,
      "step": 82
    },
    {
      "epoch": 0.332,
      "grad_norm": 5.813029766082764,
      "learning_rate": 0.00018688,
      "loss": 3.9249,
      "step": 83
    },
    {
      "epoch": 0.336,
      "grad_norm": 4.811916351318359,
      "learning_rate": 0.00018672,
      "loss": 3.5929,
      "step": 84
    },
    {
      "epoch": 0.34,
      "grad_norm": 5.752218246459961,
      "learning_rate": 0.00018656,
      "loss": 3.932,
      "step": 85
    },
    {
      "epoch": 0.344,
      "grad_norm": 5.546236038208008,
      "learning_rate": 0.00018640000000000003,
      "loss": 3.7701,
      "step": 86
    },
    {
      "epoch": 0.348,
      "grad_norm": 5.735795497894287,
      "learning_rate": 0.00018624,
      "loss": 3.3828,
      "step": 87
    },
    {
      "epoch": 0.352,
      "grad_norm": 4.993557929992676,
      "learning_rate": 0.00018608,
      "loss": 3.7047,
      "step": 88
    },
    {
      "epoch": 0.356,
      "grad_norm": 6.100672721862793,
      "learning_rate": 0.00018592,
      "loss": 3.6853,
      "step": 89
    },
    {
      "epoch": 0.36,
      "grad_norm": 4.698208808898926,
      "learning_rate": 0.00018576,
      "loss": 3.2724,
      "step": 90
    },
    {
      "epoch": 0.364,
      "grad_norm": 4.437368392944336,
      "learning_rate": 0.0001856,
      "loss": 2.9934,
      "step": 91
    },
    {
      "epoch": 0.368,
      "grad_norm": 5.244500637054443,
      "learning_rate": 0.00018544,
      "loss": 3.7069,
      "step": 92
    },
    {
      "epoch": 0.372,
      "grad_norm": 4.6333112716674805,
      "learning_rate": 0.00018528000000000001,
      "loss": 3.1417,
      "step": 93
    },
    {
      "epoch": 0.376,
      "grad_norm": 4.940316200256348,
      "learning_rate": 0.00018512000000000002,
      "loss": 2.9669,
      "step": 94
    },
    {
      "epoch": 0.38,
      "grad_norm": 4.6651482582092285,
      "learning_rate": 0.00018496,
      "loss": 2.8832,
      "step": 95
    },
    {
      "epoch": 0.384,
      "grad_norm": 6.155269145965576,
      "learning_rate": 0.00018480000000000002,
      "loss": 3.4003,
      "step": 96
    },
    {
      "epoch": 0.388,
      "grad_norm": 5.623291492462158,
      "learning_rate": 0.00018464000000000002,
      "loss": 3.4634,
      "step": 97
    },
    {
      "epoch": 0.392,
      "grad_norm": 6.164555549621582,
      "learning_rate": 0.00018448,
      "loss": 3.6843,
      "step": 98
    },
    {
      "epoch": 0.396,
      "grad_norm": 6.754148006439209,
      "learning_rate": 0.00018432,
      "loss": 3.7817,
      "step": 99
    },
    {
      "epoch": 0.4,
      "grad_norm": 6.1398491859436035,
      "learning_rate": 0.00018416,
      "loss": 3.6657,
      "step": 100
    },
    {
      "epoch": 0.404,
      "grad_norm": 5.409054279327393,
      "learning_rate": 0.00018400000000000003,
      "loss": 3.0344,
      "step": 101
    },
    {
      "epoch": 0.408,
      "grad_norm": 5.791460037231445,
      "learning_rate": 0.00018384,
      "loss": 3.1951,
      "step": 102
    },
    {
      "epoch": 0.412,
      "grad_norm": 5.225376605987549,
      "learning_rate": 0.00018368,
      "loss": 3.0794,
      "step": 103
    },
    {
      "epoch": 0.416,
      "grad_norm": 5.312775611877441,
      "learning_rate": 0.00018352,
      "loss": 3.1355,
      "step": 104
    },
    {
      "epoch": 0.42,
      "grad_norm": 4.749699592590332,
      "learning_rate": 0.00018336,
      "loss": 2.7452,
      "step": 105
    },
    {
      "epoch": 0.424,
      "grad_norm": 5.826817512512207,
      "learning_rate": 0.0001832,
      "loss": 3.168,
      "step": 106
    },
    {
      "epoch": 0.428,
      "grad_norm": 5.773918628692627,
      "learning_rate": 0.00018304,
      "loss": 3.4332,
      "step": 107
    },
    {
      "epoch": 0.432,
      "grad_norm": 5.433931827545166,
      "learning_rate": 0.00018288,
      "loss": 3.1158,
      "step": 108
    },
    {
      "epoch": 0.436,
      "grad_norm": 6.514406681060791,
      "learning_rate": 0.00018272,
      "loss": 3.5616,
      "step": 109
    },
    {
      "epoch": 0.44,
      "grad_norm": 7.085414886474609,
      "learning_rate": 0.00018256,
      "loss": 3.096,
      "step": 110
    },
    {
      "epoch": 0.444,
      "grad_norm": 4.8559184074401855,
      "learning_rate": 0.00018240000000000002,
      "loss": 2.4727,
      "step": 111
    },
    {
      "epoch": 0.448,
      "grad_norm": 5.52430534362793,
      "learning_rate": 0.00018224000000000002,
      "loss": 3.3499,
      "step": 112
    },
    {
      "epoch": 0.452,
      "grad_norm": 4.158419132232666,
      "learning_rate": 0.00018208000000000002,
      "loss": 2.4561,
      "step": 113
    },
    {
      "epoch": 0.456,
      "grad_norm": 5.368037700653076,
      "learning_rate": 0.00018192,
      "loss": 3.1268,
      "step": 114
    },
    {
      "epoch": 0.46,
      "grad_norm": 5.345850944519043,
      "learning_rate": 0.00018176000000000002,
      "loss": 2.7136,
      "step": 115
    },
    {
      "epoch": 0.464,
      "grad_norm": 5.80402946472168,
      "learning_rate": 0.00018160000000000002,
      "loss": 2.7477,
      "step": 116
    },
    {
      "epoch": 0.468,
      "grad_norm": 5.7265143394470215,
      "learning_rate": 0.00018144,
      "loss": 3.4568,
      "step": 117
    },
    {
      "epoch": 0.472,
      "grad_norm": 5.277104377746582,
      "learning_rate": 0.00018128,
      "loss": 2.9851,
      "step": 118
    },
    {
      "epoch": 0.476,
      "grad_norm": 5.747945785522461,
      "learning_rate": 0.00018112,
      "loss": 2.8196,
      "step": 119
    },
    {
      "epoch": 0.48,
      "grad_norm": 7.443965435028076,
      "learning_rate": 0.00018096000000000003,
      "loss": 2.9673,
      "step": 120
    },
    {
      "epoch": 0.484,
      "grad_norm": 4.889324188232422,
      "learning_rate": 0.0001808,
      "loss": 2.822,
      "step": 121
    },
    {
      "epoch": 0.488,
      "grad_norm": 5.423235893249512,
      "learning_rate": 0.00018064,
      "loss": 2.6206,
      "step": 122
    },
    {
      "epoch": 0.492,
      "grad_norm": 5.518697261810303,
      "learning_rate": 0.00018048,
      "loss": 3.1267,
      "step": 123
    },
    {
      "epoch": 0.496,
      "grad_norm": 6.374375343322754,
      "learning_rate": 0.00018032,
      "loss": 3.085,
      "step": 124
    },
    {
      "epoch": 0.5,
      "grad_norm": 4.999446868896484,
      "learning_rate": 0.00018016,
      "loss": 2.8014,
      "step": 125
    },
    {
      "epoch": 0.504,
      "grad_norm": 5.231761455535889,
      "learning_rate": 0.00018,
      "loss": 2.6808,
      "step": 126
    },
    {
      "epoch": 0.508,
      "grad_norm": 5.945644378662109,
      "learning_rate": 0.00017984,
      "loss": 3.0002,
      "step": 127
    },
    {
      "epoch": 0.512,
      "grad_norm": 5.3187384605407715,
      "learning_rate": 0.00017968000000000001,
      "loss": 2.6531,
      "step": 128
    },
    {
      "epoch": 0.516,
      "grad_norm": 6.295984268188477,
      "learning_rate": 0.00017952,
      "loss": 2.7098,
      "step": 129
    },
    {
      "epoch": 0.52,
      "grad_norm": 5.974246978759766,
      "learning_rate": 0.00017936000000000002,
      "loss": 2.8866,
      "step": 130
    },
    {
      "epoch": 0.524,
      "grad_norm": 5.961630344390869,
      "learning_rate": 0.00017920000000000002,
      "loss": 3.0856,
      "step": 131
    },
    {
      "epoch": 0.528,
      "grad_norm": 6.1553568840026855,
      "learning_rate": 0.00017904000000000002,
      "loss": 2.7783,
      "step": 132
    },
    {
      "epoch": 0.532,
      "grad_norm": 6.592569828033447,
      "learning_rate": 0.00017888,
      "loss": 3.3856,
      "step": 133
    },
    {
      "epoch": 0.536,
      "grad_norm": 4.249228000640869,
      "learning_rate": 0.00017872,
      "loss": 2.4089,
      "step": 134
    },
    {
      "epoch": 0.54,
      "grad_norm": 5.532865524291992,
      "learning_rate": 0.00017856000000000003,
      "loss": 2.272,
      "step": 135
    },
    {
      "epoch": 0.544,
      "grad_norm": 5.909515380859375,
      "learning_rate": 0.0001784,
      "loss": 2.7106,
      "step": 136
    },
    {
      "epoch": 0.548,
      "grad_norm": 5.514476776123047,
      "learning_rate": 0.00017824,
      "loss": 2.9806,
      "step": 137
    },
    {
      "epoch": 0.552,
      "grad_norm": 5.720211505889893,
      "learning_rate": 0.00017808,
      "loss": 2.6658,
      "step": 138
    },
    {
      "epoch": 0.556,
      "grad_norm": 6.866875171661377,
      "learning_rate": 0.00017792,
      "loss": 2.5408,
      "step": 139
    },
    {
      "epoch": 0.56,
      "grad_norm": 7.206021785736084,
      "learning_rate": 0.00017776,
      "loss": 3.0297,
      "step": 140
    },
    {
      "epoch": 0.564,
      "grad_norm": 4.315744400024414,
      "learning_rate": 0.0001776,
      "loss": 2.346,
      "step": 141
    },
    {
      "epoch": 0.568,
      "grad_norm": 4.838962554931641,
      "learning_rate": 0.00017744,
      "loss": 2.3557,
      "step": 142
    },
    {
      "epoch": 0.572,
      "grad_norm": 11.581454277038574,
      "learning_rate": 0.00017728,
      "loss": 2.9713,
      "step": 143
    },
    {
      "epoch": 0.576,
      "grad_norm": 7.453816890716553,
      "learning_rate": 0.00017712,
      "loss": 2.7134,
      "step": 144
    },
    {
      "epoch": 0.58,
      "grad_norm": 6.601674556732178,
      "learning_rate": 0.00017696,
      "loss": 2.1658,
      "step": 145
    },
    {
      "epoch": 0.584,
      "grad_norm": 5.090623378753662,
      "learning_rate": 0.00017680000000000001,
      "loss": 2.2697,
      "step": 146
    },
    {
      "epoch": 0.588,
      "grad_norm": 7.718135356903076,
      "learning_rate": 0.00017664000000000002,
      "loss": 3.1343,
      "step": 147
    },
    {
      "epoch": 0.592,
      "grad_norm": 6.975397109985352,
      "learning_rate": 0.00017648,
      "loss": 2.4681,
      "step": 148
    },
    {
      "epoch": 0.596,
      "grad_norm": 5.684065341949463,
      "learning_rate": 0.00017632000000000002,
      "loss": 2.8838,
      "step": 149
    },
    {
      "epoch": 0.6,
      "grad_norm": 6.807322025299072,
      "learning_rate": 0.00017616000000000002,
      "loss": 2.1581,
      "step": 150
    },
    {
      "epoch": 0.604,
      "grad_norm": 6.040463447570801,
      "learning_rate": 0.00017600000000000002,
      "loss": 2.5699,
      "step": 151
    },
    {
      "epoch": 0.608,
      "grad_norm": 8.169671058654785,
      "learning_rate": 0.00017584,
      "loss": 1.9177,
      "step": 152
    },
    {
      "epoch": 0.612,
      "grad_norm": 8.99902057647705,
      "learning_rate": 0.00017568,
      "loss": 2.0212,
      "step": 153
    },
    {
      "epoch": 0.616,
      "grad_norm": 8.023752212524414,
      "learning_rate": 0.00017552000000000003,
      "loss": 2.8541,
      "step": 154
    },
    {
      "epoch": 0.62,
      "grad_norm": 6.039097785949707,
      "learning_rate": 0.00017536,
      "loss": 1.9321,
      "step": 155
    },
    {
      "epoch": 0.624,
      "grad_norm": 6.866164207458496,
      "learning_rate": 0.0001752,
      "loss": 2.2973,
      "step": 156
    },
    {
      "epoch": 0.628,
      "grad_norm": 6.0636420249938965,
      "learning_rate": 0.00017504,
      "loss": 2.2396,
      "step": 157
    },
    {
      "epoch": 0.632,
      "grad_norm": 5.782043933868408,
      "learning_rate": 0.00017488,
      "loss": 2.0509,
      "step": 158
    },
    {
      "epoch": 0.636,
      "grad_norm": 6.177903652191162,
      "learning_rate": 0.00017472,
      "loss": 2.5804,
      "step": 159
    },
    {
      "epoch": 0.64,
      "grad_norm": 7.216057300567627,
      "learning_rate": 0.00017456,
      "loss": 2.8944,
      "step": 160
    },
    {
      "epoch": 0.644,
      "grad_norm": 5.148056983947754,
      "learning_rate": 0.0001744,
      "loss": 2.006,
      "step": 161
    },
    {
      "epoch": 0.648,
      "grad_norm": 6.389691352844238,
      "learning_rate": 0.00017424,
      "loss": 2.5323,
      "step": 162
    },
    {
      "epoch": 0.652,
      "grad_norm": 6.005954265594482,
      "learning_rate": 0.00017408,
      "loss": 2.4932,
      "step": 163
    },
    {
      "epoch": 0.656,
      "grad_norm": 7.809159755706787,
      "learning_rate": 0.00017392000000000002,
      "loss": 2.9447,
      "step": 164
    },
    {
      "epoch": 0.66,
      "grad_norm": 9.238113403320312,
      "learning_rate": 0.00017376000000000002,
      "loss": 2.1152,
      "step": 165
    },
    {
      "epoch": 0.664,
      "grad_norm": 5.55888032913208,
      "learning_rate": 0.00017360000000000002,
      "loss": 1.8915,
      "step": 166
    },
    {
      "epoch": 0.668,
      "grad_norm": 7.234232425689697,
      "learning_rate": 0.00017344,
      "loss": 2.9043,
      "step": 167
    },
    {
      "epoch": 0.672,
      "grad_norm": 4.657034873962402,
      "learning_rate": 0.00017328,
      "loss": 2.5273,
      "step": 168
    },
    {
      "epoch": 0.676,
      "grad_norm": 5.366483211517334,
      "learning_rate": 0.00017312000000000002,
      "loss": 2.5872,
      "step": 169
    },
    {
      "epoch": 0.68,
      "grad_norm": 6.109314441680908,
      "learning_rate": 0.00017296,
      "loss": 2.9147,
      "step": 170
    },
    {
      "epoch": 0.684,
      "grad_norm": 7.368006229400635,
      "learning_rate": 0.0001728,
      "loss": 2.4503,
      "step": 171
    },
    {
      "epoch": 0.688,
      "grad_norm": 6.777222156524658,
      "learning_rate": 0.00017264,
      "loss": 2.2797,
      "step": 172
    },
    {
      "epoch": 0.692,
      "grad_norm": 5.2148847579956055,
      "learning_rate": 0.00017248000000000003,
      "loss": 2.4159,
      "step": 173
    },
    {
      "epoch": 0.696,
      "grad_norm": 6.62406063079834,
      "learning_rate": 0.00017232,
      "loss": 2.8084,
      "step": 174
    },
    {
      "epoch": 0.7,
      "grad_norm": 4.846388816833496,
      "learning_rate": 0.00017216,
      "loss": 2.2295,
      "step": 175
    },
    {
      "epoch": 0.704,
      "grad_norm": 5.3615522384643555,
      "learning_rate": 0.000172,
      "loss": 2.441,
      "step": 176
    },
    {
      "epoch": 0.708,
      "grad_norm": 4.775156497955322,
      "learning_rate": 0.00017184,
      "loss": 2.1642,
      "step": 177
    },
    {
      "epoch": 0.712,
      "grad_norm": 6.527190685272217,
      "learning_rate": 0.00017168,
      "loss": 2.0973,
      "step": 178
    },
    {
      "epoch": 0.716,
      "grad_norm": 7.9831390380859375,
      "learning_rate": 0.00017152,
      "loss": 2.8547,
      "step": 179
    },
    {
      "epoch": 0.72,
      "grad_norm": 5.595345973968506,
      "learning_rate": 0.00017136,
      "loss": 2.6222,
      "step": 180
    },
    {
      "epoch": 0.724,
      "grad_norm": 7.183767318725586,
      "learning_rate": 0.00017120000000000001,
      "loss": 2.8357,
      "step": 181
    },
    {
      "epoch": 0.728,
      "grad_norm": 5.6146626472473145,
      "learning_rate": 0.00017104,
      "loss": 2.2177,
      "step": 182
    },
    {
      "epoch": 0.732,
      "grad_norm": 7.670121192932129,
      "learning_rate": 0.00017088000000000002,
      "loss": 2.9379,
      "step": 183
    },
    {
      "epoch": 0.736,
      "grad_norm": 6.593764781951904,
      "learning_rate": 0.00017072000000000002,
      "loss": 2.4427,
      "step": 184
    },
    {
      "epoch": 0.74,
      "grad_norm": 6.362069606781006,
      "learning_rate": 0.00017056000000000002,
      "loss": 2.8479,
      "step": 185
    },
    {
      "epoch": 0.744,
      "grad_norm": 6.936216831207275,
      "learning_rate": 0.0001704,
      "loss": 2.2973,
      "step": 186
    },
    {
      "epoch": 0.748,
      "grad_norm": 8.912379264831543,
      "learning_rate": 0.00017024,
      "loss": 2.6737,
      "step": 187
    },
    {
      "epoch": 0.752,
      "grad_norm": 9.236968994140625,
      "learning_rate": 0.00017008000000000002,
      "loss": 2.6184,
      "step": 188
    },
    {
      "epoch": 0.756,
      "grad_norm": 6.602722644805908,
      "learning_rate": 0.00016992,
      "loss": 2.556,
      "step": 189
    },
    {
      "epoch": 0.76,
      "grad_norm": 8.09381103515625,
      "learning_rate": 0.00016976,
      "loss": 2.3847,
      "step": 190
    },
    {
      "epoch": 0.764,
      "grad_norm": 6.60624361038208,
      "learning_rate": 0.0001696,
      "loss": 2.242,
      "step": 191
    },
    {
      "epoch": 0.768,
      "grad_norm": 6.8640594482421875,
      "learning_rate": 0.00016944,
      "loss": 2.3488,
      "step": 192
    },
    {
      "epoch": 0.772,
      "grad_norm": 5.327777862548828,
      "learning_rate": 0.00016928,
      "loss": 2.2709,
      "step": 193
    },
    {
      "epoch": 0.776,
      "grad_norm": 6.527009010314941,
      "learning_rate": 0.00016912,
      "loss": 2.5107,
      "step": 194
    },
    {
      "epoch": 0.78,
      "grad_norm": 5.515925407409668,
      "learning_rate": 0.00016896,
      "loss": 1.5621,
      "step": 195
    },
    {
      "epoch": 0.784,
      "grad_norm": 8.203314781188965,
      "learning_rate": 0.0001688,
      "loss": 2.5434,
      "step": 196
    },
    {
      "epoch": 0.788,
      "grad_norm": 6.428277015686035,
      "learning_rate": 0.00016863999999999998,
      "loss": 2.4004,
      "step": 197
    },
    {
      "epoch": 0.792,
      "grad_norm": 7.940975189208984,
      "learning_rate": 0.00016848,
      "loss": 2.4152,
      "step": 198
    },
    {
      "epoch": 0.796,
      "grad_norm": 6.977865695953369,
      "learning_rate": 0.00016832000000000001,
      "loss": 2.109,
      "step": 199
    },
    {
      "epoch": 0.8,
      "grad_norm": 7.320204257965088,
      "learning_rate": 0.00016816000000000002,
      "loss": 2.4895,
      "step": 200
    },
    {
      "epoch": 0.804,
      "grad_norm": 8.067181587219238,
      "learning_rate": 0.000168,
      "loss": 2.639,
      "step": 201
    },
    {
      "epoch": 0.808,
      "grad_norm": 4.819338798522949,
      "learning_rate": 0.00016784,
      "loss": 2.0463,
      "step": 202
    },
    {
      "epoch": 0.812,
      "grad_norm": 7.603203296661377,
      "learning_rate": 0.00016768000000000002,
      "loss": 2.4898,
      "step": 203
    },
    {
      "epoch": 0.816,
      "grad_norm": 4.974973201751709,
      "learning_rate": 0.00016752000000000002,
      "loss": 1.9254,
      "step": 204
    },
    {
      "epoch": 0.82,
      "grad_norm": 8.102285385131836,
      "learning_rate": 0.00016736,
      "loss": 2.1929,
      "step": 205
    },
    {
      "epoch": 0.824,
      "grad_norm": 7.179849147796631,
      "learning_rate": 0.0001672,
      "loss": 2.305,
      "step": 206
    },
    {
      "epoch": 0.828,
      "grad_norm": 6.842353820800781,
      "learning_rate": 0.00016704000000000003,
      "loss": 1.9854,
      "step": 207
    },
    {
      "epoch": 0.832,
      "grad_norm": 7.652048110961914,
      "learning_rate": 0.00016688,
      "loss": 2.4521,
      "step": 208
    },
    {
      "epoch": 0.836,
      "grad_norm": 6.743287086486816,
      "learning_rate": 0.00016672,
      "loss": 2.4651,
      "step": 209
    },
    {
      "epoch": 0.84,
      "grad_norm": 6.4714884757995605,
      "learning_rate": 0.00016656,
      "loss": 1.5526,
      "step": 210
    },
    {
      "epoch": 0.844,
      "grad_norm": 5.491672039031982,
      "learning_rate": 0.0001664,
      "loss": 2.1219,
      "step": 211
    },
    {
      "epoch": 0.848,
      "grad_norm": 11.52011775970459,
      "learning_rate": 0.00016624,
      "loss": 2.6311,
      "step": 212
    },
    {
      "epoch": 0.852,
      "grad_norm": 9.928232192993164,
      "learning_rate": 0.00016608,
      "loss": 2.2447,
      "step": 213
    },
    {
      "epoch": 0.856,
      "grad_norm": 5.656605243682861,
      "learning_rate": 0.00016592,
      "loss": 2.0458,
      "step": 214
    },
    {
      "epoch": 0.86,
      "grad_norm": 5.848855018615723,
      "learning_rate": 0.00016576,
      "loss": 2.1039,
      "step": 215
    },
    {
      "epoch": 0.864,
      "grad_norm": 7.423372268676758,
      "learning_rate": 0.0001656,
      "loss": 2.0789,
      "step": 216
    },
    {
      "epoch": 0.868,
      "grad_norm": 9.44163703918457,
      "learning_rate": 0.00016544000000000002,
      "loss": 2.2639,
      "step": 217
    },
    {
      "epoch": 0.872,
      "grad_norm": 8.226950645446777,
      "learning_rate": 0.00016528000000000002,
      "loss": 2.5621,
      "step": 218
    },
    {
      "epoch": 0.876,
      "grad_norm": 7.459712028503418,
      "learning_rate": 0.00016512000000000002,
      "loss": 2.0997,
      "step": 219
    },
    {
      "epoch": 0.88,
      "grad_norm": 7.711915016174316,
      "learning_rate": 0.00016496,
      "loss": 2.5755,
      "step": 220
    },
    {
      "epoch": 0.884,
      "grad_norm": 6.786635398864746,
      "learning_rate": 0.0001648,
      "loss": 2.493,
      "step": 221
    },
    {
      "epoch": 0.888,
      "grad_norm": 5.6724629402160645,
      "learning_rate": 0.00016464000000000002,
      "loss": 2.0452,
      "step": 222
    },
    {
      "epoch": 0.892,
      "grad_norm": 10.361727714538574,
      "learning_rate": 0.00016448000000000002,
      "loss": 1.8612,
      "step": 223
    },
    {
      "epoch": 0.896,
      "grad_norm": 7.39326810836792,
      "learning_rate": 0.00016432,
      "loss": 2.4278,
      "step": 224
    },
    {
      "epoch": 0.9,
      "grad_norm": 6.377220630645752,
      "learning_rate": 0.00016416,
      "loss": 1.7891,
      "step": 225
    },
    {
      "epoch": 0.904,
      "grad_norm": 6.679140567779541,
      "learning_rate": 0.000164,
      "loss": 1.7694,
      "step": 226
    },
    {
      "epoch": 0.908,
      "grad_norm": 6.310567855834961,
      "learning_rate": 0.00016384,
      "loss": 1.6201,
      "step": 227
    },
    {
      "epoch": 0.912,
      "grad_norm": 8.143274307250977,
      "learning_rate": 0.00016368,
      "loss": 2.9025,
      "step": 228
    },
    {
      "epoch": 0.916,
      "grad_norm": 12.110005378723145,
      "learning_rate": 0.00016352,
      "loss": 2.31,
      "step": 229
    },
    {
      "epoch": 0.92,
      "grad_norm": 6.706005096435547,
      "learning_rate": 0.00016336,
      "loss": 1.9019,
      "step": 230
    },
    {
      "epoch": 0.924,
      "grad_norm": 6.739883899688721,
      "learning_rate": 0.0001632,
      "loss": 2.4531,
      "step": 231
    },
    {
      "epoch": 0.928,
      "grad_norm": 4.9407172203063965,
      "learning_rate": 0.00016304,
      "loss": 1.948,
      "step": 232
    },
    {
      "epoch": 0.932,
      "grad_norm": 8.40628719329834,
      "learning_rate": 0.00016288,
      "loss": 2.2863,
      "step": 233
    },
    {
      "epoch": 0.936,
      "grad_norm": 6.81451940536499,
      "learning_rate": 0.00016272000000000001,
      "loss": 2.41,
      "step": 234
    },
    {
      "epoch": 0.94,
      "grad_norm": 6.64410400390625,
      "learning_rate": 0.00016256,
      "loss": 2.061,
      "step": 235
    },
    {
      "epoch": 0.944,
      "grad_norm": 6.169590950012207,
      "learning_rate": 0.00016240000000000002,
      "loss": 1.9539,
      "step": 236
    },
    {
      "epoch": 0.948,
      "grad_norm": 5.248652458190918,
      "learning_rate": 0.00016224000000000002,
      "loss": 1.8477,
      "step": 237
    },
    {
      "epoch": 0.952,
      "grad_norm": 10.668152809143066,
      "learning_rate": 0.00016208000000000002,
      "loss": 2.8104,
      "step": 238
    },
    {
      "epoch": 0.956,
      "grad_norm": 7.231560230255127,
      "learning_rate": 0.00016192,
      "loss": 2.5826,
      "step": 239
    },
    {
      "epoch": 0.96,
      "grad_norm": 8.112794876098633,
      "learning_rate": 0.00016176,
      "loss": 2.3535,
      "step": 240
    },
    {
      "epoch": 0.964,
      "grad_norm": 5.584040641784668,
      "learning_rate": 0.00016160000000000002,
      "loss": 2.0614,
      "step": 241
    },
    {
      "epoch": 0.968,
      "grad_norm": 7.645662784576416,
      "learning_rate": 0.00016144000000000003,
      "loss": 2.5431,
      "step": 242
    },
    {
      "epoch": 0.972,
      "grad_norm": 7.734259605407715,
      "learning_rate": 0.00016128,
      "loss": 2.2413,
      "step": 243
    },
    {
      "epoch": 0.976,
      "grad_norm": 8.048775672912598,
      "learning_rate": 0.00016112,
      "loss": 2.2289,
      "step": 244
    },
    {
      "epoch": 0.98,
      "grad_norm": 4.400021076202393,
      "learning_rate": 0.00016096,
      "loss": 1.4007,
      "step": 245
    },
    {
      "epoch": 0.984,
      "grad_norm": 5.0224199295043945,
      "learning_rate": 0.0001608,
      "loss": 1.7064,
      "step": 246
    },
    {
      "epoch": 0.988,
      "grad_norm": 5.088878631591797,
      "learning_rate": 0.00016064,
      "loss": 1.8685,
      "step": 247
    },
    {
      "epoch": 0.992,
      "grad_norm": 6.807990074157715,
      "learning_rate": 0.00016048,
      "loss": 2.4145,
      "step": 248
    },
    {
      "epoch": 0.996,
      "grad_norm": 9.323984146118164,
      "learning_rate": 0.00016032,
      "loss": 2.3147,
      "step": 249
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.866551399230957,
      "learning_rate": 0.00016016,
      "loss": 1.8705,
      "step": 250
    },
    {
      "epoch": 1.004,
      "grad_norm": 5.942836761474609,
      "learning_rate": 0.00016,
      "loss": 1.7261,
      "step": 251
    },
    {
      "epoch": 1.008,
      "grad_norm": 6.786577224731445,
      "learning_rate": 0.00015984000000000001,
      "loss": 2.391,
      "step": 252
    },
    {
      "epoch": 1.012,
      "grad_norm": 8.037464141845703,
      "learning_rate": 0.00015968000000000002,
      "loss": 2.332,
      "step": 253
    },
    {
      "epoch": 1.016,
      "grad_norm": 6.193657398223877,
      "learning_rate": 0.00015952,
      "loss": 1.687,
      "step": 254
    },
    {
      "epoch": 1.02,
      "grad_norm": 7.4686198234558105,
      "learning_rate": 0.00015936,
      "loss": 2.2221,
      "step": 255
    },
    {
      "epoch": 1.024,
      "grad_norm": 17.253522872924805,
      "learning_rate": 0.00015920000000000002,
      "loss": 2.3497,
      "step": 256
    },
    {
      "epoch": 1.028,
      "grad_norm": 5.875200271606445,
      "learning_rate": 0.00015904000000000002,
      "loss": 1.8488,
      "step": 257
    },
    {
      "epoch": 1.032,
      "grad_norm": 9.140656471252441,
      "learning_rate": 0.00015888,
      "loss": 2.2855,
      "step": 258
    },
    {
      "epoch": 1.036,
      "grad_norm": 7.9525017738342285,
      "learning_rate": 0.00015872,
      "loss": 2.2256,
      "step": 259
    },
    {
      "epoch": 1.04,
      "grad_norm": 6.914470195770264,
      "learning_rate": 0.00015856,
      "loss": 2.2736,
      "step": 260
    },
    {
      "epoch": 1.044,
      "grad_norm": 4.974582195281982,
      "learning_rate": 0.00015840000000000003,
      "loss": 1.6595,
      "step": 261
    },
    {
      "epoch": 1.048,
      "grad_norm": 6.305220603942871,
      "learning_rate": 0.00015824,
      "loss": 1.6665,
      "step": 262
    },
    {
      "epoch": 1.052,
      "grad_norm": 6.491339206695557,
      "learning_rate": 0.00015808,
      "loss": 2.0718,
      "step": 263
    },
    {
      "epoch": 1.056,
      "grad_norm": 6.6006975173950195,
      "learning_rate": 0.00015792,
      "loss": 2.2283,
      "step": 264
    },
    {
      "epoch": 1.06,
      "grad_norm": 6.87750244140625,
      "learning_rate": 0.00015776,
      "loss": 2.0775,
      "step": 265
    },
    {
      "epoch": 1.064,
      "grad_norm": 4.629426002502441,
      "learning_rate": 0.0001576,
      "loss": 1.7514,
      "step": 266
    },
    {
      "epoch": 1.068,
      "grad_norm": 5.565486431121826,
      "learning_rate": 0.00015744,
      "loss": 1.8309,
      "step": 267
    },
    {
      "epoch": 1.072,
      "grad_norm": 4.536942005157471,
      "learning_rate": 0.00015728,
      "loss": 1.7181,
      "step": 268
    },
    {
      "epoch": 1.076,
      "grad_norm": 4.712855339050293,
      "learning_rate": 0.00015712000000000001,
      "loss": 1.7616,
      "step": 269
    },
    {
      "epoch": 1.08,
      "grad_norm": 6.785312175750732,
      "learning_rate": 0.00015696000000000002,
      "loss": 2.1313,
      "step": 270
    },
    {
      "epoch": 1.084,
      "grad_norm": 5.508609771728516,
      "learning_rate": 0.00015680000000000002,
      "loss": 1.9522,
      "step": 271
    },
    {
      "epoch": 1.088,
      "grad_norm": 5.011802673339844,
      "learning_rate": 0.00015664000000000002,
      "loss": 1.4441,
      "step": 272
    },
    {
      "epoch": 1.092,
      "grad_norm": 7.853767395019531,
      "learning_rate": 0.00015648,
      "loss": 2.1521,
      "step": 273
    },
    {
      "epoch": 1.096,
      "grad_norm": 9.192794799804688,
      "learning_rate": 0.00015632,
      "loss": 2.0909,
      "step": 274
    },
    {
      "epoch": 1.1,
      "grad_norm": 10.287554740905762,
      "learning_rate": 0.00015616000000000002,
      "loss": 2.3156,
      "step": 275
    },
    {
      "epoch": 1.104,
      "grad_norm": 5.868574142456055,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.2117,
      "step": 276
    },
    {
      "epoch": 1.108,
      "grad_norm": 8.490694999694824,
      "learning_rate": 0.00015584,
      "loss": 2.1413,
      "step": 277
    },
    {
      "epoch": 1.112,
      "grad_norm": 7.004171371459961,
      "learning_rate": 0.00015568,
      "loss": 2.2795,
      "step": 278
    },
    {
      "epoch": 1.116,
      "grad_norm": 7.059776782989502,
      "learning_rate": 0.00015552,
      "loss": 1.9529,
      "step": 279
    },
    {
      "epoch": 1.12,
      "grad_norm": 8.662093162536621,
      "learning_rate": 0.00015536,
      "loss": 2.2159,
      "step": 280
    },
    {
      "epoch": 1.124,
      "grad_norm": 7.5470967292785645,
      "learning_rate": 0.0001552,
      "loss": 1.8352,
      "step": 281
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 6.950476169586182,
      "learning_rate": 0.00015504,
      "loss": 2.1055,
      "step": 282
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 7.912735462188721,
      "learning_rate": 0.00015488,
      "loss": 2.0728,
      "step": 283
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 6.8342976570129395,
      "learning_rate": 0.00015472,
      "loss": 1.9992,
      "step": 284
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 9.627208709716797,
      "learning_rate": 0.00015456,
      "loss": 2.1171,
      "step": 285
    },
    {
      "epoch": 1.144,
      "grad_norm": 11.003297805786133,
      "learning_rate": 0.0001544,
      "loss": 2.5356,
      "step": 286
    },
    {
      "epoch": 1.148,
      "grad_norm": 5.30509614944458,
      "learning_rate": 0.00015424000000000001,
      "loss": 1.5589,
      "step": 287
    },
    {
      "epoch": 1.152,
      "grad_norm": 6.890736103057861,
      "learning_rate": 0.00015408000000000002,
      "loss": 2.0452,
      "step": 288
    },
    {
      "epoch": 1.156,
      "grad_norm": 7.612248420715332,
      "learning_rate": 0.00015392,
      "loss": 2.2817,
      "step": 289
    },
    {
      "epoch": 1.16,
      "grad_norm": 8.049529075622559,
      "learning_rate": 0.00015376000000000002,
      "loss": 2.1372,
      "step": 290
    },
    {
      "epoch": 1.164,
      "grad_norm": 7.300662040710449,
      "learning_rate": 0.00015360000000000002,
      "loss": 1.9779,
      "step": 291
    },
    {
      "epoch": 1.168,
      "grad_norm": 5.553105354309082,
      "learning_rate": 0.00015344,
      "loss": 1.9599,
      "step": 292
    },
    {
      "epoch": 1.172,
      "grad_norm": 5.6827521324157715,
      "learning_rate": 0.00015328,
      "loss": 1.9011,
      "step": 293
    },
    {
      "epoch": 1.176,
      "grad_norm": 8.121776580810547,
      "learning_rate": 0.00015312,
      "loss": 1.8845,
      "step": 294
    },
    {
      "epoch": 1.18,
      "grad_norm": 7.586585998535156,
      "learning_rate": 0.00015296000000000003,
      "loss": 2.3688,
      "step": 295
    },
    {
      "epoch": 1.184,
      "grad_norm": 7.147330284118652,
      "learning_rate": 0.0001528,
      "loss": 1.8959,
      "step": 296
    },
    {
      "epoch": 1.188,
      "grad_norm": 6.921658992767334,
      "learning_rate": 0.00015264,
      "loss": 1.7886,
      "step": 297
    },
    {
      "epoch": 1.192,
      "grad_norm": 5.6613264083862305,
      "learning_rate": 0.00015248,
      "loss": 1.7598,
      "step": 298
    },
    {
      "epoch": 1.196,
      "grad_norm": 7.025380611419678,
      "learning_rate": 0.00015232,
      "loss": 2.0847,
      "step": 299
    },
    {
      "epoch": 1.2,
      "grad_norm": 5.753090858459473,
      "learning_rate": 0.00015216,
      "loss": 1.6553,
      "step": 300
    },
    {
      "epoch": 1.204,
      "grad_norm": 7.300815105438232,
      "learning_rate": 0.000152,
      "loss": 1.9941,
      "step": 301
    },
    {
      "epoch": 1.208,
      "grad_norm": 6.810046672821045,
      "learning_rate": 0.00015184,
      "loss": 1.9689,
      "step": 302
    },
    {
      "epoch": 1.212,
      "grad_norm": 7.458855628967285,
      "learning_rate": 0.00015168,
      "loss": 2.2224,
      "step": 303
    },
    {
      "epoch": 1.216,
      "grad_norm": 6.106318473815918,
      "learning_rate": 0.00015152,
      "loss": 1.9279,
      "step": 304
    },
    {
      "epoch": 1.22,
      "grad_norm": 8.33799934387207,
      "learning_rate": 0.00015136000000000001,
      "loss": 2.2042,
      "step": 305
    },
    {
      "epoch": 1.224,
      "grad_norm": 13.660890579223633,
      "learning_rate": 0.00015120000000000002,
      "loss": 2.0256,
      "step": 306
    },
    {
      "epoch": 1.228,
      "grad_norm": 6.083114147186279,
      "learning_rate": 0.00015104,
      "loss": 1.9268,
      "step": 307
    },
    {
      "epoch": 1.232,
      "grad_norm": 6.445638179779053,
      "learning_rate": 0.00015088,
      "loss": 1.9629,
      "step": 308
    },
    {
      "epoch": 1.236,
      "grad_norm": 5.770823001861572,
      "learning_rate": 0.00015072000000000002,
      "loss": 1.7714,
      "step": 309
    },
    {
      "epoch": 1.24,
      "grad_norm": 5.791841983795166,
      "learning_rate": 0.00015056000000000002,
      "loss": 1.7176,
      "step": 310
    },
    {
      "epoch": 1.244,
      "grad_norm": 4.690688610076904,
      "learning_rate": 0.0001504,
      "loss": 1.4746,
      "step": 311
    },
    {
      "epoch": 1.248,
      "grad_norm": 6.629851818084717,
      "learning_rate": 0.00015024,
      "loss": 1.8399,
      "step": 312
    },
    {
      "epoch": 1.252,
      "grad_norm": 4.446739673614502,
      "learning_rate": 0.00015008,
      "loss": 1.487,
      "step": 313
    },
    {
      "epoch": 1.256,
      "grad_norm": 4.712885856628418,
      "learning_rate": 0.00014992000000000003,
      "loss": 1.5657,
      "step": 314
    },
    {
      "epoch": 1.26,
      "grad_norm": 8.174098014831543,
      "learning_rate": 0.00014976,
      "loss": 2.0355,
      "step": 315
    },
    {
      "epoch": 1.264,
      "grad_norm": 6.573497295379639,
      "learning_rate": 0.0001496,
      "loss": 2.1908,
      "step": 316
    },
    {
      "epoch": 1.268,
      "grad_norm": 9.455598831176758,
      "learning_rate": 0.00014944,
      "loss": 2.2561,
      "step": 317
    },
    {
      "epoch": 1.272,
      "grad_norm": 10.097123146057129,
      "learning_rate": 0.00014928,
      "loss": 1.6754,
      "step": 318
    },
    {
      "epoch": 1.276,
      "grad_norm": 9.843781471252441,
      "learning_rate": 0.00014912,
      "loss": 2.0646,
      "step": 319
    },
    {
      "epoch": 1.28,
      "grad_norm": 6.866537094116211,
      "learning_rate": 0.00014896,
      "loss": 1.5994,
      "step": 320
    },
    {
      "epoch": 1.284,
      "grad_norm": 7.327573299407959,
      "learning_rate": 0.0001488,
      "loss": 1.6417,
      "step": 321
    },
    {
      "epoch": 1.288,
      "grad_norm": 7.675360679626465,
      "learning_rate": 0.00014864,
      "loss": 1.6415,
      "step": 322
    },
    {
      "epoch": 1.292,
      "grad_norm": 7.309958457946777,
      "learning_rate": 0.00014848,
      "loss": 2.1664,
      "step": 323
    },
    {
      "epoch": 1.296,
      "grad_norm": 7.130386829376221,
      "learning_rate": 0.00014832000000000002,
      "loss": 2.0529,
      "step": 324
    },
    {
      "epoch": 1.3,
      "grad_norm": 9.573897361755371,
      "learning_rate": 0.00014816000000000002,
      "loss": 1.8575,
      "step": 325
    },
    {
      "epoch": 1.304,
      "grad_norm": 7.15645694732666,
      "learning_rate": 0.000148,
      "loss": 2.0354,
      "step": 326
    },
    {
      "epoch": 1.308,
      "grad_norm": 7.168455600738525,
      "learning_rate": 0.00014784,
      "loss": 2.2331,
      "step": 327
    },
    {
      "epoch": 1.312,
      "grad_norm": 8.931185722351074,
      "learning_rate": 0.00014768,
      "loss": 2.1943,
      "step": 328
    },
    {
      "epoch": 1.316,
      "grad_norm": 6.798892021179199,
      "learning_rate": 0.00014752000000000002,
      "loss": 2.3685,
      "step": 329
    },
    {
      "epoch": 1.32,
      "grad_norm": 10.483330726623535,
      "learning_rate": 0.00014736,
      "loss": 2.4567,
      "step": 330
    },
    {
      "epoch": 1.324,
      "grad_norm": 6.2329230308532715,
      "learning_rate": 0.0001472,
      "loss": 1.601,
      "step": 331
    },
    {
      "epoch": 1.328,
      "grad_norm": 4.956996917724609,
      "learning_rate": 0.00014704,
      "loss": 1.7239,
      "step": 332
    },
    {
      "epoch": 1.332,
      "grad_norm": 4.622799873352051,
      "learning_rate": 0.00014688000000000003,
      "loss": 1.6062,
      "step": 333
    },
    {
      "epoch": 1.336,
      "grad_norm": 6.1260881423950195,
      "learning_rate": 0.00014672,
      "loss": 1.9502,
      "step": 334
    },
    {
      "epoch": 1.34,
      "grad_norm": 5.726418495178223,
      "learning_rate": 0.00014656,
      "loss": 1.6268,
      "step": 335
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 4.899117946624756,
      "learning_rate": 0.0001464,
      "loss": 1.8077,
      "step": 336
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 7.698525428771973,
      "learning_rate": 0.00014624,
      "loss": 2.0836,
      "step": 337
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 7.312567710876465,
      "learning_rate": 0.00014608,
      "loss": 1.8366,
      "step": 338
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 7.721719741821289,
      "learning_rate": 0.00014592,
      "loss": 1.9885,
      "step": 339
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 5.90806245803833,
      "learning_rate": 0.00014576000000000001,
      "loss": 2.0126,
      "step": 340
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 6.621655464172363,
      "learning_rate": 0.00014560000000000002,
      "loss": 2.039,
      "step": 341
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 7.029352188110352,
      "learning_rate": 0.00014544,
      "loss": 1.7059,
      "step": 342
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 5.862698078155518,
      "learning_rate": 0.00014528000000000002,
      "loss": 1.934,
      "step": 343
    },
    {
      "epoch": 1.376,
      "grad_norm": 6.689715385437012,
      "learning_rate": 0.00014512000000000002,
      "loss": 2.0214,
      "step": 344
    },
    {
      "epoch": 1.38,
      "grad_norm": 7.169663906097412,
      "learning_rate": 0.00014496,
      "loss": 1.8594,
      "step": 345
    },
    {
      "epoch": 1.384,
      "grad_norm": 8.263505935668945,
      "learning_rate": 0.0001448,
      "loss": 2.3169,
      "step": 346
    },
    {
      "epoch": 1.388,
      "grad_norm": 5.184934616088867,
      "learning_rate": 0.00014464,
      "loss": 1.6344,
      "step": 347
    },
    {
      "epoch": 1.392,
      "grad_norm": 7.977652072906494,
      "learning_rate": 0.00014448000000000003,
      "loss": 2.3225,
      "step": 348
    },
    {
      "epoch": 1.396,
      "grad_norm": 6.845685958862305,
      "learning_rate": 0.00014432,
      "loss": 2.136,
      "step": 349
    },
    {
      "epoch": 1.4,
      "grad_norm": 7.330181121826172,
      "learning_rate": 0.00014416,
      "loss": 1.9326,
      "step": 350
    },
    {
      "epoch": 1.404,
      "grad_norm": 6.9149169921875,
      "learning_rate": 0.000144,
      "loss": 2.1826,
      "step": 351
    },
    {
      "epoch": 1.408,
      "grad_norm": 5.35628604888916,
      "learning_rate": 0.00014384,
      "loss": 1.9261,
      "step": 352
    },
    {
      "epoch": 1.412,
      "grad_norm": 7.704285144805908,
      "learning_rate": 0.00014368,
      "loss": 2.1016,
      "step": 353
    },
    {
      "epoch": 1.416,
      "grad_norm": 7.467400074005127,
      "learning_rate": 0.00014352,
      "loss": 2.0403,
      "step": 354
    },
    {
      "epoch": 1.42,
      "grad_norm": 8.03312873840332,
      "learning_rate": 0.00014336,
      "loss": 2.3182,
      "step": 355
    },
    {
      "epoch": 1.424,
      "grad_norm": 4.249825954437256,
      "learning_rate": 0.0001432,
      "loss": 1.5275,
      "step": 356
    },
    {
      "epoch": 1.428,
      "grad_norm": 6.388926982879639,
      "learning_rate": 0.00014303999999999999,
      "loss": 1.6542,
      "step": 357
    },
    {
      "epoch": 1.432,
      "grad_norm": 6.697959899902344,
      "learning_rate": 0.00014288000000000001,
      "loss": 1.766,
      "step": 358
    },
    {
      "epoch": 1.436,
      "grad_norm": 6.888723850250244,
      "learning_rate": 0.00014272000000000002,
      "loss": 1.9018,
      "step": 359
    },
    {
      "epoch": 1.44,
      "grad_norm": 6.319578647613525,
      "learning_rate": 0.00014256000000000002,
      "loss": 1.9719,
      "step": 360
    },
    {
      "epoch": 1.444,
      "grad_norm": 7.0562872886657715,
      "learning_rate": 0.0001424,
      "loss": 1.6997,
      "step": 361
    },
    {
      "epoch": 1.448,
      "grad_norm": 6.364173412322998,
      "learning_rate": 0.00014224000000000002,
      "loss": 1.8847,
      "step": 362
    },
    {
      "epoch": 1.452,
      "grad_norm": 6.037821292877197,
      "learning_rate": 0.00014208000000000002,
      "loss": 1.6325,
      "step": 363
    },
    {
      "epoch": 1.456,
      "grad_norm": 5.697460651397705,
      "learning_rate": 0.00014192,
      "loss": 1.5265,
      "step": 364
    },
    {
      "epoch": 1.46,
      "grad_norm": 4.966507911682129,
      "learning_rate": 0.00014176,
      "loss": 1.5468,
      "step": 365
    },
    {
      "epoch": 1.464,
      "grad_norm": 6.418218612670898,
      "learning_rate": 0.0001416,
      "loss": 1.7084,
      "step": 366
    },
    {
      "epoch": 1.468,
      "grad_norm": 5.164483070373535,
      "learning_rate": 0.00014144000000000003,
      "loss": 1.6742,
      "step": 367
    },
    {
      "epoch": 1.472,
      "grad_norm": 5.7169318199157715,
      "learning_rate": 0.00014128,
      "loss": 1.9525,
      "step": 368
    },
    {
      "epoch": 1.476,
      "grad_norm": 7.195239543914795,
      "learning_rate": 0.00014112,
      "loss": 1.8896,
      "step": 369
    },
    {
      "epoch": 1.48,
      "grad_norm": 7.203161716461182,
      "learning_rate": 0.00014096,
      "loss": 1.6972,
      "step": 370
    },
    {
      "epoch": 1.484,
      "grad_norm": 10.431048393249512,
      "learning_rate": 0.0001408,
      "loss": 2.1487,
      "step": 371
    },
    {
      "epoch": 1.488,
      "grad_norm": 7.368358612060547,
      "learning_rate": 0.00014064,
      "loss": 1.8483,
      "step": 372
    },
    {
      "epoch": 1.492,
      "grad_norm": 9.794440269470215,
      "learning_rate": 0.00014048,
      "loss": 2.0819,
      "step": 373
    },
    {
      "epoch": 1.496,
      "grad_norm": 6.026915073394775,
      "learning_rate": 0.00014032,
      "loss": 1.5129,
      "step": 374
    },
    {
      "epoch": 1.5,
      "grad_norm": 5.898965358734131,
      "learning_rate": 0.00014016,
      "loss": 1.8436,
      "step": 375
    },
    {
      "epoch": 1.504,
      "grad_norm": 12.922170639038086,
      "learning_rate": 0.00014,
      "loss": 2.1988,
      "step": 376
    },
    {
      "epoch": 1.508,
      "grad_norm": 9.522605895996094,
      "learning_rate": 0.00013984000000000002,
      "loss": 2.0411,
      "step": 377
    },
    {
      "epoch": 1.512,
      "grad_norm": 5.401339530944824,
      "learning_rate": 0.00013968000000000002,
      "loss": 1.6462,
      "step": 378
    },
    {
      "epoch": 1.516,
      "grad_norm": 6.790580749511719,
      "learning_rate": 0.00013952000000000002,
      "loss": 1.9057,
      "step": 379
    },
    {
      "epoch": 1.52,
      "grad_norm": 9.734490394592285,
      "learning_rate": 0.00013936,
      "loss": 2.1083,
      "step": 380
    },
    {
      "epoch": 1.524,
      "grad_norm": 9.89297866821289,
      "learning_rate": 0.0001392,
      "loss": 2.0851,
      "step": 381
    },
    {
      "epoch": 1.528,
      "grad_norm": 8.10142993927002,
      "learning_rate": 0.00013904000000000002,
      "loss": 1.9083,
      "step": 382
    },
    {
      "epoch": 1.532,
      "grad_norm": 5.338695049285889,
      "learning_rate": 0.00013888,
      "loss": 1.8061,
      "step": 383
    },
    {
      "epoch": 1.536,
      "grad_norm": 7.467570781707764,
      "learning_rate": 0.00013872,
      "loss": 2.1542,
      "step": 384
    },
    {
      "epoch": 1.54,
      "grad_norm": 7.266283988952637,
      "learning_rate": 0.00013856,
      "loss": 1.8663,
      "step": 385
    },
    {
      "epoch": 1.544,
      "grad_norm": 10.376792907714844,
      "learning_rate": 0.0001384,
      "loss": 2.1713,
      "step": 386
    },
    {
      "epoch": 1.548,
      "grad_norm": 8.348955154418945,
      "learning_rate": 0.00013824,
      "loss": 1.8421,
      "step": 387
    },
    {
      "epoch": 1.552,
      "grad_norm": 5.413852214813232,
      "learning_rate": 0.00013808,
      "loss": 1.6848,
      "step": 388
    },
    {
      "epoch": 1.556,
      "grad_norm": 5.4613447189331055,
      "learning_rate": 0.00013792,
      "loss": 1.914,
      "step": 389
    },
    {
      "epoch": 1.56,
      "grad_norm": 6.168673038482666,
      "learning_rate": 0.00013776,
      "loss": 1.8332,
      "step": 390
    },
    {
      "epoch": 1.564,
      "grad_norm": 5.93131160736084,
      "learning_rate": 0.00013759999999999998,
      "loss": 1.589,
      "step": 391
    },
    {
      "epoch": 1.568,
      "grad_norm": 7.771561145782471,
      "learning_rate": 0.00013744,
      "loss": 2.1354,
      "step": 392
    },
    {
      "epoch": 1.572,
      "grad_norm": 5.07252836227417,
      "learning_rate": 0.00013728000000000001,
      "loss": 1.6116,
      "step": 393
    },
    {
      "epoch": 1.576,
      "grad_norm": 6.773917198181152,
      "learning_rate": 0.00013712000000000002,
      "loss": 1.8218,
      "step": 394
    },
    {
      "epoch": 1.58,
      "grad_norm": 7.182506561279297,
      "learning_rate": 0.00013696,
      "loss": 2.0191,
      "step": 395
    },
    {
      "epoch": 1.584,
      "grad_norm": 6.214800834655762,
      "learning_rate": 0.00013680000000000002,
      "loss": 1.554,
      "step": 396
    },
    {
      "epoch": 1.588,
      "grad_norm": 4.574408054351807,
      "learning_rate": 0.00013664000000000002,
      "loss": 1.6994,
      "step": 397
    },
    {
      "epoch": 1.592,
      "grad_norm": 6.247331619262695,
      "learning_rate": 0.00013648,
      "loss": 1.9889,
      "step": 398
    },
    {
      "epoch": 1.596,
      "grad_norm": 5.888181686401367,
      "learning_rate": 0.00013632,
      "loss": 2.1995,
      "step": 399
    },
    {
      "epoch": 1.6,
      "grad_norm": 5.268586158752441,
      "learning_rate": 0.00013616,
      "loss": 1.8513,
      "step": 400
    },
    {
      "epoch": 1.604,
      "grad_norm": 8.866680145263672,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.1637,
      "step": 401
    },
    {
      "epoch": 1.608,
      "grad_norm": 5.4596076011657715,
      "learning_rate": 0.00013584,
      "loss": 1.488,
      "step": 402
    },
    {
      "epoch": 1.612,
      "grad_norm": 7.650770664215088,
      "learning_rate": 0.00013568,
      "loss": 2.2395,
      "step": 403
    },
    {
      "epoch": 1.616,
      "grad_norm": 6.161032199859619,
      "learning_rate": 0.00013552,
      "loss": 1.9734,
      "step": 404
    },
    {
      "epoch": 1.62,
      "grad_norm": 7.679971694946289,
      "learning_rate": 0.00013536,
      "loss": 2.0382,
      "step": 405
    },
    {
      "epoch": 1.624,
      "grad_norm": 4.155566692352295,
      "learning_rate": 0.0001352,
      "loss": 1.604,
      "step": 406
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 6.0661702156066895,
      "learning_rate": 0.00013504,
      "loss": 1.5967,
      "step": 407
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 5.149538993835449,
      "learning_rate": 0.00013488,
      "loss": 1.5273,
      "step": 408
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 5.721893787384033,
      "learning_rate": 0.00013472,
      "loss": 1.4481,
      "step": 409
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 5.687551021575928,
      "learning_rate": 0.00013455999999999999,
      "loss": 1.8216,
      "step": 410
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 8.859166145324707,
      "learning_rate": 0.00013440000000000001,
      "loss": 1.9866,
      "step": 411
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 7.272766590118408,
      "learning_rate": 0.00013424000000000002,
      "loss": 1.9268,
      "step": 412
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 8.424202919006348,
      "learning_rate": 0.00013408000000000002,
      "loss": 1.7658,
      "step": 413
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 6.7083306312561035,
      "learning_rate": 0.00013392,
      "loss": 1.8095,
      "step": 414
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 5.72471284866333,
      "learning_rate": 0.00013376,
      "loss": 1.5467,
      "step": 415
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 5.668639659881592,
      "learning_rate": 0.00013360000000000002,
      "loss": 1.8305,
      "step": 416
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 8.19909381866455,
      "learning_rate": 0.00013344,
      "loss": 1.9085,
      "step": 417
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 15.376572608947754,
      "learning_rate": 0.00013328,
      "loss": 2.3345,
      "step": 418
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 7.422257900238037,
      "learning_rate": 0.00013312,
      "loss": 1.8684,
      "step": 419
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 8.29081916809082,
      "learning_rate": 0.00013296,
      "loss": 1.9974,
      "step": 420
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 6.434749126434326,
      "learning_rate": 0.0001328,
      "loss": 1.8832,
      "step": 421
    },
    {
      "epoch": 1.688,
      "grad_norm": 6.045869827270508,
      "learning_rate": 0.00013264,
      "loss": 1.6986,
      "step": 422
    },
    {
      "epoch": 1.692,
      "grad_norm": 5.3184404373168945,
      "learning_rate": 0.00013248,
      "loss": 1.6408,
      "step": 423
    },
    {
      "epoch": 1.696,
      "grad_norm": 7.393991470336914,
      "learning_rate": 0.00013232,
      "loss": 1.7514,
      "step": 424
    },
    {
      "epoch": 1.7,
      "grad_norm": 5.777606010437012,
      "learning_rate": 0.00013216,
      "loss": 1.6985,
      "step": 425
    },
    {
      "epoch": 1.704,
      "grad_norm": 7.01081657409668,
      "learning_rate": 0.000132,
      "loss": 1.7612,
      "step": 426
    },
    {
      "epoch": 1.708,
      "grad_norm": 6.716073513031006,
      "learning_rate": 0.00013184,
      "loss": 2.1474,
      "step": 427
    },
    {
      "epoch": 1.712,
      "grad_norm": 5.796477317810059,
      "learning_rate": 0.00013168,
      "loss": 1.7405,
      "step": 428
    },
    {
      "epoch": 1.716,
      "grad_norm": 7.74202299118042,
      "learning_rate": 0.00013152,
      "loss": 2.0515,
      "step": 429
    },
    {
      "epoch": 1.72,
      "grad_norm": 6.146820068359375,
      "learning_rate": 0.00013136000000000002,
      "loss": 1.8557,
      "step": 430
    },
    {
      "epoch": 1.724,
      "grad_norm": 5.03961181640625,
      "learning_rate": 0.00013120000000000002,
      "loss": 1.7072,
      "step": 431
    },
    {
      "epoch": 1.728,
      "grad_norm": 5.94712495803833,
      "learning_rate": 0.00013104000000000002,
      "loss": 1.8889,
      "step": 432
    },
    {
      "epoch": 1.732,
      "grad_norm": 6.533980846405029,
      "learning_rate": 0.00013088,
      "loss": 2.0542,
      "step": 433
    },
    {
      "epoch": 1.736,
      "grad_norm": 6.042849540710449,
      "learning_rate": 0.00013072,
      "loss": 1.7136,
      "step": 434
    },
    {
      "epoch": 1.74,
      "grad_norm": 8.597199440002441,
      "learning_rate": 0.00013056000000000002,
      "loss": 1.7645,
      "step": 435
    },
    {
      "epoch": 1.744,
      "grad_norm": 5.222116470336914,
      "learning_rate": 0.0001304,
      "loss": 1.395,
      "step": 436
    },
    {
      "epoch": 1.748,
      "grad_norm": 6.5082478523254395,
      "learning_rate": 0.00013024,
      "loss": 1.9542,
      "step": 437
    },
    {
      "epoch": 1.752,
      "grad_norm": 10.499886512756348,
      "learning_rate": 0.00013008,
      "loss": 2.0892,
      "step": 438
    },
    {
      "epoch": 1.756,
      "grad_norm": 8.885937690734863,
      "learning_rate": 0.00012992,
      "loss": 2.0514,
      "step": 439
    },
    {
      "epoch": 1.76,
      "grad_norm": 4.862423419952393,
      "learning_rate": 0.00012976,
      "loss": 1.712,
      "step": 440
    },
    {
      "epoch": 1.764,
      "grad_norm": 5.005436897277832,
      "learning_rate": 0.0001296,
      "loss": 1.4963,
      "step": 441
    },
    {
      "epoch": 1.768,
      "grad_norm": 4.701839447021484,
      "learning_rate": 0.00012944,
      "loss": 1.5903,
      "step": 442
    },
    {
      "epoch": 1.772,
      "grad_norm": 5.464725494384766,
      "learning_rate": 0.00012928,
      "loss": 1.5684,
      "step": 443
    },
    {
      "epoch": 1.776,
      "grad_norm": 5.698300838470459,
      "learning_rate": 0.00012911999999999998,
      "loss": 1.6659,
      "step": 444
    },
    {
      "epoch": 1.78,
      "grad_norm": 4.686809539794922,
      "learning_rate": 0.00012896,
      "loss": 1.4739,
      "step": 445
    },
    {
      "epoch": 1.784,
      "grad_norm": 6.960489749908447,
      "learning_rate": 0.00012880000000000001,
      "loss": 1.8534,
      "step": 446
    },
    {
      "epoch": 1.788,
      "grad_norm": 10.033658027648926,
      "learning_rate": 0.00012864000000000002,
      "loss": 2.336,
      "step": 447
    },
    {
      "epoch": 1.792,
      "grad_norm": 5.098815441131592,
      "learning_rate": 0.00012848,
      "loss": 1.7818,
      "step": 448
    },
    {
      "epoch": 1.796,
      "grad_norm": 6.394221305847168,
      "learning_rate": 0.00012832,
      "loss": 1.9068,
      "step": 449
    },
    {
      "epoch": 1.8,
      "grad_norm": 6.906430721282959,
      "learning_rate": 0.00012816000000000002,
      "loss": 1.7859,
      "step": 450
    },
    {
      "epoch": 1.804,
      "grad_norm": 6.602263450622559,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.8776,
      "step": 451
    },
    {
      "epoch": 1.808,
      "grad_norm": 6.783379554748535,
      "learning_rate": 0.00012784,
      "loss": 2.1419,
      "step": 452
    },
    {
      "epoch": 1.812,
      "grad_norm": 6.160068035125732,
      "learning_rate": 0.00012768,
      "loss": 2.0052,
      "step": 453
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 7.388343811035156,
      "learning_rate": 0.00012752,
      "loss": 2.1698,
      "step": 454
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 14.125105857849121,
      "learning_rate": 0.00012736,
      "loss": 1.9427,
      "step": 455
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 4.555936336517334,
      "learning_rate": 0.0001272,
      "loss": 1.7064,
      "step": 456
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 3.890760660171509,
      "learning_rate": 0.00012704,
      "loss": 1.4044,
      "step": 457
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 4.5667829513549805,
      "learning_rate": 0.00012688,
      "loss": 1.6121,
      "step": 458
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 6.656391143798828,
      "learning_rate": 0.00012672,
      "loss": 1.7194,
      "step": 459
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 7.823883533477783,
      "learning_rate": 0.00012656,
      "loss": 1.6565,
      "step": 460
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 7.0174641609191895,
      "learning_rate": 0.0001264,
      "loss": 1.7063,
      "step": 461
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 6.384890556335449,
      "learning_rate": 0.00012624,
      "loss": 1.8205,
      "step": 462
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 8.878642082214355,
      "learning_rate": 0.00012607999999999999,
      "loss": 2.1946,
      "step": 463
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 6.297119617462158,
      "learning_rate": 0.00012592000000000001,
      "loss": 1.6066,
      "step": 464
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 8.116422653198242,
      "learning_rate": 0.00012576000000000002,
      "loss": 2.2984,
      "step": 465
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 6.405300617218018,
      "learning_rate": 0.00012560000000000002,
      "loss": 1.8438,
      "step": 466
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 5.309669017791748,
      "learning_rate": 0.00012544,
      "loss": 1.3967,
      "step": 467
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 7.564121246337891,
      "learning_rate": 0.00012528,
      "loss": 1.902,
      "step": 468
    },
    {
      "epoch": 1.876,
      "grad_norm": 8.255280494689941,
      "learning_rate": 0.00012512000000000002,
      "loss": 1.8699,
      "step": 469
    },
    {
      "epoch": 1.88,
      "grad_norm": 6.010495185852051,
      "learning_rate": 0.00012496000000000002,
      "loss": 1.8008,
      "step": 470
    },
    {
      "epoch": 1.884,
      "grad_norm": 8.868429183959961,
      "learning_rate": 0.0001248,
      "loss": 2.0691,
      "step": 471
    },
    {
      "epoch": 1.888,
      "grad_norm": 5.6282148361206055,
      "learning_rate": 0.00012464,
      "loss": 1.553,
      "step": 472
    },
    {
      "epoch": 1.892,
      "grad_norm": 5.099389553070068,
      "learning_rate": 0.00012448,
      "loss": 1.7195,
      "step": 473
    },
    {
      "epoch": 1.896,
      "grad_norm": 7.815262317657471,
      "learning_rate": 0.00012432,
      "loss": 1.9808,
      "step": 474
    },
    {
      "epoch": 1.9,
      "grad_norm": 5.5088911056518555,
      "learning_rate": 0.00012416,
      "loss": 1.809,
      "step": 475
    },
    {
      "epoch": 1.904,
      "grad_norm": 4.784985542297363,
      "learning_rate": 0.000124,
      "loss": 1.7308,
      "step": 476
    },
    {
      "epoch": 1.908,
      "grad_norm": 9.168571472167969,
      "learning_rate": 0.00012384,
      "loss": 1.926,
      "step": 477
    },
    {
      "epoch": 1.912,
      "grad_norm": 5.784032344818115,
      "learning_rate": 0.00012368,
      "loss": 1.4424,
      "step": 478
    },
    {
      "epoch": 1.916,
      "grad_norm": 10.995197296142578,
      "learning_rate": 0.00012352,
      "loss": 2.0017,
      "step": 479
    },
    {
      "epoch": 1.92,
      "grad_norm": 7.900128364562988,
      "learning_rate": 0.00012336,
      "loss": 1.8448,
      "step": 480
    },
    {
      "epoch": 1.924,
      "grad_norm": 5.4426751136779785,
      "learning_rate": 0.0001232,
      "loss": 1.6867,
      "step": 481
    },
    {
      "epoch": 1.928,
      "grad_norm": 7.972498893737793,
      "learning_rate": 0.00012304,
      "loss": 2.2253,
      "step": 482
    },
    {
      "epoch": 1.932,
      "grad_norm": 7.31363582611084,
      "learning_rate": 0.00012288,
      "loss": 2.0686,
      "step": 483
    },
    {
      "epoch": 1.936,
      "grad_norm": 6.73394775390625,
      "learning_rate": 0.00012272000000000002,
      "loss": 1.794,
      "step": 484
    },
    {
      "epoch": 1.94,
      "grad_norm": 7.7990946769714355,
      "learning_rate": 0.00012256000000000002,
      "loss": 1.8068,
      "step": 485
    },
    {
      "epoch": 1.944,
      "grad_norm": 7.139718532562256,
      "learning_rate": 0.0001224,
      "loss": 1.6588,
      "step": 486
    },
    {
      "epoch": 1.948,
      "grad_norm": 5.756583213806152,
      "learning_rate": 0.00012224,
      "loss": 1.7955,
      "step": 487
    },
    {
      "epoch": 1.952,
      "grad_norm": 9.782487869262695,
      "learning_rate": 0.00012208000000000002,
      "loss": 2.0077,
      "step": 488
    },
    {
      "epoch": 1.956,
      "grad_norm": 6.614487648010254,
      "learning_rate": 0.00012192000000000001,
      "loss": 1.9703,
      "step": 489
    },
    {
      "epoch": 1.96,
      "grad_norm": 7.362422943115234,
      "learning_rate": 0.00012176000000000001,
      "loss": 2.1233,
      "step": 490
    },
    {
      "epoch": 1.964,
      "grad_norm": 6.454296112060547,
      "learning_rate": 0.0001216,
      "loss": 1.9169,
      "step": 491
    },
    {
      "epoch": 1.968,
      "grad_norm": 13.13704776763916,
      "learning_rate": 0.00012144,
      "loss": 2.028,
      "step": 492
    },
    {
      "epoch": 1.972,
      "grad_norm": 10.537186622619629,
      "learning_rate": 0.00012128000000000002,
      "loss": 2.0859,
      "step": 493
    },
    {
      "epoch": 1.976,
      "grad_norm": 7.83820104598999,
      "learning_rate": 0.00012112,
      "loss": 2.0867,
      "step": 494
    },
    {
      "epoch": 1.98,
      "grad_norm": 5.591901779174805,
      "learning_rate": 0.00012096000000000001,
      "loss": 1.5535,
      "step": 495
    },
    {
      "epoch": 1.984,
      "grad_norm": 6.609068870544434,
      "learning_rate": 0.0001208,
      "loss": 1.675,
      "step": 496
    },
    {
      "epoch": 1.988,
      "grad_norm": 8.118403434753418,
      "learning_rate": 0.00012064,
      "loss": 2.0039,
      "step": 497
    },
    {
      "epoch": 1.992,
      "grad_norm": 7.099626541137695,
      "learning_rate": 0.00012048000000000001,
      "loss": 1.7318,
      "step": 498
    },
    {
      "epoch": 1.996,
      "grad_norm": 7.5821123123168945,
      "learning_rate": 0.00012032000000000001,
      "loss": 1.9057,
      "step": 499
    },
    {
      "epoch": 2.0,
      "grad_norm": 5.481663227081299,
      "learning_rate": 0.00012016,
      "loss": 1.9379,
      "step": 500
    },
    {
      "epoch": 2.004,
      "grad_norm": 5.940384864807129,
      "learning_rate": 0.00012,
      "loss": 1.9106,
      "step": 501
    },
    {
      "epoch": 2.008,
      "grad_norm": 5.6410417556762695,
      "learning_rate": 0.00011983999999999999,
      "loss": 1.6516,
      "step": 502
    },
    {
      "epoch": 2.012,
      "grad_norm": 6.555720329284668,
      "learning_rate": 0.00011968000000000002,
      "loss": 1.3625,
      "step": 503
    },
    {
      "epoch": 2.016,
      "grad_norm": 5.315065383911133,
      "learning_rate": 0.00011952000000000001,
      "loss": 1.9268,
      "step": 504
    },
    {
      "epoch": 2.02,
      "grad_norm": 5.868418216705322,
      "learning_rate": 0.00011936000000000001,
      "loss": 1.8343,
      "step": 505
    },
    {
      "epoch": 2.024,
      "grad_norm": 5.321111679077148,
      "learning_rate": 0.0001192,
      "loss": 1.8437,
      "step": 506
    },
    {
      "epoch": 2.028,
      "grad_norm": 6.591033935546875,
      "learning_rate": 0.00011904,
      "loss": 1.8241,
      "step": 507
    },
    {
      "epoch": 2.032,
      "grad_norm": 5.0406599044799805,
      "learning_rate": 0.00011888000000000001,
      "loss": 1.5911,
      "step": 508
    },
    {
      "epoch": 2.036,
      "grad_norm": 6.740966796875,
      "learning_rate": 0.00011872000000000002,
      "loss": 1.7437,
      "step": 509
    },
    {
      "epoch": 2.04,
      "grad_norm": 6.279397964477539,
      "learning_rate": 0.00011856,
      "loss": 1.5926,
      "step": 510
    },
    {
      "epoch": 2.044,
      "grad_norm": 7.4990034103393555,
      "learning_rate": 0.0001184,
      "loss": 1.7995,
      "step": 511
    },
    {
      "epoch": 2.048,
      "grad_norm": 10.18626880645752,
      "learning_rate": 0.00011823999999999999,
      "loss": 1.9771,
      "step": 512
    },
    {
      "epoch": 2.052,
      "grad_norm": 5.40902853012085,
      "learning_rate": 0.00011808000000000001,
      "loss": 1.6984,
      "step": 513
    },
    {
      "epoch": 2.056,
      "grad_norm": 5.391842842102051,
      "learning_rate": 0.00011792000000000001,
      "loss": 1.7518,
      "step": 514
    },
    {
      "epoch": 2.06,
      "grad_norm": 6.908221244812012,
      "learning_rate": 0.00011776,
      "loss": 1.6635,
      "step": 515
    },
    {
      "epoch": 2.064,
      "grad_norm": 5.828265190124512,
      "learning_rate": 0.0001176,
      "loss": 1.9004,
      "step": 516
    },
    {
      "epoch": 2.068,
      "grad_norm": 4.710851669311523,
      "learning_rate": 0.00011744000000000001,
      "loss": 1.4927,
      "step": 517
    },
    {
      "epoch": 2.072,
      "grad_norm": 8.613919258117676,
      "learning_rate": 0.00011728000000000002,
      "loss": 1.9284,
      "step": 518
    },
    {
      "epoch": 2.076,
      "grad_norm": 8.789811134338379,
      "learning_rate": 0.00011712,
      "loss": 1.8437,
      "step": 519
    },
    {
      "epoch": 2.08,
      "grad_norm": 7.220355987548828,
      "learning_rate": 0.00011696,
      "loss": 1.8451,
      "step": 520
    },
    {
      "epoch": 2.084,
      "grad_norm": 6.506123065948486,
      "learning_rate": 0.00011679999999999999,
      "loss": 1.729,
      "step": 521
    },
    {
      "epoch": 2.088,
      "grad_norm": 6.400182247161865,
      "learning_rate": 0.00011664000000000002,
      "loss": 1.7734,
      "step": 522
    },
    {
      "epoch": 2.092,
      "grad_norm": 5.669806957244873,
      "learning_rate": 0.00011648000000000001,
      "loss": 1.8552,
      "step": 523
    },
    {
      "epoch": 2.096,
      "grad_norm": 6.420759201049805,
      "learning_rate": 0.00011632000000000001,
      "loss": 1.819,
      "step": 524
    },
    {
      "epoch": 2.1,
      "grad_norm": 6.0876946449279785,
      "learning_rate": 0.00011616,
      "loss": 1.8644,
      "step": 525
    },
    {
      "epoch": 2.104,
      "grad_norm": 6.097690582275391,
      "learning_rate": 0.000116,
      "loss": 1.5582,
      "step": 526
    },
    {
      "epoch": 2.108,
      "grad_norm": 9.11924934387207,
      "learning_rate": 0.00011584000000000002,
      "loss": 1.9439,
      "step": 527
    },
    {
      "epoch": 2.112,
      "grad_norm": 7.889655113220215,
      "learning_rate": 0.00011568000000000002,
      "loss": 2.1211,
      "step": 528
    },
    {
      "epoch": 2.116,
      "grad_norm": 6.108984470367432,
      "learning_rate": 0.00011552,
      "loss": 1.918,
      "step": 529
    },
    {
      "epoch": 2.12,
      "grad_norm": 5.1455397605896,
      "learning_rate": 0.00011536000000000001,
      "loss": 1.5964,
      "step": 530
    },
    {
      "epoch": 2.124,
      "grad_norm": 5.656246185302734,
      "learning_rate": 0.0001152,
      "loss": 1.5861,
      "step": 531
    },
    {
      "epoch": 2.128,
      "grad_norm": 7.546778678894043,
      "learning_rate": 0.00011504000000000001,
      "loss": 1.7068,
      "step": 532
    },
    {
      "epoch": 2.132,
      "grad_norm": 6.690592288970947,
      "learning_rate": 0.00011488000000000001,
      "loss": 1.9143,
      "step": 533
    },
    {
      "epoch": 2.136,
      "grad_norm": 4.961540222167969,
      "learning_rate": 0.00011472,
      "loss": 1.6145,
      "step": 534
    },
    {
      "epoch": 2.14,
      "grad_norm": 7.163198471069336,
      "learning_rate": 0.00011456,
      "loss": 2.0499,
      "step": 535
    },
    {
      "epoch": 2.144,
      "grad_norm": 6.037426471710205,
      "learning_rate": 0.0001144,
      "loss": 1.6895,
      "step": 536
    },
    {
      "epoch": 2.148,
      "grad_norm": 7.058038234710693,
      "learning_rate": 0.00011424000000000002,
      "loss": 1.5892,
      "step": 537
    },
    {
      "epoch": 2.152,
      "grad_norm": 4.836101531982422,
      "learning_rate": 0.00011408,
      "loss": 1.5624,
      "step": 538
    },
    {
      "epoch": 2.156,
      "grad_norm": 7.997204780578613,
      "learning_rate": 0.00011392000000000001,
      "loss": 1.8494,
      "step": 539
    },
    {
      "epoch": 2.16,
      "grad_norm": 6.552596569061279,
      "learning_rate": 0.00011376,
      "loss": 1.6888,
      "step": 540
    },
    {
      "epoch": 2.164,
      "grad_norm": 6.926529884338379,
      "learning_rate": 0.0001136,
      "loss": 1.793,
      "step": 541
    },
    {
      "epoch": 2.168,
      "grad_norm": 5.2864532470703125,
      "learning_rate": 0.00011344000000000001,
      "loss": 1.5518,
      "step": 542
    },
    {
      "epoch": 2.172,
      "grad_norm": 7.652963638305664,
      "learning_rate": 0.00011328000000000001,
      "loss": 1.868,
      "step": 543
    },
    {
      "epoch": 2.176,
      "grad_norm": 5.699134349822998,
      "learning_rate": 0.00011312,
      "loss": 1.6908,
      "step": 544
    },
    {
      "epoch": 2.18,
      "grad_norm": 8.279101371765137,
      "learning_rate": 0.00011296,
      "loss": 1.8871,
      "step": 545
    },
    {
      "epoch": 2.184,
      "grad_norm": 4.9644060134887695,
      "learning_rate": 0.00011279999999999999,
      "loss": 1.5565,
      "step": 546
    },
    {
      "epoch": 2.188,
      "grad_norm": 4.9125213623046875,
      "learning_rate": 0.00011264,
      "loss": 1.5472,
      "step": 547
    },
    {
      "epoch": 2.192,
      "grad_norm": 4.369017601013184,
      "learning_rate": 0.00011248000000000001,
      "loss": 1.3954,
      "step": 548
    },
    {
      "epoch": 2.196,
      "grad_norm": 6.910228252410889,
      "learning_rate": 0.00011232000000000001,
      "loss": 1.696,
      "step": 549
    },
    {
      "epoch": 2.2,
      "grad_norm": 8.24075984954834,
      "learning_rate": 0.00011216,
      "loss": 1.6983,
      "step": 550
    },
    {
      "epoch": 2.204,
      "grad_norm": 10.498213768005371,
      "learning_rate": 0.00011200000000000001,
      "loss": 2.0202,
      "step": 551
    },
    {
      "epoch": 2.208,
      "grad_norm": 4.770895004272461,
      "learning_rate": 0.00011184000000000001,
      "loss": 1.4367,
      "step": 552
    },
    {
      "epoch": 2.212,
      "grad_norm": 5.343869209289551,
      "learning_rate": 0.00011168,
      "loss": 1.5128,
      "step": 553
    },
    {
      "epoch": 2.216,
      "grad_norm": 8.066463470458984,
      "learning_rate": 0.00011152,
      "loss": 1.9098,
      "step": 554
    },
    {
      "epoch": 2.22,
      "grad_norm": 4.8851399421691895,
      "learning_rate": 0.00011135999999999999,
      "loss": 1.6497,
      "step": 555
    },
    {
      "epoch": 2.224,
      "grad_norm": 7.342016220092773,
      "learning_rate": 0.00011120000000000002,
      "loss": 1.963,
      "step": 556
    },
    {
      "epoch": 2.228,
      "grad_norm": 7.039245128631592,
      "learning_rate": 0.00011104000000000001,
      "loss": 1.8352,
      "step": 557
    },
    {
      "epoch": 2.232,
      "grad_norm": 8.940979957580566,
      "learning_rate": 0.00011088000000000001,
      "loss": 1.7975,
      "step": 558
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 8.987855911254883,
      "learning_rate": 0.00011072,
      "loss": 2.0621,
      "step": 559
    },
    {
      "epoch": 2.24,
      "grad_norm": 5.0448408126831055,
      "learning_rate": 0.00011056,
      "loss": 1.5226,
      "step": 560
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 2.7612133026123047,
      "learning_rate": 0.00011040000000000001,
      "loss": 1.1347,
      "step": 561
    },
    {
      "epoch": 2.248,
      "grad_norm": 6.679413795471191,
      "learning_rate": 0.00011024000000000002,
      "loss": 1.9645,
      "step": 562
    },
    {
      "epoch": 2.252,
      "grad_norm": 5.325795650482178,
      "learning_rate": 0.00011008,
      "loss": 1.5788,
      "step": 563
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 6.034379482269287,
      "learning_rate": 0.00010992,
      "loss": 1.92,
      "step": 564
    },
    {
      "epoch": 2.26,
      "grad_norm": 5.288915157318115,
      "learning_rate": 0.00010975999999999999,
      "loss": 1.9389,
      "step": 565
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 4.450214862823486,
      "learning_rate": 0.00010960000000000001,
      "loss": 1.6476,
      "step": 566
    },
    {
      "epoch": 2.268,
      "grad_norm": 7.706882953643799,
      "learning_rate": 0.00010944000000000001,
      "loss": 2.0575,
      "step": 567
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 4.126708984375,
      "learning_rate": 0.00010928000000000001,
      "loss": 1.1566,
      "step": 568
    },
    {
      "epoch": 2.276,
      "grad_norm": 7.6979780197143555,
      "learning_rate": 0.00010912,
      "loss": 1.9429,
      "step": 569
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 7.465795516967773,
      "learning_rate": 0.00010896,
      "loss": 2.2388,
      "step": 570
    },
    {
      "epoch": 2.284,
      "grad_norm": 5.119113445281982,
      "learning_rate": 0.00010880000000000002,
      "loss": 1.2948,
      "step": 571
    },
    {
      "epoch": 2.288,
      "grad_norm": 6.322544097900391,
      "learning_rate": 0.00010864,
      "loss": 1.8065,
      "step": 572
    },
    {
      "epoch": 2.292,
      "grad_norm": 7.014814853668213,
      "learning_rate": 0.00010848,
      "loss": 1.8343,
      "step": 573
    },
    {
      "epoch": 2.296,
      "grad_norm": 8.148430824279785,
      "learning_rate": 0.00010831999999999999,
      "loss": 2.0524,
      "step": 574
    },
    {
      "epoch": 2.3,
      "grad_norm": 4.95649528503418,
      "learning_rate": 0.00010816,
      "loss": 1.5522,
      "step": 575
    },
    {
      "epoch": 2.304,
      "grad_norm": 6.3107008934021,
      "learning_rate": 0.00010800000000000001,
      "loss": 2.0095,
      "step": 576
    },
    {
      "epoch": 2.308,
      "grad_norm": 4.200030326843262,
      "learning_rate": 0.00010784000000000001,
      "loss": 1.3576,
      "step": 577
    },
    {
      "epoch": 2.312,
      "grad_norm": 9.218705177307129,
      "learning_rate": 0.00010768,
      "loss": 1.9586,
      "step": 578
    },
    {
      "epoch": 2.316,
      "grad_norm": 8.735410690307617,
      "learning_rate": 0.00010752,
      "loss": 2.1142,
      "step": 579
    },
    {
      "epoch": 2.32,
      "grad_norm": 5.400144100189209,
      "learning_rate": 0.00010736000000000002,
      "loss": 1.6745,
      "step": 580
    },
    {
      "epoch": 2.324,
      "grad_norm": 7.698390483856201,
      "learning_rate": 0.00010720000000000002,
      "loss": 1.9086,
      "step": 581
    },
    {
      "epoch": 2.328,
      "grad_norm": 5.415480136871338,
      "learning_rate": 0.00010704,
      "loss": 1.5526,
      "step": 582
    },
    {
      "epoch": 2.332,
      "grad_norm": 6.4591145515441895,
      "learning_rate": 0.00010688,
      "loss": 1.9125,
      "step": 583
    },
    {
      "epoch": 2.336,
      "grad_norm": 5.800990581512451,
      "learning_rate": 0.00010672,
      "loss": 1.9971,
      "step": 584
    },
    {
      "epoch": 2.34,
      "grad_norm": 7.508500099182129,
      "learning_rate": 0.00010656000000000001,
      "loss": 1.8767,
      "step": 585
    },
    {
      "epoch": 2.344,
      "grad_norm": 4.514077663421631,
      "learning_rate": 0.00010640000000000001,
      "loss": 1.5205,
      "step": 586
    },
    {
      "epoch": 2.348,
      "grad_norm": 6.313086986541748,
      "learning_rate": 0.00010624000000000001,
      "loss": 1.6141,
      "step": 587
    },
    {
      "epoch": 2.352,
      "grad_norm": 6.794745445251465,
      "learning_rate": 0.00010608,
      "loss": 1.8565,
      "step": 588
    },
    {
      "epoch": 2.356,
      "grad_norm": 5.929198741912842,
      "learning_rate": 0.00010592,
      "loss": 1.8224,
      "step": 589
    },
    {
      "epoch": 2.36,
      "grad_norm": 4.432093143463135,
      "learning_rate": 0.00010576000000000002,
      "loss": 1.4988,
      "step": 590
    },
    {
      "epoch": 2.364,
      "grad_norm": 6.612326145172119,
      "learning_rate": 0.0001056,
      "loss": 1.8017,
      "step": 591
    },
    {
      "epoch": 2.368,
      "grad_norm": 5.547219753265381,
      "learning_rate": 0.00010544000000000001,
      "loss": 1.7282,
      "step": 592
    },
    {
      "epoch": 2.372,
      "grad_norm": 6.322256565093994,
      "learning_rate": 0.00010528,
      "loss": 2.0609,
      "step": 593
    },
    {
      "epoch": 2.376,
      "grad_norm": 4.910738945007324,
      "learning_rate": 0.00010512,
      "loss": 1.6831,
      "step": 594
    },
    {
      "epoch": 2.38,
      "grad_norm": 5.580610752105713,
      "learning_rate": 0.00010496000000000001,
      "loss": 1.6301,
      "step": 595
    },
    {
      "epoch": 2.384,
      "grad_norm": 7.523212909698486,
      "learning_rate": 0.00010480000000000001,
      "loss": 1.6267,
      "step": 596
    },
    {
      "epoch": 2.388,
      "grad_norm": 7.6045684814453125,
      "learning_rate": 0.00010464,
      "loss": 1.8778,
      "step": 597
    },
    {
      "epoch": 2.392,
      "grad_norm": 10.156129837036133,
      "learning_rate": 0.00010448,
      "loss": 1.9648,
      "step": 598
    },
    {
      "epoch": 2.396,
      "grad_norm": 8.245152473449707,
      "learning_rate": 0.00010431999999999999,
      "loss": 1.7875,
      "step": 599
    },
    {
      "epoch": 2.4,
      "grad_norm": 8.152165412902832,
      "learning_rate": 0.00010416000000000002,
      "loss": 1.8336,
      "step": 600
    },
    {
      "epoch": 2.404,
      "grad_norm": 6.7381205558776855,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.8908,
      "step": 601
    },
    {
      "epoch": 2.408,
      "grad_norm": 5.389423847198486,
      "learning_rate": 0.00010384000000000001,
      "loss": 1.3234,
      "step": 602
    },
    {
      "epoch": 2.412,
      "grad_norm": 9.296747207641602,
      "learning_rate": 0.00010368,
      "loss": 2.0755,
      "step": 603
    },
    {
      "epoch": 2.416,
      "grad_norm": 6.790730953216553,
      "learning_rate": 0.00010352,
      "loss": 1.5685,
      "step": 604
    },
    {
      "epoch": 2.42,
      "grad_norm": 5.952898025512695,
      "learning_rate": 0.00010336000000000001,
      "loss": 2.1008,
      "step": 605
    },
    {
      "epoch": 2.424,
      "grad_norm": 6.54784631729126,
      "learning_rate": 0.0001032,
      "loss": 1.576,
      "step": 606
    },
    {
      "epoch": 2.428,
      "grad_norm": 4.53102445602417,
      "learning_rate": 0.00010304,
      "loss": 1.4433,
      "step": 607
    },
    {
      "epoch": 2.432,
      "grad_norm": 7.038459777832031,
      "learning_rate": 0.00010288,
      "loss": 1.8708,
      "step": 608
    },
    {
      "epoch": 2.436,
      "grad_norm": 4.664323806762695,
      "learning_rate": 0.00010271999999999999,
      "loss": 1.7078,
      "step": 609
    },
    {
      "epoch": 2.44,
      "grad_norm": 6.304579257965088,
      "learning_rate": 0.00010256000000000001,
      "loss": 1.667,
      "step": 610
    },
    {
      "epoch": 2.444,
      "grad_norm": 5.997839450836182,
      "learning_rate": 0.00010240000000000001,
      "loss": 1.6014,
      "step": 611
    },
    {
      "epoch": 2.448,
      "grad_norm": 6.037712097167969,
      "learning_rate": 0.00010224,
      "loss": 1.5506,
      "step": 612
    },
    {
      "epoch": 2.452,
      "grad_norm": 5.765015602111816,
      "learning_rate": 0.00010208,
      "loss": 1.9242,
      "step": 613
    },
    {
      "epoch": 2.456,
      "grad_norm": 6.4838690757751465,
      "learning_rate": 0.00010192000000000001,
      "loss": 1.7316,
      "step": 614
    },
    {
      "epoch": 2.46,
      "grad_norm": 6.743500709533691,
      "learning_rate": 0.00010176000000000002,
      "loss": 1.8357,
      "step": 615
    },
    {
      "epoch": 2.464,
      "grad_norm": 7.221778392791748,
      "learning_rate": 0.0001016,
      "loss": 1.927,
      "step": 616
    },
    {
      "epoch": 2.468,
      "grad_norm": 5.617276191711426,
      "learning_rate": 0.00010144,
      "loss": 1.9018,
      "step": 617
    },
    {
      "epoch": 2.472,
      "grad_norm": 8.180266380310059,
      "learning_rate": 0.00010127999999999999,
      "loss": 2.0752,
      "step": 618
    },
    {
      "epoch": 2.476,
      "grad_norm": 7.3744001388549805,
      "learning_rate": 0.00010112000000000002,
      "loss": 2.1412,
      "step": 619
    },
    {
      "epoch": 2.48,
      "grad_norm": 6.372687339782715,
      "learning_rate": 0.00010096000000000001,
      "loss": 1.4674,
      "step": 620
    },
    {
      "epoch": 2.484,
      "grad_norm": 7.431884288787842,
      "learning_rate": 0.00010080000000000001,
      "loss": 1.9651,
      "step": 621
    },
    {
      "epoch": 2.488,
      "grad_norm": 4.831572532653809,
      "learning_rate": 0.00010064,
      "loss": 1.566,
      "step": 622
    },
    {
      "epoch": 2.492,
      "grad_norm": 7.936460018157959,
      "learning_rate": 0.00010048,
      "loss": 1.994,
      "step": 623
    },
    {
      "epoch": 2.496,
      "grad_norm": 7.368463039398193,
      "learning_rate": 0.00010032000000000002,
      "loss": 1.6907,
      "step": 624
    },
    {
      "epoch": 2.5,
      "grad_norm": 8.870749473571777,
      "learning_rate": 0.00010016,
      "loss": 1.9842,
      "step": 625
    },
    {
      "epoch": 2.504,
      "grad_norm": 7.023390769958496,
      "learning_rate": 0.0001,
      "loss": 2.0528,
      "step": 626
    },
    {
      "epoch": 2.508,
      "grad_norm": 7.243425369262695,
      "learning_rate": 9.984e-05,
      "loss": 1.7672,
      "step": 627
    },
    {
      "epoch": 2.512,
      "grad_norm": 5.940667629241943,
      "learning_rate": 9.968000000000001e-05,
      "loss": 1.9958,
      "step": 628
    },
    {
      "epoch": 2.516,
      "grad_norm": 6.971555233001709,
      "learning_rate": 9.952e-05,
      "loss": 1.8023,
      "step": 629
    },
    {
      "epoch": 2.52,
      "grad_norm": 5.27405309677124,
      "learning_rate": 9.936000000000001e-05,
      "loss": 1.4951,
      "step": 630
    },
    {
      "epoch": 2.524,
      "grad_norm": 11.470515251159668,
      "learning_rate": 9.92e-05,
      "loss": 2.1578,
      "step": 631
    },
    {
      "epoch": 2.528,
      "grad_norm": 8.180957794189453,
      "learning_rate": 9.904e-05,
      "loss": 1.9329,
      "step": 632
    },
    {
      "epoch": 2.532,
      "grad_norm": 5.739694595336914,
      "learning_rate": 9.888e-05,
      "loss": 1.7394,
      "step": 633
    },
    {
      "epoch": 2.536,
      "grad_norm": 4.93685245513916,
      "learning_rate": 9.872e-05,
      "loss": 1.4097,
      "step": 634
    },
    {
      "epoch": 2.54,
      "grad_norm": 6.713939666748047,
      "learning_rate": 9.856e-05,
      "loss": 1.7664,
      "step": 635
    },
    {
      "epoch": 2.544,
      "grad_norm": 4.8295207023620605,
      "learning_rate": 9.84e-05,
      "loss": 1.4089,
      "step": 636
    },
    {
      "epoch": 2.548,
      "grad_norm": 7.4366655349731445,
      "learning_rate": 9.824000000000001e-05,
      "loss": 1.8673,
      "step": 637
    },
    {
      "epoch": 2.552,
      "grad_norm": 5.339713096618652,
      "learning_rate": 9.808000000000001e-05,
      "loss": 1.3261,
      "step": 638
    },
    {
      "epoch": 2.556,
      "grad_norm": 9.970913887023926,
      "learning_rate": 9.792e-05,
      "loss": 2.1842,
      "step": 639
    },
    {
      "epoch": 2.56,
      "grad_norm": 6.979682922363281,
      "learning_rate": 9.776000000000001e-05,
      "loss": 1.9901,
      "step": 640
    },
    {
      "epoch": 2.564,
      "grad_norm": 7.827946186065674,
      "learning_rate": 9.76e-05,
      "loss": 2.2156,
      "step": 641
    },
    {
      "epoch": 2.568,
      "grad_norm": 9.358217239379883,
      "learning_rate": 9.744000000000002e-05,
      "loss": 1.7656,
      "step": 642
    },
    {
      "epoch": 2.572,
      "grad_norm": 10.310171127319336,
      "learning_rate": 9.728e-05,
      "loss": 2.1006,
      "step": 643
    },
    {
      "epoch": 2.576,
      "grad_norm": 5.138543605804443,
      "learning_rate": 9.712e-05,
      "loss": 1.7234,
      "step": 644
    },
    {
      "epoch": 2.58,
      "grad_norm": 8.248128890991211,
      "learning_rate": 9.696000000000001e-05,
      "loss": 1.8157,
      "step": 645
    },
    {
      "epoch": 2.584,
      "grad_norm": 6.173007011413574,
      "learning_rate": 9.680000000000001e-05,
      "loss": 1.5722,
      "step": 646
    },
    {
      "epoch": 2.588,
      "grad_norm": 4.726573944091797,
      "learning_rate": 9.664000000000001e-05,
      "loss": 1.544,
      "step": 647
    },
    {
      "epoch": 2.592,
      "grad_norm": 5.881770133972168,
      "learning_rate": 9.648e-05,
      "loss": 1.7582,
      "step": 648
    },
    {
      "epoch": 2.596,
      "grad_norm": 6.117801189422607,
      "learning_rate": 9.632e-05,
      "loss": 1.888,
      "step": 649
    },
    {
      "epoch": 2.6,
      "grad_norm": 7.019489765167236,
      "learning_rate": 9.616e-05,
      "loss": 1.9239,
      "step": 650
    },
    {
      "epoch": 2.604,
      "grad_norm": 5.803529262542725,
      "learning_rate": 9.6e-05,
      "loss": 1.6966,
      "step": 651
    },
    {
      "epoch": 2.608,
      "grad_norm": 5.9337897300720215,
      "learning_rate": 9.584e-05,
      "loss": 2.0036,
      "step": 652
    },
    {
      "epoch": 2.612,
      "grad_norm": 5.3170647621154785,
      "learning_rate": 9.568e-05,
      "loss": 1.4813,
      "step": 653
    },
    {
      "epoch": 2.616,
      "grad_norm": 5.723328590393066,
      "learning_rate": 9.552000000000001e-05,
      "loss": 1.8513,
      "step": 654
    },
    {
      "epoch": 2.62,
      "grad_norm": 5.78199577331543,
      "learning_rate": 9.536000000000001e-05,
      "loss": 1.8193,
      "step": 655
    },
    {
      "epoch": 2.624,
      "grad_norm": 5.846887588500977,
      "learning_rate": 9.52e-05,
      "loss": 1.3699,
      "step": 656
    },
    {
      "epoch": 2.628,
      "grad_norm": 8.135455131530762,
      "learning_rate": 9.504000000000001e-05,
      "loss": 2.0514,
      "step": 657
    },
    {
      "epoch": 2.632,
      "grad_norm": 5.628015041351318,
      "learning_rate": 9.488e-05,
      "loss": 1.6283,
      "step": 658
    },
    {
      "epoch": 2.636,
      "grad_norm": 6.225338459014893,
      "learning_rate": 9.472000000000001e-05,
      "loss": 1.7398,
      "step": 659
    },
    {
      "epoch": 2.64,
      "grad_norm": 6.529741287231445,
      "learning_rate": 9.456e-05,
      "loss": 1.7688,
      "step": 660
    },
    {
      "epoch": 2.644,
      "grad_norm": 8.508491516113281,
      "learning_rate": 9.44e-05,
      "loss": 1.8955,
      "step": 661
    },
    {
      "epoch": 2.648,
      "grad_norm": 4.9705939292907715,
      "learning_rate": 9.424e-05,
      "loss": 1.4375,
      "step": 662
    },
    {
      "epoch": 2.652,
      "grad_norm": 6.078433036804199,
      "learning_rate": 9.408000000000001e-05,
      "loss": 1.8095,
      "step": 663
    },
    {
      "epoch": 2.656,
      "grad_norm": 5.74189567565918,
      "learning_rate": 9.392000000000001e-05,
      "loss": 1.8419,
      "step": 664
    },
    {
      "epoch": 2.66,
      "grad_norm": 5.077754497528076,
      "learning_rate": 9.376e-05,
      "loss": 1.6915,
      "step": 665
    },
    {
      "epoch": 2.664,
      "grad_norm": 5.349099636077881,
      "learning_rate": 9.360000000000001e-05,
      "loss": 1.8684,
      "step": 666
    },
    {
      "epoch": 2.668,
      "grad_norm": 4.091900825500488,
      "learning_rate": 9.344e-05,
      "loss": 1.2991,
      "step": 667
    },
    {
      "epoch": 2.672,
      "grad_norm": 5.393754959106445,
      "learning_rate": 9.328e-05,
      "loss": 1.4368,
      "step": 668
    },
    {
      "epoch": 2.676,
      "grad_norm": 5.7147417068481445,
      "learning_rate": 9.312e-05,
      "loss": 1.5436,
      "step": 669
    },
    {
      "epoch": 2.68,
      "grad_norm": 7.369647026062012,
      "learning_rate": 9.296e-05,
      "loss": 1.839,
      "step": 670
    },
    {
      "epoch": 2.684,
      "grad_norm": 7.612278938293457,
      "learning_rate": 9.28e-05,
      "loss": 2.0445,
      "step": 671
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 6.143947124481201,
      "learning_rate": 9.264000000000001e-05,
      "loss": 1.5792,
      "step": 672
    },
    {
      "epoch": 2.692,
      "grad_norm": 5.668621063232422,
      "learning_rate": 9.248e-05,
      "loss": 1.751,
      "step": 673
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 5.117522716522217,
      "learning_rate": 9.232000000000001e-05,
      "loss": 1.4593,
      "step": 674
    },
    {
      "epoch": 2.7,
      "grad_norm": 5.8813042640686035,
      "learning_rate": 9.216e-05,
      "loss": 1.8895,
      "step": 675
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 7.778062343597412,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.8936,
      "step": 676
    },
    {
      "epoch": 2.708,
      "grad_norm": 4.921030044555664,
      "learning_rate": 9.184e-05,
      "loss": 1.5799,
      "step": 677
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 3.1788196563720703,
      "learning_rate": 9.168e-05,
      "loss": 1.3244,
      "step": 678
    },
    {
      "epoch": 2.716,
      "grad_norm": 6.128871917724609,
      "learning_rate": 9.152e-05,
      "loss": 1.7077,
      "step": 679
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 3.79887056350708,
      "learning_rate": 9.136e-05,
      "loss": 1.5166,
      "step": 680
    },
    {
      "epoch": 2.724,
      "grad_norm": 4.999841213226318,
      "learning_rate": 9.120000000000001e-05,
      "loss": 1.4355,
      "step": 681
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 3.971025228500366,
      "learning_rate": 9.104000000000001e-05,
      "loss": 1.5334,
      "step": 682
    },
    {
      "epoch": 2.732,
      "grad_norm": 8.19605541229248,
      "learning_rate": 9.088000000000001e-05,
      "loss": 1.9871,
      "step": 683
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 7.827507019042969,
      "learning_rate": 9.072e-05,
      "loss": 1.785,
      "step": 684
    },
    {
      "epoch": 2.74,
      "grad_norm": 6.409167766571045,
      "learning_rate": 9.056e-05,
      "loss": 1.7744,
      "step": 685
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 5.059020042419434,
      "learning_rate": 9.04e-05,
      "loss": 1.5017,
      "step": 686
    },
    {
      "epoch": 2.748,
      "grad_norm": 6.597023010253906,
      "learning_rate": 9.024e-05,
      "loss": 1.7324,
      "step": 687
    },
    {
      "epoch": 2.752,
      "grad_norm": 5.8428802490234375,
      "learning_rate": 9.008e-05,
      "loss": 1.8007,
      "step": 688
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 6.536743640899658,
      "learning_rate": 8.992e-05,
      "loss": 1.7706,
      "step": 689
    },
    {
      "epoch": 2.76,
      "grad_norm": 4.427262306213379,
      "learning_rate": 8.976e-05,
      "loss": 1.4384,
      "step": 690
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 7.7749223709106445,
      "learning_rate": 8.960000000000001e-05,
      "loss": 1.9915,
      "step": 691
    },
    {
      "epoch": 2.768,
      "grad_norm": 6.335897922515869,
      "learning_rate": 8.944e-05,
      "loss": 1.6454,
      "step": 692
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 7.4387383460998535,
      "learning_rate": 8.928000000000001e-05,
      "loss": 1.8205,
      "step": 693
    },
    {
      "epoch": 2.776,
      "grad_norm": 4.919814586639404,
      "learning_rate": 8.912e-05,
      "loss": 1.4192,
      "step": 694
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 6.103670120239258,
      "learning_rate": 8.896e-05,
      "loss": 1.729,
      "step": 695
    },
    {
      "epoch": 2.784,
      "grad_norm": 5.727778911590576,
      "learning_rate": 8.88e-05,
      "loss": 1.5371,
      "step": 696
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 5.183461666107178,
      "learning_rate": 8.864e-05,
      "loss": 1.4721,
      "step": 697
    },
    {
      "epoch": 2.792,
      "grad_norm": 6.123169422149658,
      "learning_rate": 8.848e-05,
      "loss": 1.7412,
      "step": 698
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 6.390108585357666,
      "learning_rate": 8.832000000000001e-05,
      "loss": 1.8541,
      "step": 699
    },
    {
      "epoch": 2.8,
      "grad_norm": 8.941458702087402,
      "learning_rate": 8.816000000000001e-05,
      "loss": 2.0406,
      "step": 700
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 6.704306125640869,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.9682,
      "step": 701
    },
    {
      "epoch": 2.808,
      "grad_norm": 5.440185546875,
      "learning_rate": 8.784e-05,
      "loss": 1.5416,
      "step": 702
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 5.694640159606934,
      "learning_rate": 8.768e-05,
      "loss": 1.7981,
      "step": 703
    },
    {
      "epoch": 2.816,
      "grad_norm": 4.898859024047852,
      "learning_rate": 8.752e-05,
      "loss": 1.6523,
      "step": 704
    },
    {
      "epoch": 2.82,
      "grad_norm": 6.125112056732178,
      "learning_rate": 8.736e-05,
      "loss": 1.7617,
      "step": 705
    },
    {
      "epoch": 2.824,
      "grad_norm": 4.001430034637451,
      "learning_rate": 8.72e-05,
      "loss": 1.4713,
      "step": 706
    },
    {
      "epoch": 2.828,
      "grad_norm": 6.4223504066467285,
      "learning_rate": 8.704e-05,
      "loss": 1.8725,
      "step": 707
    },
    {
      "epoch": 2.832,
      "grad_norm": 5.598531723022461,
      "learning_rate": 8.688000000000001e-05,
      "loss": 1.6231,
      "step": 708
    },
    {
      "epoch": 2.836,
      "grad_norm": 8.00029468536377,
      "learning_rate": 8.672e-05,
      "loss": 2.1059,
      "step": 709
    },
    {
      "epoch": 2.84,
      "grad_norm": 5.791058540344238,
      "learning_rate": 8.656000000000001e-05,
      "loss": 1.8362,
      "step": 710
    },
    {
      "epoch": 2.844,
      "grad_norm": 5.038321495056152,
      "learning_rate": 8.64e-05,
      "loss": 1.7915,
      "step": 711
    },
    {
      "epoch": 2.848,
      "grad_norm": 6.290260314941406,
      "learning_rate": 8.624000000000001e-05,
      "loss": 1.7173,
      "step": 712
    },
    {
      "epoch": 2.852,
      "grad_norm": 6.523760795593262,
      "learning_rate": 8.608e-05,
      "loss": 1.7902,
      "step": 713
    },
    {
      "epoch": 2.856,
      "grad_norm": 6.483467102050781,
      "learning_rate": 8.592e-05,
      "loss": 1.8313,
      "step": 714
    },
    {
      "epoch": 2.86,
      "grad_norm": 4.2023749351501465,
      "learning_rate": 8.576e-05,
      "loss": 1.6206,
      "step": 715
    },
    {
      "epoch": 2.864,
      "grad_norm": 4.76747989654541,
      "learning_rate": 8.560000000000001e-05,
      "loss": 1.6538,
      "step": 716
    },
    {
      "epoch": 2.868,
      "grad_norm": 6.224123477935791,
      "learning_rate": 8.544000000000001e-05,
      "loss": 1.7577,
      "step": 717
    },
    {
      "epoch": 2.872,
      "grad_norm": 5.312840938568115,
      "learning_rate": 8.528000000000001e-05,
      "loss": 1.7245,
      "step": 718
    },
    {
      "epoch": 2.876,
      "grad_norm": 5.584693908691406,
      "learning_rate": 8.512e-05,
      "loss": 1.4761,
      "step": 719
    },
    {
      "epoch": 2.88,
      "grad_norm": 5.7326788902282715,
      "learning_rate": 8.496e-05,
      "loss": 1.5496,
      "step": 720
    },
    {
      "epoch": 2.884,
      "grad_norm": 7.893243312835693,
      "learning_rate": 8.48e-05,
      "loss": 1.928,
      "step": 721
    },
    {
      "epoch": 2.888,
      "grad_norm": 9.720199584960938,
      "learning_rate": 8.464e-05,
      "loss": 1.9061,
      "step": 722
    },
    {
      "epoch": 2.892,
      "grad_norm": 5.902457237243652,
      "learning_rate": 8.448e-05,
      "loss": 1.7874,
      "step": 723
    },
    {
      "epoch": 2.896,
      "grad_norm": 6.720553398132324,
      "learning_rate": 8.431999999999999e-05,
      "loss": 1.4816,
      "step": 724
    },
    {
      "epoch": 2.9,
      "grad_norm": 3.848830461502075,
      "learning_rate": 8.416000000000001e-05,
      "loss": 1.2889,
      "step": 725
    },
    {
      "epoch": 2.904,
      "grad_norm": 5.998596668243408,
      "learning_rate": 8.4e-05,
      "loss": 1.6641,
      "step": 726
    },
    {
      "epoch": 2.908,
      "grad_norm": 6.247623920440674,
      "learning_rate": 8.384000000000001e-05,
      "loss": 1.4838,
      "step": 727
    },
    {
      "epoch": 2.912,
      "grad_norm": 4.892543792724609,
      "learning_rate": 8.368e-05,
      "loss": 1.5817,
      "step": 728
    },
    {
      "epoch": 2.916,
      "grad_norm": 4.984796047210693,
      "learning_rate": 8.352000000000001e-05,
      "loss": 1.4882,
      "step": 729
    },
    {
      "epoch": 2.92,
      "grad_norm": 6.4225850105285645,
      "learning_rate": 8.336e-05,
      "loss": 1.5336,
      "step": 730
    },
    {
      "epoch": 2.924,
      "grad_norm": 5.012654781341553,
      "learning_rate": 8.32e-05,
      "loss": 1.5004,
      "step": 731
    },
    {
      "epoch": 2.928,
      "grad_norm": 6.854738235473633,
      "learning_rate": 8.304e-05,
      "loss": 1.899,
      "step": 732
    },
    {
      "epoch": 2.932,
      "grad_norm": 3.9190914630889893,
      "learning_rate": 8.288e-05,
      "loss": 1.4861,
      "step": 733
    },
    {
      "epoch": 2.936,
      "grad_norm": 5.80761194229126,
      "learning_rate": 8.272000000000001e-05,
      "loss": 1.679,
      "step": 734
    },
    {
      "epoch": 2.94,
      "grad_norm": 5.439258575439453,
      "learning_rate": 8.256000000000001e-05,
      "loss": 1.5832,
      "step": 735
    },
    {
      "epoch": 2.944,
      "grad_norm": 7.829578876495361,
      "learning_rate": 8.24e-05,
      "loss": 2.2027,
      "step": 736
    },
    {
      "epoch": 2.948,
      "grad_norm": 4.848813056945801,
      "learning_rate": 8.224000000000001e-05,
      "loss": 1.573,
      "step": 737
    },
    {
      "epoch": 2.952,
      "grad_norm": 7.2853498458862305,
      "learning_rate": 8.208e-05,
      "loss": 1.7306,
      "step": 738
    },
    {
      "epoch": 2.956,
      "grad_norm": 8.31292724609375,
      "learning_rate": 8.192e-05,
      "loss": 2.0341,
      "step": 739
    },
    {
      "epoch": 2.96,
      "grad_norm": 7.982420921325684,
      "learning_rate": 8.176e-05,
      "loss": 1.6863,
      "step": 740
    },
    {
      "epoch": 2.964,
      "grad_norm": 4.2724127769470215,
      "learning_rate": 8.16e-05,
      "loss": 1.5395,
      "step": 741
    },
    {
      "epoch": 2.968,
      "grad_norm": 6.870901107788086,
      "learning_rate": 8.144e-05,
      "loss": 1.6169,
      "step": 742
    },
    {
      "epoch": 2.972,
      "grad_norm": 6.91935920715332,
      "learning_rate": 8.128e-05,
      "loss": 1.836,
      "step": 743
    },
    {
      "epoch": 2.976,
      "grad_norm": 11.089381217956543,
      "learning_rate": 8.112000000000001e-05,
      "loss": 2.0989,
      "step": 744
    },
    {
      "epoch": 2.98,
      "grad_norm": 5.222934246063232,
      "learning_rate": 8.096e-05,
      "loss": 1.7373,
      "step": 745
    },
    {
      "epoch": 2.984,
      "grad_norm": 9.482598304748535,
      "learning_rate": 8.080000000000001e-05,
      "loss": 2.104,
      "step": 746
    },
    {
      "epoch": 2.988,
      "grad_norm": 5.635605335235596,
      "learning_rate": 8.064e-05,
      "loss": 1.7901,
      "step": 747
    },
    {
      "epoch": 2.992,
      "grad_norm": 6.485483169555664,
      "learning_rate": 8.048e-05,
      "loss": 1.7053,
      "step": 748
    },
    {
      "epoch": 2.996,
      "grad_norm": 6.415140151977539,
      "learning_rate": 8.032e-05,
      "loss": 1.6941,
      "step": 749
    },
    {
      "epoch": 3.0,
      "grad_norm": 8.231477737426758,
      "learning_rate": 8.016e-05,
      "loss": 1.6773,
      "step": 750
    },
    {
      "epoch": 3.004,
      "grad_norm": 4.946258544921875,
      "learning_rate": 8e-05,
      "loss": 1.4884,
      "step": 751
    },
    {
      "epoch": 3.008,
      "grad_norm": 6.7572712898254395,
      "learning_rate": 7.984000000000001e-05,
      "loss": 2.011,
      "step": 752
    },
    {
      "epoch": 3.012,
      "grad_norm": 4.345776081085205,
      "learning_rate": 7.968e-05,
      "loss": 1.5689,
      "step": 753
    },
    {
      "epoch": 3.016,
      "grad_norm": 5.309454441070557,
      "learning_rate": 7.952000000000001e-05,
      "loss": 1.4893,
      "step": 754
    },
    {
      "epoch": 3.02,
      "grad_norm": 4.792531490325928,
      "learning_rate": 7.936e-05,
      "loss": 1.5358,
      "step": 755
    },
    {
      "epoch": 3.024,
      "grad_norm": 5.80997371673584,
      "learning_rate": 7.920000000000001e-05,
      "loss": 1.8733,
      "step": 756
    },
    {
      "epoch": 3.028,
      "grad_norm": 5.369412899017334,
      "learning_rate": 7.904e-05,
      "loss": 1.4996,
      "step": 757
    },
    {
      "epoch": 3.032,
      "grad_norm": 5.254436492919922,
      "learning_rate": 7.888e-05,
      "loss": 1.596,
      "step": 758
    },
    {
      "epoch": 3.036,
      "grad_norm": 6.989182949066162,
      "learning_rate": 7.872e-05,
      "loss": 1.8491,
      "step": 759
    },
    {
      "epoch": 3.04,
      "grad_norm": 4.450595855712891,
      "learning_rate": 7.856000000000001e-05,
      "loss": 1.4544,
      "step": 760
    },
    {
      "epoch": 3.044,
      "grad_norm": 4.530455112457275,
      "learning_rate": 7.840000000000001e-05,
      "loss": 1.5678,
      "step": 761
    },
    {
      "epoch": 3.048,
      "grad_norm": 6.004306316375732,
      "learning_rate": 7.824e-05,
      "loss": 1.673,
      "step": 762
    },
    {
      "epoch": 3.052,
      "grad_norm": 4.361453533172607,
      "learning_rate": 7.808000000000001e-05,
      "loss": 1.4834,
      "step": 763
    },
    {
      "epoch": 3.056,
      "grad_norm": 6.374406814575195,
      "learning_rate": 7.792e-05,
      "loss": 1.5629,
      "step": 764
    },
    {
      "epoch": 3.06,
      "grad_norm": 6.650307655334473,
      "learning_rate": 7.776e-05,
      "loss": 1.9755,
      "step": 765
    },
    {
      "epoch": 3.064,
      "grad_norm": 5.978879451751709,
      "learning_rate": 7.76e-05,
      "loss": 1.5813,
      "step": 766
    },
    {
      "epoch": 3.068,
      "grad_norm": 6.849701881408691,
      "learning_rate": 7.744e-05,
      "loss": 1.7797,
      "step": 767
    },
    {
      "epoch": 3.072,
      "grad_norm": 7.06037712097168,
      "learning_rate": 7.728e-05,
      "loss": 1.7685,
      "step": 768
    },
    {
      "epoch": 3.076,
      "grad_norm": 4.421539783477783,
      "learning_rate": 7.712000000000001e-05,
      "loss": 1.4498,
      "step": 769
    },
    {
      "epoch": 3.08,
      "grad_norm": 8.1830415725708,
      "learning_rate": 7.696e-05,
      "loss": 1.9133,
      "step": 770
    },
    {
      "epoch": 3.084,
      "grad_norm": 6.300017833709717,
      "learning_rate": 7.680000000000001e-05,
      "loss": 1.7752,
      "step": 771
    },
    {
      "epoch": 3.088,
      "grad_norm": 8.710901260375977,
      "learning_rate": 7.664e-05,
      "loss": 1.838,
      "step": 772
    },
    {
      "epoch": 3.092,
      "grad_norm": 8.82192611694336,
      "learning_rate": 7.648000000000001e-05,
      "loss": 1.8485,
      "step": 773
    },
    {
      "epoch": 3.096,
      "grad_norm": 6.442232131958008,
      "learning_rate": 7.632e-05,
      "loss": 1.7905,
      "step": 774
    },
    {
      "epoch": 3.1,
      "grad_norm": 6.806575775146484,
      "learning_rate": 7.616e-05,
      "loss": 1.8519,
      "step": 775
    },
    {
      "epoch": 3.104,
      "grad_norm": 7.470277786254883,
      "learning_rate": 7.6e-05,
      "loss": 1.7294,
      "step": 776
    },
    {
      "epoch": 3.108,
      "grad_norm": 7.64396333694458,
      "learning_rate": 7.584e-05,
      "loss": 1.7498,
      "step": 777
    },
    {
      "epoch": 3.112,
      "grad_norm": 8.234567642211914,
      "learning_rate": 7.568000000000001e-05,
      "loss": 1.9683,
      "step": 778
    },
    {
      "epoch": 3.116,
      "grad_norm": 7.2332916259765625,
      "learning_rate": 7.552e-05,
      "loss": 1.7037,
      "step": 779
    },
    {
      "epoch": 3.12,
      "grad_norm": 9.258318901062012,
      "learning_rate": 7.536000000000001e-05,
      "loss": 2.0805,
      "step": 780
    },
    {
      "epoch": 3.124,
      "grad_norm": 6.842573642730713,
      "learning_rate": 7.52e-05,
      "loss": 2.0374,
      "step": 781
    },
    {
      "epoch": 3.128,
      "grad_norm": 8.36818790435791,
      "learning_rate": 7.504e-05,
      "loss": 1.8868,
      "step": 782
    },
    {
      "epoch": 3.132,
      "grad_norm": 9.000212669372559,
      "learning_rate": 7.488e-05,
      "loss": 2.0961,
      "step": 783
    },
    {
      "epoch": 3.136,
      "grad_norm": 7.499824523925781,
      "learning_rate": 7.472e-05,
      "loss": 1.4092,
      "step": 784
    },
    {
      "epoch": 3.14,
      "grad_norm": 5.6507110595703125,
      "learning_rate": 7.456e-05,
      "loss": 1.5249,
      "step": 785
    },
    {
      "epoch": 3.144,
      "grad_norm": 6.7969651222229,
      "learning_rate": 7.44e-05,
      "loss": 1.7158,
      "step": 786
    },
    {
      "epoch": 3.148,
      "grad_norm": 5.844050407409668,
      "learning_rate": 7.424e-05,
      "loss": 1.5318,
      "step": 787
    },
    {
      "epoch": 3.152,
      "grad_norm": 6.768030643463135,
      "learning_rate": 7.408000000000001e-05,
      "loss": 1.899,
      "step": 788
    },
    {
      "epoch": 3.156,
      "grad_norm": 5.673191547393799,
      "learning_rate": 7.392e-05,
      "loss": 1.6547,
      "step": 789
    },
    {
      "epoch": 3.16,
      "grad_norm": 6.599664211273193,
      "learning_rate": 7.376000000000001e-05,
      "loss": 1.6037,
      "step": 790
    },
    {
      "epoch": 3.164,
      "grad_norm": 6.953890800476074,
      "learning_rate": 7.36e-05,
      "loss": 1.77,
      "step": 791
    },
    {
      "epoch": 3.168,
      "grad_norm": 4.038701057434082,
      "learning_rate": 7.344000000000002e-05,
      "loss": 1.2891,
      "step": 792
    },
    {
      "epoch": 3.172,
      "grad_norm": 4.821394920349121,
      "learning_rate": 7.328e-05,
      "loss": 1.4271,
      "step": 793
    },
    {
      "epoch": 3.176,
      "grad_norm": 4.341714859008789,
      "learning_rate": 7.312e-05,
      "loss": 1.4566,
      "step": 794
    },
    {
      "epoch": 3.18,
      "grad_norm": 5.873843193054199,
      "learning_rate": 7.296e-05,
      "loss": 1.6959,
      "step": 795
    },
    {
      "epoch": 3.184,
      "grad_norm": 7.353909015655518,
      "learning_rate": 7.280000000000001e-05,
      "loss": 1.9644,
      "step": 796
    },
    {
      "epoch": 3.188,
      "grad_norm": 5.267315864562988,
      "learning_rate": 7.264000000000001e-05,
      "loss": 1.4154,
      "step": 797
    },
    {
      "epoch": 3.192,
      "grad_norm": 3.569967746734619,
      "learning_rate": 7.248e-05,
      "loss": 1.2227,
      "step": 798
    },
    {
      "epoch": 3.196,
      "grad_norm": 6.140643119812012,
      "learning_rate": 7.232e-05,
      "loss": 1.4709,
      "step": 799
    },
    {
      "epoch": 3.2,
      "grad_norm": 6.386977672576904,
      "learning_rate": 7.216e-05,
      "loss": 1.7524,
      "step": 800
    },
    {
      "epoch": 3.204,
      "grad_norm": 8.474180221557617,
      "learning_rate": 7.2e-05,
      "loss": 2.1018,
      "step": 801
    },
    {
      "epoch": 3.208,
      "grad_norm": 6.416451930999756,
      "learning_rate": 7.184e-05,
      "loss": 1.927,
      "step": 802
    },
    {
      "epoch": 3.212,
      "grad_norm": 4.18167781829834,
      "learning_rate": 7.168e-05,
      "loss": 1.3015,
      "step": 803
    },
    {
      "epoch": 3.216,
      "grad_norm": 6.216038227081299,
      "learning_rate": 7.151999999999999e-05,
      "loss": 1.6137,
      "step": 804
    },
    {
      "epoch": 3.22,
      "grad_norm": 5.380537986755371,
      "learning_rate": 7.136000000000001e-05,
      "loss": 1.6491,
      "step": 805
    },
    {
      "epoch": 3.224,
      "grad_norm": 8.591715812683105,
      "learning_rate": 7.12e-05,
      "loss": 1.7522,
      "step": 806
    },
    {
      "epoch": 3.228,
      "grad_norm": 5.350430011749268,
      "learning_rate": 7.104000000000001e-05,
      "loss": 1.463,
      "step": 807
    },
    {
      "epoch": 3.232,
      "grad_norm": 7.572851657867432,
      "learning_rate": 7.088e-05,
      "loss": 1.9463,
      "step": 808
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 6.928192615509033,
      "learning_rate": 7.072000000000001e-05,
      "loss": 1.5511,
      "step": 809
    },
    {
      "epoch": 3.24,
      "grad_norm": 6.143862724304199,
      "learning_rate": 7.056e-05,
      "loss": 1.5968,
      "step": 810
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 10.67340087890625,
      "learning_rate": 7.04e-05,
      "loss": 2.1321,
      "step": 811
    },
    {
      "epoch": 3.248,
      "grad_norm": 12.391562461853027,
      "learning_rate": 7.024e-05,
      "loss": 1.9939,
      "step": 812
    },
    {
      "epoch": 3.252,
      "grad_norm": 13.039081573486328,
      "learning_rate": 7.008e-05,
      "loss": 1.9875,
      "step": 813
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 8.419189453125,
      "learning_rate": 6.992000000000001e-05,
      "loss": 1.9896,
      "step": 814
    },
    {
      "epoch": 3.26,
      "grad_norm": 7.526015758514404,
      "learning_rate": 6.976000000000001e-05,
      "loss": 1.6543,
      "step": 815
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 8.170540809631348,
      "learning_rate": 6.96e-05,
      "loss": 1.8723,
      "step": 816
    },
    {
      "epoch": 3.268,
      "grad_norm": 7.361018657684326,
      "learning_rate": 6.944e-05,
      "loss": 1.4234,
      "step": 817
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 5.123220920562744,
      "learning_rate": 6.928e-05,
      "loss": 1.5021,
      "step": 818
    },
    {
      "epoch": 3.276,
      "grad_norm": 5.6374053955078125,
      "learning_rate": 6.912e-05,
      "loss": 1.3336,
      "step": 819
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 5.384160041809082,
      "learning_rate": 6.896e-05,
      "loss": 1.8202,
      "step": 820
    },
    {
      "epoch": 3.284,
      "grad_norm": 6.433859825134277,
      "learning_rate": 6.879999999999999e-05,
      "loss": 1.6611,
      "step": 821
    },
    {
      "epoch": 3.288,
      "grad_norm": 6.006266117095947,
      "learning_rate": 6.864000000000001e-05,
      "loss": 1.893,
      "step": 822
    },
    {
      "epoch": 3.292,
      "grad_norm": 5.36715030670166,
      "learning_rate": 6.848e-05,
      "loss": 1.7651,
      "step": 823
    },
    {
      "epoch": 3.296,
      "grad_norm": 4.958649635314941,
      "learning_rate": 6.832000000000001e-05,
      "loss": 1.7675,
      "step": 824
    },
    {
      "epoch": 3.3,
      "grad_norm": 3.4214694499969482,
      "learning_rate": 6.816e-05,
      "loss": 1.2634,
      "step": 825
    },
    {
      "epoch": 3.304,
      "grad_norm": 9.10405158996582,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.8661,
      "step": 826
    },
    {
      "epoch": 3.308,
      "grad_norm": 6.045767307281494,
      "learning_rate": 6.784e-05,
      "loss": 1.6671,
      "step": 827
    },
    {
      "epoch": 3.312,
      "grad_norm": 6.1806721687316895,
      "learning_rate": 6.768e-05,
      "loss": 1.8569,
      "step": 828
    },
    {
      "epoch": 3.316,
      "grad_norm": 4.526642799377441,
      "learning_rate": 6.752e-05,
      "loss": 1.3978,
      "step": 829
    },
    {
      "epoch": 3.32,
      "grad_norm": 4.888079643249512,
      "learning_rate": 6.736e-05,
      "loss": 1.477,
      "step": 830
    },
    {
      "epoch": 3.324,
      "grad_norm": 6.577571868896484,
      "learning_rate": 6.720000000000001e-05,
      "loss": 1.9096,
      "step": 831
    },
    {
      "epoch": 3.328,
      "grad_norm": 7.172149658203125,
      "learning_rate": 6.704000000000001e-05,
      "loss": 1.8816,
      "step": 832
    },
    {
      "epoch": 3.332,
      "grad_norm": 8.484599113464355,
      "learning_rate": 6.688e-05,
      "loss": 1.6674,
      "step": 833
    },
    {
      "epoch": 3.336,
      "grad_norm": 7.904799938201904,
      "learning_rate": 6.672e-05,
      "loss": 1.9096,
      "step": 834
    },
    {
      "epoch": 3.34,
      "grad_norm": 7.086028099060059,
      "learning_rate": 6.656e-05,
      "loss": 1.7704,
      "step": 835
    },
    {
      "epoch": 3.344,
      "grad_norm": 5.393238544464111,
      "learning_rate": 6.64e-05,
      "loss": 1.5223,
      "step": 836
    },
    {
      "epoch": 3.348,
      "grad_norm": 5.571996688842773,
      "learning_rate": 6.624e-05,
      "loss": 1.5027,
      "step": 837
    },
    {
      "epoch": 3.352,
      "grad_norm": 6.187685489654541,
      "learning_rate": 6.608e-05,
      "loss": 1.6391,
      "step": 838
    },
    {
      "epoch": 3.356,
      "grad_norm": 4.930446147918701,
      "learning_rate": 6.592e-05,
      "loss": 1.3336,
      "step": 839
    },
    {
      "epoch": 3.36,
      "grad_norm": 4.129704475402832,
      "learning_rate": 6.576e-05,
      "loss": 1.3262,
      "step": 840
    },
    {
      "epoch": 3.364,
      "grad_norm": 6.544857978820801,
      "learning_rate": 6.560000000000001e-05,
      "loss": 1.8335,
      "step": 841
    },
    {
      "epoch": 3.368,
      "grad_norm": 7.597747802734375,
      "learning_rate": 6.544e-05,
      "loss": 1.8894,
      "step": 842
    },
    {
      "epoch": 3.372,
      "grad_norm": 5.514895439147949,
      "learning_rate": 6.528000000000001e-05,
      "loss": 1.6864,
      "step": 843
    },
    {
      "epoch": 3.376,
      "grad_norm": 5.610110282897949,
      "learning_rate": 6.512e-05,
      "loss": 1.527,
      "step": 844
    },
    {
      "epoch": 3.38,
      "grad_norm": 4.389835357666016,
      "learning_rate": 6.496e-05,
      "loss": 1.5618,
      "step": 845
    },
    {
      "epoch": 3.384,
      "grad_norm": 5.296425819396973,
      "learning_rate": 6.48e-05,
      "loss": 1.3642,
      "step": 846
    },
    {
      "epoch": 3.388,
      "grad_norm": 5.67123556137085,
      "learning_rate": 6.464e-05,
      "loss": 1.8309,
      "step": 847
    },
    {
      "epoch": 3.392,
      "grad_norm": 4.531219959259033,
      "learning_rate": 6.448e-05,
      "loss": 1.3961,
      "step": 848
    },
    {
      "epoch": 3.396,
      "grad_norm": 5.157459735870361,
      "learning_rate": 6.432000000000001e-05,
      "loss": 1.4481,
      "step": 849
    },
    {
      "epoch": 3.4,
      "grad_norm": 5.786228656768799,
      "learning_rate": 6.416e-05,
      "loss": 1.9739,
      "step": 850
    },
    {
      "epoch": 3.404,
      "grad_norm": 4.985844612121582,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.5155,
      "step": 851
    },
    {
      "epoch": 3.408,
      "grad_norm": 5.589810848236084,
      "learning_rate": 6.384e-05,
      "loss": 1.6429,
      "step": 852
    },
    {
      "epoch": 3.412,
      "grad_norm": 6.012226581573486,
      "learning_rate": 6.368e-05,
      "loss": 1.8207,
      "step": 853
    },
    {
      "epoch": 3.416,
      "grad_norm": 7.221569061279297,
      "learning_rate": 6.352e-05,
      "loss": 1.8089,
      "step": 854
    },
    {
      "epoch": 3.42,
      "grad_norm": 7.630059719085693,
      "learning_rate": 6.336e-05,
      "loss": 1.4992,
      "step": 855
    },
    {
      "epoch": 3.424,
      "grad_norm": 7.773629188537598,
      "learning_rate": 6.32e-05,
      "loss": 1.7403,
      "step": 856
    },
    {
      "epoch": 3.428,
      "grad_norm": 4.810598373413086,
      "learning_rate": 6.303999999999999e-05,
      "loss": 1.5342,
      "step": 857
    },
    {
      "epoch": 3.432,
      "grad_norm": 6.4857354164123535,
      "learning_rate": 6.288000000000001e-05,
      "loss": 1.7904,
      "step": 858
    },
    {
      "epoch": 3.436,
      "grad_norm": 5.019655227661133,
      "learning_rate": 6.272e-05,
      "loss": 1.5641,
      "step": 859
    },
    {
      "epoch": 3.44,
      "grad_norm": 7.5805158615112305,
      "learning_rate": 6.256000000000001e-05,
      "loss": 2.014,
      "step": 860
    },
    {
      "epoch": 3.444,
      "grad_norm": 7.14462423324585,
      "learning_rate": 6.24e-05,
      "loss": 1.9542,
      "step": 861
    },
    {
      "epoch": 3.448,
      "grad_norm": 4.320904731750488,
      "learning_rate": 6.224e-05,
      "loss": 1.4682,
      "step": 862
    },
    {
      "epoch": 3.452,
      "grad_norm": 6.725042343139648,
      "learning_rate": 6.208e-05,
      "loss": 1.5646,
      "step": 863
    },
    {
      "epoch": 3.456,
      "grad_norm": 5.390566349029541,
      "learning_rate": 6.192e-05,
      "loss": 1.5817,
      "step": 864
    },
    {
      "epoch": 3.46,
      "grad_norm": 5.281230449676514,
      "learning_rate": 6.176e-05,
      "loss": 1.6722,
      "step": 865
    },
    {
      "epoch": 3.464,
      "grad_norm": 5.2780280113220215,
      "learning_rate": 6.16e-05,
      "loss": 1.8261,
      "step": 866
    },
    {
      "epoch": 3.468,
      "grad_norm": 9.391737937927246,
      "learning_rate": 6.144e-05,
      "loss": 1.9273,
      "step": 867
    },
    {
      "epoch": 3.472,
      "grad_norm": 8.132258415222168,
      "learning_rate": 6.128000000000001e-05,
      "loss": 1.9257,
      "step": 868
    },
    {
      "epoch": 3.476,
      "grad_norm": 4.75341796875,
      "learning_rate": 6.112e-05,
      "loss": 1.4898,
      "step": 869
    },
    {
      "epoch": 3.48,
      "grad_norm": 6.7309956550598145,
      "learning_rate": 6.0960000000000006e-05,
      "loss": 1.815,
      "step": 870
    },
    {
      "epoch": 3.484,
      "grad_norm": 9.486279487609863,
      "learning_rate": 6.08e-05,
      "loss": 1.7596,
      "step": 871
    },
    {
      "epoch": 3.488,
      "grad_norm": 7.603857517242432,
      "learning_rate": 6.064000000000001e-05,
      "loss": 1.9251,
      "step": 872
    },
    {
      "epoch": 3.492,
      "grad_norm": 5.96312141418457,
      "learning_rate": 6.0480000000000004e-05,
      "loss": 1.6398,
      "step": 873
    },
    {
      "epoch": 3.496,
      "grad_norm": 6.1076483726501465,
      "learning_rate": 6.032e-05,
      "loss": 1.6262,
      "step": 874
    },
    {
      "epoch": 3.5,
      "grad_norm": 5.108808517456055,
      "learning_rate": 6.016000000000001e-05,
      "loss": 1.4182,
      "step": 875
    },
    {
      "epoch": 3.504,
      "grad_norm": 6.305811405181885,
      "learning_rate": 6e-05,
      "loss": 1.7396,
      "step": 876
    },
    {
      "epoch": 3.508,
      "grad_norm": 8.137672424316406,
      "learning_rate": 5.984000000000001e-05,
      "loss": 2.0438,
      "step": 877
    },
    {
      "epoch": 3.512,
      "grad_norm": 4.597314357757568,
      "learning_rate": 5.9680000000000005e-05,
      "loss": 1.2942,
      "step": 878
    },
    {
      "epoch": 3.516,
      "grad_norm": 6.764189720153809,
      "learning_rate": 5.952e-05,
      "loss": 1.9127,
      "step": 879
    },
    {
      "epoch": 3.52,
      "grad_norm": 6.7533793449401855,
      "learning_rate": 5.936000000000001e-05,
      "loss": 1.8615,
      "step": 880
    },
    {
      "epoch": 3.524,
      "grad_norm": 4.9740753173828125,
      "learning_rate": 5.92e-05,
      "loss": 1.7095,
      "step": 881
    },
    {
      "epoch": 3.528,
      "grad_norm": 6.050384044647217,
      "learning_rate": 5.9040000000000004e-05,
      "loss": 1.6587,
      "step": 882
    },
    {
      "epoch": 3.532,
      "grad_norm": 9.385946273803711,
      "learning_rate": 5.888e-05,
      "loss": 1.8907,
      "step": 883
    },
    {
      "epoch": 3.536,
      "grad_norm": 6.2151384353637695,
      "learning_rate": 5.872000000000001e-05,
      "loss": 1.4501,
      "step": 884
    },
    {
      "epoch": 3.54,
      "grad_norm": 5.226980686187744,
      "learning_rate": 5.856e-05,
      "loss": 1.3844,
      "step": 885
    },
    {
      "epoch": 3.544,
      "grad_norm": 8.03925895690918,
      "learning_rate": 5.8399999999999997e-05,
      "loss": 1.9458,
      "step": 886
    },
    {
      "epoch": 3.548,
      "grad_norm": 7.391629219055176,
      "learning_rate": 5.8240000000000005e-05,
      "loss": 1.7666,
      "step": 887
    },
    {
      "epoch": 3.552,
      "grad_norm": 6.013792514801025,
      "learning_rate": 5.808e-05,
      "loss": 1.7878,
      "step": 888
    },
    {
      "epoch": 3.556,
      "grad_norm": 4.167361259460449,
      "learning_rate": 5.792000000000001e-05,
      "loss": 1.1837,
      "step": 889
    },
    {
      "epoch": 3.56,
      "grad_norm": 5.286985397338867,
      "learning_rate": 5.776e-05,
      "loss": 1.427,
      "step": 890
    },
    {
      "epoch": 3.564,
      "grad_norm": 4.606113910675049,
      "learning_rate": 5.76e-05,
      "loss": 1.5192,
      "step": 891
    },
    {
      "epoch": 3.568,
      "grad_norm": 5.8976569175720215,
      "learning_rate": 5.7440000000000006e-05,
      "loss": 1.4467,
      "step": 892
    },
    {
      "epoch": 3.572,
      "grad_norm": 6.394311904907227,
      "learning_rate": 5.728e-05,
      "loss": 1.2822,
      "step": 893
    },
    {
      "epoch": 3.576,
      "grad_norm": 6.979560852050781,
      "learning_rate": 5.712000000000001e-05,
      "loss": 1.6551,
      "step": 894
    },
    {
      "epoch": 3.58,
      "grad_norm": 7.590145111083984,
      "learning_rate": 5.6960000000000004e-05,
      "loss": 1.9391,
      "step": 895
    },
    {
      "epoch": 3.584,
      "grad_norm": 7.680620193481445,
      "learning_rate": 5.68e-05,
      "loss": 1.8018,
      "step": 896
    },
    {
      "epoch": 3.588,
      "grad_norm": 7.388815879821777,
      "learning_rate": 5.6640000000000007e-05,
      "loss": 1.6874,
      "step": 897
    },
    {
      "epoch": 3.592,
      "grad_norm": 7.879110336303711,
      "learning_rate": 5.648e-05,
      "loss": 1.9202,
      "step": 898
    },
    {
      "epoch": 3.596,
      "grad_norm": 6.963174343109131,
      "learning_rate": 5.632e-05,
      "loss": 1.3646,
      "step": 899
    },
    {
      "epoch": 3.6,
      "grad_norm": 10.242128372192383,
      "learning_rate": 5.6160000000000004e-05,
      "loss": 1.9196,
      "step": 900
    },
    {
      "epoch": 3.604,
      "grad_norm": 6.095396041870117,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.6076,
      "step": 901
    },
    {
      "epoch": 3.608,
      "grad_norm": 7.022883415222168,
      "learning_rate": 5.584e-05,
      "loss": 1.7851,
      "step": 902
    },
    {
      "epoch": 3.612,
      "grad_norm": 6.160843372344971,
      "learning_rate": 5.5679999999999995e-05,
      "loss": 1.8293,
      "step": 903
    },
    {
      "epoch": 3.616,
      "grad_norm": 7.5875630378723145,
      "learning_rate": 5.5520000000000004e-05,
      "loss": 1.8478,
      "step": 904
    },
    {
      "epoch": 3.62,
      "grad_norm": 5.159484386444092,
      "learning_rate": 5.536e-05,
      "loss": 1.5087,
      "step": 905
    },
    {
      "epoch": 3.624,
      "grad_norm": 8.67883014678955,
      "learning_rate": 5.520000000000001e-05,
      "loss": 2.0439,
      "step": 906
    },
    {
      "epoch": 3.628,
      "grad_norm": 5.8690900802612305,
      "learning_rate": 5.504e-05,
      "loss": 1.6487,
      "step": 907
    },
    {
      "epoch": 3.632,
      "grad_norm": 6.111248970031738,
      "learning_rate": 5.4879999999999996e-05,
      "loss": 1.4483,
      "step": 908
    },
    {
      "epoch": 3.636,
      "grad_norm": 8.289827346801758,
      "learning_rate": 5.4720000000000005e-05,
      "loss": 1.9814,
      "step": 909
    },
    {
      "epoch": 3.64,
      "grad_norm": 7.595183849334717,
      "learning_rate": 5.456e-05,
      "loss": 1.7909,
      "step": 910
    },
    {
      "epoch": 3.644,
      "grad_norm": 5.714905738830566,
      "learning_rate": 5.440000000000001e-05,
      "loss": 1.5381,
      "step": 911
    },
    {
      "epoch": 3.648,
      "grad_norm": 6.442798614501953,
      "learning_rate": 5.424e-05,
      "loss": 1.7598,
      "step": 912
    },
    {
      "epoch": 3.652,
      "grad_norm": 5.960470676422119,
      "learning_rate": 5.408e-05,
      "loss": 1.4738,
      "step": 913
    },
    {
      "epoch": 3.656,
      "grad_norm": 5.835057735443115,
      "learning_rate": 5.3920000000000006e-05,
      "loss": 1.4566,
      "step": 914
    },
    {
      "epoch": 3.66,
      "grad_norm": 5.795670509338379,
      "learning_rate": 5.376e-05,
      "loss": 1.4938,
      "step": 915
    },
    {
      "epoch": 3.664,
      "grad_norm": 4.325904369354248,
      "learning_rate": 5.360000000000001e-05,
      "loss": 1.4178,
      "step": 916
    },
    {
      "epoch": 3.668,
      "grad_norm": 4.833683490753174,
      "learning_rate": 5.344e-05,
      "loss": 1.5345,
      "step": 917
    },
    {
      "epoch": 3.672,
      "grad_norm": 6.830775260925293,
      "learning_rate": 5.3280000000000005e-05,
      "loss": 1.9894,
      "step": 918
    },
    {
      "epoch": 3.676,
      "grad_norm": 6.081147193908691,
      "learning_rate": 5.3120000000000006e-05,
      "loss": 1.8382,
      "step": 919
    },
    {
      "epoch": 3.68,
      "grad_norm": 5.961858749389648,
      "learning_rate": 5.296e-05,
      "loss": 1.7859,
      "step": 920
    },
    {
      "epoch": 3.684,
      "grad_norm": 6.183650970458984,
      "learning_rate": 5.28e-05,
      "loss": 1.6658,
      "step": 921
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 5.599915027618408,
      "learning_rate": 5.264e-05,
      "loss": 1.7324,
      "step": 922
    },
    {
      "epoch": 3.692,
      "grad_norm": 5.887571334838867,
      "learning_rate": 5.2480000000000006e-05,
      "loss": 1.8154,
      "step": 923
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 5.777707099914551,
      "learning_rate": 5.232e-05,
      "loss": 1.6191,
      "step": 924
    },
    {
      "epoch": 3.7,
      "grad_norm": 7.127244472503662,
      "learning_rate": 5.2159999999999995e-05,
      "loss": 1.7778,
      "step": 925
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 8.497098922729492,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.866,
      "step": 926
    },
    {
      "epoch": 3.708,
      "grad_norm": 6.810840606689453,
      "learning_rate": 5.184e-05,
      "loss": 1.8242,
      "step": 927
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 5.955816745758057,
      "learning_rate": 5.168000000000001e-05,
      "loss": 1.6271,
      "step": 928
    },
    {
      "epoch": 3.716,
      "grad_norm": 8.955551147460938,
      "learning_rate": 5.152e-05,
      "loss": 2.1173,
      "step": 929
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 5.430041313171387,
      "learning_rate": 5.1359999999999996e-05,
      "loss": 1.8346,
      "step": 930
    },
    {
      "epoch": 3.724,
      "grad_norm": 6.92194128036499,
      "learning_rate": 5.1200000000000004e-05,
      "loss": 1.8318,
      "step": 931
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 4.64628267288208,
      "learning_rate": 5.104e-05,
      "loss": 1.4599,
      "step": 932
    },
    {
      "epoch": 3.732,
      "grad_norm": 7.860032081604004,
      "learning_rate": 5.088000000000001e-05,
      "loss": 2.0689,
      "step": 933
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 6.386728286743164,
      "learning_rate": 5.072e-05,
      "loss": 1.6293,
      "step": 934
    },
    {
      "epoch": 3.74,
      "grad_norm": 4.718145370483398,
      "learning_rate": 5.056000000000001e-05,
      "loss": 1.4287,
      "step": 935
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 5.659677505493164,
      "learning_rate": 5.0400000000000005e-05,
      "loss": 1.7214,
      "step": 936
    },
    {
      "epoch": 3.748,
      "grad_norm": 7.422356128692627,
      "learning_rate": 5.024e-05,
      "loss": 1.7317,
      "step": 937
    },
    {
      "epoch": 3.752,
      "grad_norm": 7.543397903442383,
      "learning_rate": 5.008e-05,
      "loss": 1.7028,
      "step": 938
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 4.675353527069092,
      "learning_rate": 4.992e-05,
      "loss": 1.3901,
      "step": 939
    },
    {
      "epoch": 3.76,
      "grad_norm": 6.6326680183410645,
      "learning_rate": 4.976e-05,
      "loss": 1.6887,
      "step": 940
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 4.9474358558654785,
      "learning_rate": 4.96e-05,
      "loss": 1.6028,
      "step": 941
    },
    {
      "epoch": 3.768,
      "grad_norm": 5.742240905761719,
      "learning_rate": 4.944e-05,
      "loss": 1.6137,
      "step": 942
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 6.1945481300354,
      "learning_rate": 4.928e-05,
      "loss": 1.6962,
      "step": 943
    },
    {
      "epoch": 3.776,
      "grad_norm": 7.686811447143555,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 1.7165,
      "step": 944
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 5.861042022705078,
      "learning_rate": 4.896e-05,
      "loss": 1.73,
      "step": 945
    },
    {
      "epoch": 3.784,
      "grad_norm": 7.719662666320801,
      "learning_rate": 4.88e-05,
      "loss": 1.7963,
      "step": 946
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 4.26810884475708,
      "learning_rate": 4.864e-05,
      "loss": 1.4281,
      "step": 947
    },
    {
      "epoch": 3.792,
      "grad_norm": 8.880529403686523,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 1.7896,
      "step": 948
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 7.274450302124023,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 1.6655,
      "step": 949
    },
    {
      "epoch": 3.8,
      "grad_norm": 8.183011054992676,
      "learning_rate": 4.816e-05,
      "loss": 1.8831,
      "step": 950
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 5.5601043701171875,
      "learning_rate": 4.8e-05,
      "loss": 1.5635,
      "step": 951
    },
    {
      "epoch": 3.808,
      "grad_norm": 5.749622344970703,
      "learning_rate": 4.784e-05,
      "loss": 1.6621,
      "step": 952
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 6.166539192199707,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 1.6988,
      "step": 953
    },
    {
      "epoch": 3.816,
      "grad_norm": 5.611320495605469,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 1.8949,
      "step": 954
    },
    {
      "epoch": 3.82,
      "grad_norm": 5.329678535461426,
      "learning_rate": 4.736000000000001e-05,
      "loss": 1.4383,
      "step": 955
    },
    {
      "epoch": 3.824,
      "grad_norm": 7.999048709869385,
      "learning_rate": 4.72e-05,
      "loss": 1.9047,
      "step": 956
    },
    {
      "epoch": 3.828,
      "grad_norm": 7.015341758728027,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 1.8213,
      "step": 957
    },
    {
      "epoch": 3.832,
      "grad_norm": 7.0434746742248535,
      "learning_rate": 4.688e-05,
      "loss": 1.8639,
      "step": 958
    },
    {
      "epoch": 3.836,
      "grad_norm": 7.686517238616943,
      "learning_rate": 4.672e-05,
      "loss": 1.9575,
      "step": 959
    },
    {
      "epoch": 3.84,
      "grad_norm": 5.833163261413574,
      "learning_rate": 4.656e-05,
      "loss": 1.5253,
      "step": 960
    },
    {
      "epoch": 3.844,
      "grad_norm": 9.054173469543457,
      "learning_rate": 4.64e-05,
      "loss": 1.6069,
      "step": 961
    },
    {
      "epoch": 3.848,
      "grad_norm": 7.098842144012451,
      "learning_rate": 4.624e-05,
      "loss": 1.7493,
      "step": 962
    },
    {
      "epoch": 3.852,
      "grad_norm": 7.090296745300293,
      "learning_rate": 4.608e-05,
      "loss": 1.8228,
      "step": 963
    },
    {
      "epoch": 3.856,
      "grad_norm": 6.263175964355469,
      "learning_rate": 4.592e-05,
      "loss": 1.4826,
      "step": 964
    },
    {
      "epoch": 3.86,
      "grad_norm": 7.38395357131958,
      "learning_rate": 4.576e-05,
      "loss": 1.9028,
      "step": 965
    },
    {
      "epoch": 3.864,
      "grad_norm": 5.902980327606201,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 1.7322,
      "step": 966
    },
    {
      "epoch": 3.868,
      "grad_norm": 7.436840534210205,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 1.8896,
      "step": 967
    },
    {
      "epoch": 3.872,
      "grad_norm": 9.039841651916504,
      "learning_rate": 4.528e-05,
      "loss": 1.96,
      "step": 968
    },
    {
      "epoch": 3.876,
      "grad_norm": 6.234333515167236,
      "learning_rate": 4.512e-05,
      "loss": 1.7963,
      "step": 969
    },
    {
      "epoch": 3.88,
      "grad_norm": 5.573158264160156,
      "learning_rate": 4.496e-05,
      "loss": 1.734,
      "step": 970
    },
    {
      "epoch": 3.884,
      "grad_norm": 5.118061542510986,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 1.1601,
      "step": 971
    },
    {
      "epoch": 3.888,
      "grad_norm": 8.538512229919434,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 1.7252,
      "step": 972
    },
    {
      "epoch": 3.892,
      "grad_norm": 7.374067783355713,
      "learning_rate": 4.448e-05,
      "loss": 1.6433,
      "step": 973
    },
    {
      "epoch": 3.896,
      "grad_norm": 7.8475022315979,
      "learning_rate": 4.432e-05,
      "loss": 2.0181,
      "step": 974
    },
    {
      "epoch": 3.9,
      "grad_norm": 7.1127190589904785,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 1.7835,
      "step": 975
    },
    {
      "epoch": 3.904,
      "grad_norm": 5.7853546142578125,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.5283,
      "step": 976
    },
    {
      "epoch": 3.908,
      "grad_norm": 5.734272003173828,
      "learning_rate": 4.384e-05,
      "loss": 1.4107,
      "step": 977
    },
    {
      "epoch": 3.912,
      "grad_norm": 7.891539573669434,
      "learning_rate": 4.368e-05,
      "loss": 1.7417,
      "step": 978
    },
    {
      "epoch": 3.916,
      "grad_norm": 5.680882930755615,
      "learning_rate": 4.352e-05,
      "loss": 1.5838,
      "step": 979
    },
    {
      "epoch": 3.92,
      "grad_norm": 5.63358736038208,
      "learning_rate": 4.336e-05,
      "loss": 1.6578,
      "step": 980
    },
    {
      "epoch": 3.924,
      "grad_norm": 8.09692096710205,
      "learning_rate": 4.32e-05,
      "loss": 1.8391,
      "step": 981
    },
    {
      "epoch": 3.928,
      "grad_norm": 6.838116645812988,
      "learning_rate": 4.304e-05,
      "loss": 1.8651,
      "step": 982
    },
    {
      "epoch": 3.932,
      "grad_norm": 7.051968097686768,
      "learning_rate": 4.288e-05,
      "loss": 1.816,
      "step": 983
    },
    {
      "epoch": 3.936,
      "grad_norm": 8.778207778930664,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 1.7715,
      "step": 984
    },
    {
      "epoch": 3.94,
      "grad_norm": 4.91340970993042,
      "learning_rate": 4.256e-05,
      "loss": 1.4059,
      "step": 985
    },
    {
      "epoch": 3.944,
      "grad_norm": 8.18446159362793,
      "learning_rate": 4.24e-05,
      "loss": 1.9355,
      "step": 986
    },
    {
      "epoch": 3.948,
      "grad_norm": 5.726400852203369,
      "learning_rate": 4.224e-05,
      "loss": 1.5728,
      "step": 987
    },
    {
      "epoch": 3.952,
      "grad_norm": 8.222396850585938,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 1.6922,
      "step": 988
    },
    {
      "epoch": 3.956,
      "grad_norm": 8.285853385925293,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 1.7025,
      "step": 989
    },
    {
      "epoch": 3.96,
      "grad_norm": 5.325300693511963,
      "learning_rate": 4.176000000000001e-05,
      "loss": 1.4229,
      "step": 990
    },
    {
      "epoch": 3.964,
      "grad_norm": 5.027022838592529,
      "learning_rate": 4.16e-05,
      "loss": 1.453,
      "step": 991
    },
    {
      "epoch": 3.968,
      "grad_norm": 4.358437538146973,
      "learning_rate": 4.144e-05,
      "loss": 1.529,
      "step": 992
    },
    {
      "epoch": 3.972,
      "grad_norm": 8.245163917541504,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 1.7099,
      "step": 993
    },
    {
      "epoch": 3.976,
      "grad_norm": 4.650252342224121,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 1.4002,
      "step": 994
    },
    {
      "epoch": 3.98,
      "grad_norm": 6.113498687744141,
      "learning_rate": 4.096e-05,
      "loss": 1.4035,
      "step": 995
    },
    {
      "epoch": 3.984,
      "grad_norm": 5.245475769042969,
      "learning_rate": 4.08e-05,
      "loss": 1.4348,
      "step": 996
    },
    {
      "epoch": 3.988,
      "grad_norm": 6.173990726470947,
      "learning_rate": 4.064e-05,
      "loss": 2.0188,
      "step": 997
    },
    {
      "epoch": 3.992,
      "grad_norm": 7.556538105010986,
      "learning_rate": 4.048e-05,
      "loss": 1.8924,
      "step": 998
    },
    {
      "epoch": 3.996,
      "grad_norm": 7.608808517456055,
      "learning_rate": 4.032e-05,
      "loss": 1.8907,
      "step": 999
    },
    {
      "epoch": 4.0,
      "grad_norm": 4.299137592315674,
      "learning_rate": 4.016e-05,
      "loss": 1.4534,
      "step": 1000
    },
    {
      "epoch": 4.004,
      "grad_norm": 5.081075668334961,
      "learning_rate": 4e-05,
      "loss": 1.6412,
      "step": 1001
    },
    {
      "epoch": 4.008,
      "grad_norm": 5.445536136627197,
      "learning_rate": 3.984e-05,
      "loss": 1.2827,
      "step": 1002
    },
    {
      "epoch": 4.012,
      "grad_norm": 6.46303129196167,
      "learning_rate": 3.968e-05,
      "loss": 1.7964,
      "step": 1003
    },
    {
      "epoch": 4.016,
      "grad_norm": 5.1518402099609375,
      "learning_rate": 3.952e-05,
      "loss": 1.5902,
      "step": 1004
    },
    {
      "epoch": 4.02,
      "grad_norm": 7.755908966064453,
      "learning_rate": 3.936e-05,
      "loss": 1.8913,
      "step": 1005
    },
    {
      "epoch": 4.024,
      "grad_norm": 5.060297012329102,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 1.6741,
      "step": 1006
    },
    {
      "epoch": 4.028,
      "grad_norm": 6.308685779571533,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 1.5807,
      "step": 1007
    },
    {
      "epoch": 4.032,
      "grad_norm": 6.034067630767822,
      "learning_rate": 3.888e-05,
      "loss": 1.6005,
      "step": 1008
    },
    {
      "epoch": 4.036,
      "grad_norm": 8.1663818359375,
      "learning_rate": 3.872e-05,
      "loss": 1.8228,
      "step": 1009
    },
    {
      "epoch": 4.04,
      "grad_norm": 8.877409934997559,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 1.9599,
      "step": 1010
    },
    {
      "epoch": 4.044,
      "grad_norm": 5.491238594055176,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 1.7224,
      "step": 1011
    },
    {
      "epoch": 4.048,
      "grad_norm": 7.205296039581299,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 1.7928,
      "step": 1012
    },
    {
      "epoch": 4.052,
      "grad_norm": 5.101932525634766,
      "learning_rate": 3.808e-05,
      "loss": 1.4884,
      "step": 1013
    },
    {
      "epoch": 4.056,
      "grad_norm": 4.7556071281433105,
      "learning_rate": 3.792e-05,
      "loss": 1.5062,
      "step": 1014
    },
    {
      "epoch": 4.06,
      "grad_norm": 5.988677978515625,
      "learning_rate": 3.776e-05,
      "loss": 1.5327,
      "step": 1015
    },
    {
      "epoch": 4.064,
      "grad_norm": 4.375580310821533,
      "learning_rate": 3.76e-05,
      "loss": 1.3448,
      "step": 1016
    },
    {
      "epoch": 4.068,
      "grad_norm": 5.824606895446777,
      "learning_rate": 3.744e-05,
      "loss": 1.5826,
      "step": 1017
    },
    {
      "epoch": 4.072,
      "grad_norm": 7.694517612457275,
      "learning_rate": 3.728e-05,
      "loss": 1.8073,
      "step": 1018
    },
    {
      "epoch": 4.076,
      "grad_norm": 7.624997138977051,
      "learning_rate": 3.712e-05,
      "loss": 1.9155,
      "step": 1019
    },
    {
      "epoch": 4.08,
      "grad_norm": 8.759309768676758,
      "learning_rate": 3.696e-05,
      "loss": 2.0064,
      "step": 1020
    },
    {
      "epoch": 4.084,
      "grad_norm": 7.790038108825684,
      "learning_rate": 3.68e-05,
      "loss": 1.7457,
      "step": 1021
    },
    {
      "epoch": 4.088,
      "grad_norm": 6.68958044052124,
      "learning_rate": 3.664e-05,
      "loss": 1.9184,
      "step": 1022
    },
    {
      "epoch": 4.092,
      "grad_norm": 9.108938217163086,
      "learning_rate": 3.648e-05,
      "loss": 1.7481,
      "step": 1023
    },
    {
      "epoch": 4.096,
      "grad_norm": 8.40371036529541,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 1.7133,
      "step": 1024
    },
    {
      "epoch": 4.1,
      "grad_norm": 7.963809490203857,
      "learning_rate": 3.616e-05,
      "loss": 1.8284,
      "step": 1025
    },
    {
      "epoch": 4.104,
      "grad_norm": 5.376991271972656,
      "learning_rate": 3.6e-05,
      "loss": 1.549,
      "step": 1026
    },
    {
      "epoch": 4.108,
      "grad_norm": 5.1490960121154785,
      "learning_rate": 3.584e-05,
      "loss": 1.5243,
      "step": 1027
    },
    {
      "epoch": 4.112,
      "grad_norm": 7.39266300201416,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 1.7284,
      "step": 1028
    },
    {
      "epoch": 4.116,
      "grad_norm": 8.287419319152832,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 1.7694,
      "step": 1029
    },
    {
      "epoch": 4.12,
      "grad_norm": 6.120724201202393,
      "learning_rate": 3.536000000000001e-05,
      "loss": 1.7137,
      "step": 1030
    },
    {
      "epoch": 4.124,
      "grad_norm": 5.418446063995361,
      "learning_rate": 3.52e-05,
      "loss": 1.6336,
      "step": 1031
    },
    {
      "epoch": 4.128,
      "grad_norm": 5.874643802642822,
      "learning_rate": 3.504e-05,
      "loss": 1.5925,
      "step": 1032
    },
    {
      "epoch": 4.132,
      "grad_norm": 7.1431803703308105,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 1.6256,
      "step": 1033
    },
    {
      "epoch": 4.136,
      "grad_norm": 5.045904159545898,
      "learning_rate": 3.472e-05,
      "loss": 1.5219,
      "step": 1034
    },
    {
      "epoch": 4.14,
      "grad_norm": 7.971476078033447,
      "learning_rate": 3.456e-05,
      "loss": 1.8209,
      "step": 1035
    },
    {
      "epoch": 4.144,
      "grad_norm": 5.08778715133667,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 1.5083,
      "step": 1036
    },
    {
      "epoch": 4.148,
      "grad_norm": 5.503670692443848,
      "learning_rate": 3.424e-05,
      "loss": 1.5566,
      "step": 1037
    },
    {
      "epoch": 4.152,
      "grad_norm": 6.4540910720825195,
      "learning_rate": 3.408e-05,
      "loss": 1.6059,
      "step": 1038
    },
    {
      "epoch": 4.156,
      "grad_norm": 4.5922088623046875,
      "learning_rate": 3.392e-05,
      "loss": 1.406,
      "step": 1039
    },
    {
      "epoch": 4.16,
      "grad_norm": 6.059433937072754,
      "learning_rate": 3.376e-05,
      "loss": 1.6448,
      "step": 1040
    },
    {
      "epoch": 4.164,
      "grad_norm": 5.354453086853027,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 1.6376,
      "step": 1041
    },
    {
      "epoch": 4.168,
      "grad_norm": 5.716418743133545,
      "learning_rate": 3.344e-05,
      "loss": 1.5862,
      "step": 1042
    },
    {
      "epoch": 4.172,
      "grad_norm": 3.772986888885498,
      "learning_rate": 3.328e-05,
      "loss": 1.4494,
      "step": 1043
    },
    {
      "epoch": 4.176,
      "grad_norm": 6.936654567718506,
      "learning_rate": 3.312e-05,
      "loss": 1.7186,
      "step": 1044
    },
    {
      "epoch": 4.18,
      "grad_norm": 5.426888465881348,
      "learning_rate": 3.296e-05,
      "loss": 1.5389,
      "step": 1045
    },
    {
      "epoch": 4.184,
      "grad_norm": 6.71232795715332,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 1.542,
      "step": 1046
    },
    {
      "epoch": 4.188,
      "grad_norm": 7.047083854675293,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 1.8576,
      "step": 1047
    },
    {
      "epoch": 4.192,
      "grad_norm": 4.993529796600342,
      "learning_rate": 3.248e-05,
      "loss": 1.3366,
      "step": 1048
    },
    {
      "epoch": 4.196,
      "grad_norm": 6.420796871185303,
      "learning_rate": 3.232e-05,
      "loss": 1.7992,
      "step": 1049
    },
    {
      "epoch": 4.2,
      "grad_norm": 5.271730899810791,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 1.7988,
      "step": 1050
    },
    {
      "epoch": 4.204,
      "grad_norm": 7.60254430770874,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.0819,
      "step": 1051
    },
    {
      "epoch": 4.208,
      "grad_norm": 6.409995079040527,
      "learning_rate": 3.184e-05,
      "loss": 1.6728,
      "step": 1052
    },
    {
      "epoch": 4.212,
      "grad_norm": 4.608436584472656,
      "learning_rate": 3.168e-05,
      "loss": 1.4526,
      "step": 1053
    },
    {
      "epoch": 4.216,
      "grad_norm": 7.4663848876953125,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 1.9942,
      "step": 1054
    },
    {
      "epoch": 4.22,
      "grad_norm": 4.530601978302002,
      "learning_rate": 3.136e-05,
      "loss": 1.5525,
      "step": 1055
    },
    {
      "epoch": 4.224,
      "grad_norm": 8.843944549560547,
      "learning_rate": 3.12e-05,
      "loss": 1.9808,
      "step": 1056
    },
    {
      "epoch": 4.228,
      "grad_norm": 5.773837089538574,
      "learning_rate": 3.104e-05,
      "loss": 1.6488,
      "step": 1057
    },
    {
      "epoch": 4.232,
      "grad_norm": 6.718923091888428,
      "learning_rate": 3.088e-05,
      "loss": 1.6906,
      "step": 1058
    },
    {
      "epoch": 4.236,
      "grad_norm": 5.653865337371826,
      "learning_rate": 3.072e-05,
      "loss": 1.3155,
      "step": 1059
    },
    {
      "epoch": 4.24,
      "grad_norm": 8.046619415283203,
      "learning_rate": 3.056e-05,
      "loss": 2.0329,
      "step": 1060
    },
    {
      "epoch": 4.244,
      "grad_norm": 5.6031494140625,
      "learning_rate": 3.04e-05,
      "loss": 1.5494,
      "step": 1061
    },
    {
      "epoch": 4.248,
      "grad_norm": 6.264688968658447,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 1.7286,
      "step": 1062
    },
    {
      "epoch": 4.252,
      "grad_norm": 6.466996669769287,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 1.6312,
      "step": 1063
    },
    {
      "epoch": 4.256,
      "grad_norm": 5.772306442260742,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 1.5664,
      "step": 1064
    },
    {
      "epoch": 4.26,
      "grad_norm": 4.734823703765869,
      "learning_rate": 2.976e-05,
      "loss": 1.6037,
      "step": 1065
    },
    {
      "epoch": 4.264,
      "grad_norm": 6.709357261657715,
      "learning_rate": 2.96e-05,
      "loss": 1.6797,
      "step": 1066
    },
    {
      "epoch": 4.268,
      "grad_norm": 7.795626163482666,
      "learning_rate": 2.944e-05,
      "loss": 1.8878,
      "step": 1067
    },
    {
      "epoch": 4.272,
      "grad_norm": 7.298018932342529,
      "learning_rate": 2.928e-05,
      "loss": 1.9647,
      "step": 1068
    },
    {
      "epoch": 4.276,
      "grad_norm": 6.0580949783325195,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 1.5797,
      "step": 1069
    },
    {
      "epoch": 4.28,
      "grad_norm": 4.882791042327881,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 1.5455,
      "step": 1070
    },
    {
      "epoch": 4.284,
      "grad_norm": 4.848793029785156,
      "learning_rate": 2.88e-05,
      "loss": 1.5727,
      "step": 1071
    },
    {
      "epoch": 4.288,
      "grad_norm": 6.19587516784668,
      "learning_rate": 2.864e-05,
      "loss": 1.5011,
      "step": 1072
    },
    {
      "epoch": 4.292,
      "grad_norm": 8.18305492401123,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 1.5781,
      "step": 1073
    },
    {
      "epoch": 4.296,
      "grad_norm": 6.909695148468018,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 1.7386,
      "step": 1074
    },
    {
      "epoch": 4.3,
      "grad_norm": 7.228397846221924,
      "learning_rate": 2.816e-05,
      "loss": 1.758,
      "step": 1075
    },
    {
      "epoch": 4.304,
      "grad_norm": 5.864647388458252,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.5318,
      "step": 1076
    },
    {
      "epoch": 4.308,
      "grad_norm": 5.420840263366699,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 1.4261,
      "step": 1077
    },
    {
      "epoch": 4.312,
      "grad_norm": 7.056911945343018,
      "learning_rate": 2.768e-05,
      "loss": 1.8229,
      "step": 1078
    },
    {
      "epoch": 4.316,
      "grad_norm": 4.9471588134765625,
      "learning_rate": 2.752e-05,
      "loss": 1.4568,
      "step": 1079
    },
    {
      "epoch": 4.32,
      "grad_norm": 4.144167900085449,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 1.24,
      "step": 1080
    },
    {
      "epoch": 4.324,
      "grad_norm": 8.629433631896973,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 1.8018,
      "step": 1081
    },
    {
      "epoch": 4.328,
      "grad_norm": 5.139467239379883,
      "learning_rate": 2.704e-05,
      "loss": 1.482,
      "step": 1082
    },
    {
      "epoch": 4.332,
      "grad_norm": 4.714954853057861,
      "learning_rate": 2.688e-05,
      "loss": 1.5394,
      "step": 1083
    },
    {
      "epoch": 4.336,
      "grad_norm": 4.521399974822998,
      "learning_rate": 2.672e-05,
      "loss": 1.4898,
      "step": 1084
    },
    {
      "epoch": 4.34,
      "grad_norm": 6.365854263305664,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 1.8004,
      "step": 1085
    },
    {
      "epoch": 4.344,
      "grad_norm": 5.6330246925354,
      "learning_rate": 2.64e-05,
      "loss": 1.5519,
      "step": 1086
    },
    {
      "epoch": 4.348,
      "grad_norm": 6.275054931640625,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 1.6804,
      "step": 1087
    },
    {
      "epoch": 4.352,
      "grad_norm": 6.255455017089844,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 1.7707,
      "step": 1088
    },
    {
      "epoch": 4.356,
      "grad_norm": 7.999589920043945,
      "learning_rate": 2.592e-05,
      "loss": 1.7835,
      "step": 1089
    },
    {
      "epoch": 4.36,
      "grad_norm": 5.5342607498168945,
      "learning_rate": 2.576e-05,
      "loss": 1.6105,
      "step": 1090
    },
    {
      "epoch": 4.364,
      "grad_norm": 6.866153240203857,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 1.579,
      "step": 1091
    },
    {
      "epoch": 4.368,
      "grad_norm": 8.640876770019531,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 1.9477,
      "step": 1092
    },
    {
      "epoch": 4.372,
      "grad_norm": 7.084814548492432,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 1.8137,
      "step": 1093
    },
    {
      "epoch": 4.376,
      "grad_norm": 7.1647562980651855,
      "learning_rate": 2.512e-05,
      "loss": 1.9495,
      "step": 1094
    },
    {
      "epoch": 4.38,
      "grad_norm": 5.017433166503906,
      "learning_rate": 2.496e-05,
      "loss": 1.4368,
      "step": 1095
    },
    {
      "epoch": 4.384,
      "grad_norm": 7.514854907989502,
      "learning_rate": 2.48e-05,
      "loss": 1.7392,
      "step": 1096
    },
    {
      "epoch": 4.388,
      "grad_norm": 7.060683727264404,
      "learning_rate": 2.464e-05,
      "loss": 1.7889,
      "step": 1097
    },
    {
      "epoch": 4.392,
      "grad_norm": 5.714412689208984,
      "learning_rate": 2.448e-05,
      "loss": 1.6362,
      "step": 1098
    },
    {
      "epoch": 4.396,
      "grad_norm": 7.687710285186768,
      "learning_rate": 2.432e-05,
      "loss": 1.6896,
      "step": 1099
    },
    {
      "epoch": 4.4,
      "grad_norm": 6.892158031463623,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 1.9924,
      "step": 1100
    },
    {
      "epoch": 4.404,
      "grad_norm": 6.983041286468506,
      "learning_rate": 2.4e-05,
      "loss": 1.6126,
      "step": 1101
    },
    {
      "epoch": 4.408,
      "grad_norm": 7.130151271820068,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 1.618,
      "step": 1102
    },
    {
      "epoch": 4.412,
      "grad_norm": 5.833762168884277,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 1.6385,
      "step": 1103
    },
    {
      "epoch": 4.416,
      "grad_norm": 4.240564346313477,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 1.3288,
      "step": 1104
    },
    {
      "epoch": 4.42,
      "grad_norm": 4.966128826141357,
      "learning_rate": 2.336e-05,
      "loss": 1.5371,
      "step": 1105
    },
    {
      "epoch": 4.424,
      "grad_norm": 6.557406425476074,
      "learning_rate": 2.32e-05,
      "loss": 1.7484,
      "step": 1106
    },
    {
      "epoch": 4.428,
      "grad_norm": 6.858332633972168,
      "learning_rate": 2.304e-05,
      "loss": 1.6512,
      "step": 1107
    },
    {
      "epoch": 4.432,
      "grad_norm": 5.5226263999938965,
      "learning_rate": 2.288e-05,
      "loss": 1.7943,
      "step": 1108
    },
    {
      "epoch": 4.436,
      "grad_norm": 8.463549613952637,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 1.8941,
      "step": 1109
    },
    {
      "epoch": 4.44,
      "grad_norm": 7.326408863067627,
      "learning_rate": 2.256e-05,
      "loss": 1.7278,
      "step": 1110
    },
    {
      "epoch": 4.444,
      "grad_norm": 6.630512714385986,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 1.7576,
      "step": 1111
    },
    {
      "epoch": 4.448,
      "grad_norm": 6.495133876800537,
      "learning_rate": 2.224e-05,
      "loss": 1.8253,
      "step": 1112
    },
    {
      "epoch": 4.452,
      "grad_norm": 5.5007195472717285,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 1.3827,
      "step": 1113
    },
    {
      "epoch": 4.456,
      "grad_norm": 8.70595645904541,
      "learning_rate": 2.192e-05,
      "loss": 1.9449,
      "step": 1114
    },
    {
      "epoch": 4.46,
      "grad_norm": 5.104728698730469,
      "learning_rate": 2.176e-05,
      "loss": 1.444,
      "step": 1115
    },
    {
      "epoch": 4.464,
      "grad_norm": 7.184357166290283,
      "learning_rate": 2.16e-05,
      "loss": 1.8761,
      "step": 1116
    },
    {
      "epoch": 4.468,
      "grad_norm": 6.353777885437012,
      "learning_rate": 2.144e-05,
      "loss": 1.7718,
      "step": 1117
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 4.536216735839844,
      "learning_rate": 2.128e-05,
      "loss": 1.3816,
      "step": 1118
    },
    {
      "epoch": 4.476,
      "grad_norm": 7.558065414428711,
      "learning_rate": 2.112e-05,
      "loss": 1.5837,
      "step": 1119
    },
    {
      "epoch": 4.48,
      "grad_norm": 4.8406524658203125,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 1.552,
      "step": 1120
    },
    {
      "epoch": 4.484,
      "grad_norm": 7.14896297454834,
      "learning_rate": 2.08e-05,
      "loss": 1.7392,
      "step": 1121
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 4.7308878898620605,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 1.4943,
      "step": 1122
    },
    {
      "epoch": 4.492,
      "grad_norm": 6.035850524902344,
      "learning_rate": 2.048e-05,
      "loss": 1.6412,
      "step": 1123
    },
    {
      "epoch": 4.496,
      "grad_norm": 6.980470657348633,
      "learning_rate": 2.032e-05,
      "loss": 1.5774,
      "step": 1124
    },
    {
      "epoch": 4.5,
      "grad_norm": 5.144959449768066,
      "learning_rate": 2.016e-05,
      "loss": 1.6614,
      "step": 1125
    },
    {
      "epoch": 4.504,
      "grad_norm": 5.45876407623291,
      "learning_rate": 2e-05,
      "loss": 1.8195,
      "step": 1126
    },
    {
      "epoch": 4.508,
      "grad_norm": 7.078369140625,
      "learning_rate": 1.984e-05,
      "loss": 1.8149,
      "step": 1127
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 7.850695610046387,
      "learning_rate": 1.968e-05,
      "loss": 1.4356,
      "step": 1128
    },
    {
      "epoch": 4.516,
      "grad_norm": 5.936999320983887,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 1.539,
      "step": 1129
    },
    {
      "epoch": 4.52,
      "grad_norm": 7.19075870513916,
      "learning_rate": 1.936e-05,
      "loss": 1.9213,
      "step": 1130
    },
    {
      "epoch": 4.524,
      "grad_norm": 7.5501298904418945,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 1.9693,
      "step": 1131
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 6.26748514175415,
      "learning_rate": 1.904e-05,
      "loss": 1.7331,
      "step": 1132
    },
    {
      "epoch": 4.532,
      "grad_norm": 5.330255031585693,
      "learning_rate": 1.888e-05,
      "loss": 1.6017,
      "step": 1133
    },
    {
      "epoch": 4.536,
      "grad_norm": 6.317018508911133,
      "learning_rate": 1.872e-05,
      "loss": 1.5834,
      "step": 1134
    },
    {
      "epoch": 4.54,
      "grad_norm": 4.9796953201293945,
      "learning_rate": 1.856e-05,
      "loss": 1.6851,
      "step": 1135
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 6.55655574798584,
      "learning_rate": 1.84e-05,
      "loss": 1.4613,
      "step": 1136
    },
    {
      "epoch": 4.548,
      "grad_norm": 7.017050743103027,
      "learning_rate": 1.824e-05,
      "loss": 1.7468,
      "step": 1137
    },
    {
      "epoch": 4.552,
      "grad_norm": 7.232454776763916,
      "learning_rate": 1.808e-05,
      "loss": 1.6262,
      "step": 1138
    },
    {
      "epoch": 4.556,
      "grad_norm": 4.724513053894043,
      "learning_rate": 1.792e-05,
      "loss": 1.4685,
      "step": 1139
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 4.039193153381348,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 1.3677,
      "step": 1140
    },
    {
      "epoch": 4.564,
      "grad_norm": 8.243865013122559,
      "learning_rate": 1.76e-05,
      "loss": 1.7488,
      "step": 1141
    },
    {
      "epoch": 4.568,
      "grad_norm": 4.919219017028809,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 1.5081,
      "step": 1142
    },
    {
      "epoch": 4.572,
      "grad_norm": 5.167930603027344,
      "learning_rate": 1.728e-05,
      "loss": 1.3726,
      "step": 1143
    },
    {
      "epoch": 4.576,
      "grad_norm": 5.7583723068237305,
      "learning_rate": 1.712e-05,
      "loss": 1.4736,
      "step": 1144
    },
    {
      "epoch": 4.58,
      "grad_norm": 6.243908882141113,
      "learning_rate": 1.696e-05,
      "loss": 1.5124,
      "step": 1145
    },
    {
      "epoch": 4.584,
      "grad_norm": 5.435628414154053,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.6965,
      "step": 1146
    },
    {
      "epoch": 4.588,
      "grad_norm": 7.71472692489624,
      "learning_rate": 1.664e-05,
      "loss": 1.9389,
      "step": 1147
    },
    {
      "epoch": 4.592,
      "grad_norm": 5.342630386352539,
      "learning_rate": 1.648e-05,
      "loss": 1.2209,
      "step": 1148
    },
    {
      "epoch": 4.596,
      "grad_norm": 8.286938667297363,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 1.9758,
      "step": 1149
    },
    {
      "epoch": 4.6,
      "grad_norm": 6.333282470703125,
      "learning_rate": 1.616e-05,
      "loss": 1.6793,
      "step": 1150
    },
    {
      "epoch": 4.604,
      "grad_norm": 5.498342990875244,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.7003,
      "step": 1151
    },
    {
      "epoch": 4.608,
      "grad_norm": 6.746371269226074,
      "learning_rate": 1.584e-05,
      "loss": 1.8508,
      "step": 1152
    },
    {
      "epoch": 4.612,
      "grad_norm": 5.336056232452393,
      "learning_rate": 1.568e-05,
      "loss": 1.4284,
      "step": 1153
    },
    {
      "epoch": 4.616,
      "grad_norm": 7.628238201141357,
      "learning_rate": 1.552e-05,
      "loss": 2.008,
      "step": 1154
    },
    {
      "epoch": 4.62,
      "grad_norm": 4.298022747039795,
      "learning_rate": 1.536e-05,
      "loss": 1.3724,
      "step": 1155
    },
    {
      "epoch": 4.624,
      "grad_norm": 8.261798858642578,
      "learning_rate": 1.52e-05,
      "loss": 2.037,
      "step": 1156
    },
    {
      "epoch": 4.628,
      "grad_norm": 4.529121398925781,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 1.5642,
      "step": 1157
    },
    {
      "epoch": 4.632,
      "grad_norm": 5.039443492889404,
      "learning_rate": 1.488e-05,
      "loss": 1.6084,
      "step": 1158
    },
    {
      "epoch": 4.636,
      "grad_norm": 6.773736476898193,
      "learning_rate": 1.472e-05,
      "loss": 1.638,
      "step": 1159
    },
    {
      "epoch": 4.64,
      "grad_norm": 5.278020858764648,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 1.4075,
      "step": 1160
    },
    {
      "epoch": 4.644,
      "grad_norm": 6.520120143890381,
      "learning_rate": 1.44e-05,
      "loss": 1.6197,
      "step": 1161
    },
    {
      "epoch": 4.648,
      "grad_norm": 6.662570476531982,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 1.6282,
      "step": 1162
    },
    {
      "epoch": 4.652,
      "grad_norm": 5.584198474884033,
      "learning_rate": 1.408e-05,
      "loss": 1.5157,
      "step": 1163
    },
    {
      "epoch": 4.656,
      "grad_norm": 7.636212348937988,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 1.9573,
      "step": 1164
    },
    {
      "epoch": 4.66,
      "grad_norm": 6.197450637817383,
      "learning_rate": 1.376e-05,
      "loss": 1.5222,
      "step": 1165
    },
    {
      "epoch": 4.664,
      "grad_norm": 7.163811683654785,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 1.849,
      "step": 1166
    },
    {
      "epoch": 4.668,
      "grad_norm": 10.044387817382812,
      "learning_rate": 1.344e-05,
      "loss": 1.9198,
      "step": 1167
    },
    {
      "epoch": 4.672,
      "grad_norm": 5.086080551147461,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 1.6922,
      "step": 1168
    },
    {
      "epoch": 4.676,
      "grad_norm": 5.0027995109558105,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 1.6297,
      "step": 1169
    },
    {
      "epoch": 4.68,
      "grad_norm": 6.291215896606445,
      "learning_rate": 1.296e-05,
      "loss": 1.5557,
      "step": 1170
    },
    {
      "epoch": 4.684,
      "grad_norm": 8.048702239990234,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 1.8334,
      "step": 1171
    },
    {
      "epoch": 4.688,
      "grad_norm": 6.20245885848999,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 1.6682,
      "step": 1172
    },
    {
      "epoch": 4.692,
      "grad_norm": 5.1734209060668945,
      "learning_rate": 1.248e-05,
      "loss": 1.2449,
      "step": 1173
    },
    {
      "epoch": 4.696,
      "grad_norm": 6.257655143737793,
      "learning_rate": 1.232e-05,
      "loss": 1.5346,
      "step": 1174
    },
    {
      "epoch": 4.7,
      "grad_norm": 9.023614883422852,
      "learning_rate": 1.216e-05,
      "loss": 1.904,
      "step": 1175
    },
    {
      "epoch": 4.704,
      "grad_norm": 6.30847692489624,
      "learning_rate": 1.2e-05,
      "loss": 1.7257,
      "step": 1176
    },
    {
      "epoch": 4.708,
      "grad_norm": 4.378356456756592,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 1.3534,
      "step": 1177
    },
    {
      "epoch": 4.712,
      "grad_norm": 8.547423362731934,
      "learning_rate": 1.168e-05,
      "loss": 1.8593,
      "step": 1178
    },
    {
      "epoch": 4.716,
      "grad_norm": 6.598344802856445,
      "learning_rate": 1.152e-05,
      "loss": 1.8973,
      "step": 1179
    },
    {
      "epoch": 4.72,
      "grad_norm": 8.07801628112793,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 1.7203,
      "step": 1180
    },
    {
      "epoch": 4.724,
      "grad_norm": 5.3697662353515625,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 1.6997,
      "step": 1181
    },
    {
      "epoch": 4.728,
      "grad_norm": 6.630523681640625,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 1.6926,
      "step": 1182
    },
    {
      "epoch": 4.732,
      "grad_norm": 6.397194862365723,
      "learning_rate": 1.088e-05,
      "loss": 1.5552,
      "step": 1183
    },
    {
      "epoch": 4.736,
      "grad_norm": 7.580784797668457,
      "learning_rate": 1.072e-05,
      "loss": 1.8897,
      "step": 1184
    },
    {
      "epoch": 4.74,
      "grad_norm": 6.101710796356201,
      "learning_rate": 1.056e-05,
      "loss": 1.7825,
      "step": 1185
    },
    {
      "epoch": 4.744,
      "grad_norm": 5.491688251495361,
      "learning_rate": 1.04e-05,
      "loss": 1.5775,
      "step": 1186
    },
    {
      "epoch": 4.748,
      "grad_norm": 6.591041088104248,
      "learning_rate": 1.024e-05,
      "loss": 1.6785,
      "step": 1187
    },
    {
      "epoch": 4.752,
      "grad_norm": 5.545299530029297,
      "learning_rate": 1.008e-05,
      "loss": 1.6602,
      "step": 1188
    },
    {
      "epoch": 4.756,
      "grad_norm": 4.53275728225708,
      "learning_rate": 9.92e-06,
      "loss": 1.5654,
      "step": 1189
    },
    {
      "epoch": 4.76,
      "grad_norm": 7.73858118057251,
      "learning_rate": 9.760000000000001e-06,
      "loss": 1.949,
      "step": 1190
    },
    {
      "epoch": 4.764,
      "grad_norm": 5.671276569366455,
      "learning_rate": 9.600000000000001e-06,
      "loss": 1.6248,
      "step": 1191
    },
    {
      "epoch": 4.768,
      "grad_norm": 5.90891695022583,
      "learning_rate": 9.44e-06,
      "loss": 1.5283,
      "step": 1192
    },
    {
      "epoch": 4.772,
      "grad_norm": 8.833294868469238,
      "learning_rate": 9.28e-06,
      "loss": 1.9257,
      "step": 1193
    },
    {
      "epoch": 4.776,
      "grad_norm": 6.826576232910156,
      "learning_rate": 9.12e-06,
      "loss": 1.7141,
      "step": 1194
    },
    {
      "epoch": 4.78,
      "grad_norm": 4.512318134307861,
      "learning_rate": 8.96e-06,
      "loss": 1.4106,
      "step": 1195
    },
    {
      "epoch": 4.784,
      "grad_norm": 6.347989559173584,
      "learning_rate": 8.8e-06,
      "loss": 1.2943,
      "step": 1196
    },
    {
      "epoch": 4.788,
      "grad_norm": 6.407090187072754,
      "learning_rate": 8.64e-06,
      "loss": 1.686,
      "step": 1197
    },
    {
      "epoch": 4.792,
      "grad_norm": 6.789288520812988,
      "learning_rate": 8.48e-06,
      "loss": 1.7568,
      "step": 1198
    },
    {
      "epoch": 4.796,
      "grad_norm": 6.213938236236572,
      "learning_rate": 8.32e-06,
      "loss": 1.5957,
      "step": 1199
    },
    {
      "epoch": 4.8,
      "grad_norm": 4.661613464355469,
      "learning_rate": 8.160000000000001e-06,
      "loss": 1.1152,
      "step": 1200
    },
    {
      "epoch": 4.804,
      "grad_norm": 4.688880920410156,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.523,
      "step": 1201
    },
    {
      "epoch": 4.808,
      "grad_norm": 6.555377006530762,
      "learning_rate": 7.84e-06,
      "loss": 1.5002,
      "step": 1202
    },
    {
      "epoch": 4.812,
      "grad_norm": 6.5371503829956055,
      "learning_rate": 7.68e-06,
      "loss": 1.5825,
      "step": 1203
    },
    {
      "epoch": 4.816,
      "grad_norm": 6.0243239402771,
      "learning_rate": 7.520000000000001e-06,
      "loss": 1.5872,
      "step": 1204
    },
    {
      "epoch": 4.82,
      "grad_norm": 7.232639312744141,
      "learning_rate": 7.36e-06,
      "loss": 1.725,
      "step": 1205
    },
    {
      "epoch": 4.824,
      "grad_norm": 5.013026237487793,
      "learning_rate": 7.2e-06,
      "loss": 1.4998,
      "step": 1206
    },
    {
      "epoch": 4.828,
      "grad_norm": 5.850916385650635,
      "learning_rate": 7.04e-06,
      "loss": 1.8091,
      "step": 1207
    },
    {
      "epoch": 4.832,
      "grad_norm": 5.540019512176514,
      "learning_rate": 6.88e-06,
      "loss": 1.6007,
      "step": 1208
    },
    {
      "epoch": 4.836,
      "grad_norm": 7.051144123077393,
      "learning_rate": 6.72e-06,
      "loss": 1.9522,
      "step": 1209
    },
    {
      "epoch": 4.84,
      "grad_norm": 6.498117446899414,
      "learning_rate": 6.560000000000001e-06,
      "loss": 1.5236,
      "step": 1210
    },
    {
      "epoch": 4.844,
      "grad_norm": 6.845030784606934,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 1.7358,
      "step": 1211
    },
    {
      "epoch": 4.848,
      "grad_norm": 7.1931071281433105,
      "learning_rate": 6.24e-06,
      "loss": 1.6458,
      "step": 1212
    },
    {
      "epoch": 4.852,
      "grad_norm": 7.541469573974609,
      "learning_rate": 6.08e-06,
      "loss": 1.7147,
      "step": 1213
    },
    {
      "epoch": 4.856,
      "grad_norm": 5.945145606994629,
      "learning_rate": 5.920000000000001e-06,
      "loss": 1.5879,
      "step": 1214
    },
    {
      "epoch": 4.86,
      "grad_norm": 4.6819634437561035,
      "learning_rate": 5.76e-06,
      "loss": 1.2149,
      "step": 1215
    },
    {
      "epoch": 4.864,
      "grad_norm": 5.698210716247559,
      "learning_rate": 5.600000000000001e-06,
      "loss": 1.3379,
      "step": 1216
    },
    {
      "epoch": 4.868,
      "grad_norm": 7.297966003417969,
      "learning_rate": 5.44e-06,
      "loss": 1.74,
      "step": 1217
    },
    {
      "epoch": 4.872,
      "grad_norm": 8.914166450500488,
      "learning_rate": 5.28e-06,
      "loss": 2.0282,
      "step": 1218
    },
    {
      "epoch": 4.876,
      "grad_norm": 6.569285869598389,
      "learning_rate": 5.12e-06,
      "loss": 1.9325,
      "step": 1219
    },
    {
      "epoch": 4.88,
      "grad_norm": 7.197657108306885,
      "learning_rate": 4.96e-06,
      "loss": 1.7001,
      "step": 1220
    },
    {
      "epoch": 4.884,
      "grad_norm": 4.1451520919799805,
      "learning_rate": 4.800000000000001e-06,
      "loss": 1.3503,
      "step": 1221
    },
    {
      "epoch": 4.888,
      "grad_norm": 8.393232345581055,
      "learning_rate": 4.64e-06,
      "loss": 1.7274,
      "step": 1222
    },
    {
      "epoch": 4.892,
      "grad_norm": 7.793945789337158,
      "learning_rate": 4.48e-06,
      "loss": 1.5664,
      "step": 1223
    },
    {
      "epoch": 4.896,
      "grad_norm": 5.23349142074585,
      "learning_rate": 4.32e-06,
      "loss": 1.6767,
      "step": 1224
    },
    {
      "epoch": 4.9,
      "grad_norm": 6.445347785949707,
      "learning_rate": 4.16e-06,
      "loss": 1.5101,
      "step": 1225
    },
    {
      "epoch": 4.904,
      "grad_norm": 6.467967987060547,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.8806,
      "step": 1226
    },
    {
      "epoch": 4.908,
      "grad_norm": 5.840785503387451,
      "learning_rate": 3.84e-06,
      "loss": 1.559,
      "step": 1227
    },
    {
      "epoch": 4.912,
      "grad_norm": 7.245634078979492,
      "learning_rate": 3.68e-06,
      "loss": 1.9456,
      "step": 1228
    },
    {
      "epoch": 4.916,
      "grad_norm": 5.138779640197754,
      "learning_rate": 3.52e-06,
      "loss": 1.3084,
      "step": 1229
    },
    {
      "epoch": 4.92,
      "grad_norm": 6.110000133514404,
      "learning_rate": 3.36e-06,
      "loss": 1.5669,
      "step": 1230
    },
    {
      "epoch": 4.924,
      "grad_norm": 6.260219097137451,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.7106,
      "step": 1231
    },
    {
      "epoch": 4.928,
      "grad_norm": 6.940800666809082,
      "learning_rate": 3.04e-06,
      "loss": 1.5865,
      "step": 1232
    },
    {
      "epoch": 4.932,
      "grad_norm": 6.890584468841553,
      "learning_rate": 2.88e-06,
      "loss": 1.8629,
      "step": 1233
    },
    {
      "epoch": 4.936,
      "grad_norm": 4.307102680206299,
      "learning_rate": 2.72e-06,
      "loss": 1.431,
      "step": 1234
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 6.1090989112854,
      "learning_rate": 2.56e-06,
      "loss": 1.6065,
      "step": 1235
    },
    {
      "epoch": 4.944,
      "grad_norm": 5.963383674621582,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 1.7107,
      "step": 1236
    },
    {
      "epoch": 4.948,
      "grad_norm": 5.765869617462158,
      "learning_rate": 2.24e-06,
      "loss": 1.6339,
      "step": 1237
    },
    {
      "epoch": 4.952,
      "grad_norm": 5.94326114654541,
      "learning_rate": 2.08e-06,
      "loss": 1.7208,
      "step": 1238
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 5.347325801849365,
      "learning_rate": 1.92e-06,
      "loss": 1.4676,
      "step": 1239
    },
    {
      "epoch": 4.96,
      "grad_norm": 6.477519512176514,
      "learning_rate": 1.76e-06,
      "loss": 1.6734,
      "step": 1240
    },
    {
      "epoch": 4.964,
      "grad_norm": 7.053297519683838,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 1.7873,
      "step": 1241
    },
    {
      "epoch": 4.968,
      "grad_norm": 7.69867467880249,
      "learning_rate": 1.44e-06,
      "loss": 1.7864,
      "step": 1242
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 3.935559034347534,
      "learning_rate": 1.28e-06,
      "loss": 1.384,
      "step": 1243
    },
    {
      "epoch": 4.976,
      "grad_norm": 5.907723903656006,
      "learning_rate": 1.12e-06,
      "loss": 1.8415,
      "step": 1244
    },
    {
      "epoch": 4.98,
      "grad_norm": 7.411308765411377,
      "learning_rate": 9.6e-07,
      "loss": 1.8353,
      "step": 1245
    },
    {
      "epoch": 4.984,
      "grad_norm": 5.983648777008057,
      "learning_rate": 8.000000000000001e-07,
      "loss": 1.6472,
      "step": 1246
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 7.041781425476074,
      "learning_rate": 6.4e-07,
      "loss": 1.8325,
      "step": 1247
    },
    {
      "epoch": 4.992,
      "grad_norm": 6.640903949737549,
      "learning_rate": 4.8e-07,
      "loss": 1.7712,
      "step": 1248
    },
    {
      "epoch": 4.996,
      "grad_norm": 7.074897766113281,
      "learning_rate": 3.2e-07,
      "loss": 1.6357,
      "step": 1249
    },
    {
      "epoch": 5.0,
      "grad_norm": 5.438364505767822,
      "learning_rate": 1.6e-07,
      "loss": 1.6336,
      "step": 1250
    }
  ],
  "logging_steps": 1,
  "max_steps": 1250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 386667479040000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
